<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>Strategy - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.distribute</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.distribute/CrossDeviceOps.htm">CrossDeviceOps</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/HierarchicalCopyAllReduce.htm">HierarchicalCopyAllReduce</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/ICrossDeviceOps.htm">ICrossDeviceOps</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IHierarchicalCopyAllReduce.htm">IHierarchicalCopyAllReduce</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IInputContext.htm">IInputContext</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IInputReplicationMode.htm">IInputReplicationMode</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IMirroredStrategy.htm">IMirroredStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/INcclAllReduce.htm">INcclAllReduce</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/InputContext.htm">InputContext</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/InputReplicationMode.htm">InputReplicationMode</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IOneDeviceStrategy.htm">IOneDeviceStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IReduceOp.htm">IReduceOp</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IReductionToOneDevice.htm">IReductionToOneDevice</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IReplicaContext.htm">IReplicaContext</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IStrategy.htm">IStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/IStrategyExtended.htm">IStrategyExtended</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/MirroredStrategy.htm">MirroredStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/NcclAllReduce.htm">NcclAllReduce</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/OneDeviceStrategy.htm">OneDeviceStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/ReduceOp.htm">ReduceOp</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/ReductionToOneDevice.htm">ReductionToOneDevice</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/ReplicaContext.htm">ReplicaContext</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/Strategy.htm" class="current">Strategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute/StrategyExtended.htm">StrategyExtended</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> Strategy</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.distribute</p>
		<p><strong>Parent</strong> <a href="../tensorflow.compat.v2.distribute/Strategy.htm">Strategy</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.distribute/IStrategy.htm">IStrategy</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">A list of devices with a state & compute distribution policy. <p></p> See [the guide](https://www.tensorflow.org/guide/distribute_strategy)
for overview and examples. <p></p> Note: Not all <a href="..\..\tf\distribute\Strategy.md"><code>tf.distribute.Strategy</code></a> implementations currently support
TensorFlow's partitioned variables (where a single variable is split across
multiple devices) at this time. 
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.distribute/Strategy.htm#experimental_run_v2">experimental_run_v2</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#make_dataset_iterator">make_dataset_iterator</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#make_dataset_iterator_dyn">make_dataset_iterator_dyn</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#make_input_fn_iterator">make_input_fn_iterator</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#make_input_fn_iterator_dyn">make_input_fn_iterator_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.distribute/Strategy.htm#extended">extended</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#extended_dyn">extended_dyn</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#num_replicas_in_sync">num_replicas_in_sync</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#num_replicas_in_sync_dyn">num_replicas_in_sync_dyn</a></li>
				<li><a href="../tensorflow.distribute/Strategy.htm#PythonObject">PythonObject</a></li>
			</ul>
		
	</div>
	
	<h3 class="section">Public instance methods</h3>

	<div id="experimental_run_v2" class="method">
		<h4>
			<span title="System.object">object</span> <strong>experimental_run_v2</strong>(<span title="System.object">object</span> fn, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> args, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">See base class. 




		</div>
	</div>
	<div id="make_dataset_iterator" class="method">
		<h4>
			<span title="System.object">object</span> <strong>make_dataset_iterator</strong>(<span title="System.object">object</span> dataset)
		</h4>
		<div class="content">Makes an iterator for input provided via `dataset`. <p></p> DEPRECATED: This method is not available in TF 2.x. <p></p> Data from the given dataset will be distributed evenly across all the
compute replicas. We will assume that the input dataset is batched by the
global batch size. With this assumption, we will make a best effort to
divide each batch across all the replicas (one or more workers).
If this effort fails, an error will be thrown, and the user should instead
use `make_input_fn_iterator` which provides more control to the user, and
does not try to divide a batch across replicas. <p></p> The user could also use `make_input_fn_iterator` if they want to
customize which input is fed to which replica/worker etc. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> dataset
						</dt>
						<dd><a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a> that will be distributed evenly across all
replicas. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>An `tf.distribute.InputIterator` which returns inputs for each step of the
computation.  User should call `initialize` on the returned iterator. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="make_dataset_iterator_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>make_dataset_iterator_dyn</strong>(<span title="System.object">object</span> dataset)
		</h4>
		<div class="content">Makes an iterator for input provided via `dataset`. <p></p> DEPRECATED: This method is not available in TF 2.x. <p></p> Data from the given dataset will be distributed evenly across all the
compute replicas. We will assume that the input dataset is batched by the
global batch size. With this assumption, we will make a best effort to
divide each batch across all the replicas (one or more workers).
If this effort fails, an error will be thrown, and the user should instead
use `make_input_fn_iterator` which provides more control to the user, and
does not try to divide a batch across replicas. <p></p> The user could also use `make_input_fn_iterator` if they want to
customize which input is fed to which replica/worker etc. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> dataset
						</dt>
						<dd><a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a> that will be distributed evenly across all
replicas. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>An `tf.distribute.InputIterator` which returns inputs for each step of the
computation.  User should call `initialize` on the returned iterator. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="make_input_fn_iterator" class="method">
		<h4>
			<span title="System.object">object</span> <strong>make_input_fn_iterator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> input_fn, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> replication_mode)
		</h4>
		<div class="content">Returns an iterator split across replicas created from an input function. <p></p> DEPRECATED: This method is not available in TF 2.x. <p></p> The `input_fn` should take an <a href="..\..\tf\distribute\InputContext.md"><code>tf.distribute.InputContext</code></a> object where
information about batching and input sharding can be accessed: <p></p> ```
def input_fn(input_context):
batch_size = input_context.get_per_replica_batch_size(global_batch_size)
d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
return d.shard(input_context.num_input_pipelines,
input_context.input_pipeline_id)
with strategy.scope():
iterator = strategy.make_input_fn_iterator(input_fn)
replica_results = strategy.experimental_run(replica_fn, iterator)
``` <p></p> The <a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a> returned by `input_fn` should have a per-replica
batch size, which may be computed using
`input_context.get_per_replica_batch_size`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> input_fn
						</dt>
						<dd>A function taking a <a href="..\..\tf\distribute\InputContext.md"><code>tf.distribute.InputContext</code></a> object and
returning a <a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a>. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> replication_mode
						</dt>
						<dd>an enum value of <a href="..\..\tf\distribute\InputReplicationMode.md"><code>tf.distribute.InputReplicationMode</code></a>.
Only `PER_WORKER` is supported currently, which means there will be
a single call to `input_fn` per worker. Replicas will dequeue from the
local <a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a> on their worker. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>An iterator object that should first be `.initialize()`-ed. It may then
either be passed to `strategy.experimental_run()` or you can
`iterator.get_next()` to get the next value to pass to
`strategy.extended.call_for_each_replica()`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="make_input_fn_iterator_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>make_input_fn_iterator_dyn</strong>(<span title="System.object">object</span> input_fn, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> replication_mode)
		</h4>
		<div class="content">Returns an iterator split across replicas created from an input function. <p></p> DEPRECATED: This method is not available in TF 2.x. <p></p> The `input_fn` should take an <a href="..\..\tf\distribute\InputContext.md"><code>tf.distribute.InputContext</code></a> object where
information about batching and input sharding can be accessed: <p></p> ```
def input_fn(input_context):
batch_size = input_context.get_per_replica_batch_size(global_batch_size)
d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
return d.shard(input_context.num_input_pipelines,
input_context.input_pipeline_id)
with strategy.scope():
iterator = strategy.make_input_fn_iterator(input_fn)
replica_results = strategy.experimental_run(replica_fn, iterator)
``` <p></p> The <a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a> returned by `input_fn` should have a per-replica
batch size, which may be computed using
`input_context.get_per_replica_batch_size`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> input_fn
						</dt>
						<dd>A function taking a <a href="..\..\tf\distribute\InputContext.md"><code>tf.distribute.InputContext</code></a> object and
returning a <a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a>. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> replication_mode
						</dt>
						<dd>an enum value of <a href="..\..\tf\distribute\InputReplicationMode.md"><code>tf.distribute.InputReplicationMode</code></a>.
Only `PER_WORKER` is supported currently, which means there will be
a single call to `input_fn` per worker. Replicas will dequeue from the
local <a href="..\..\tf\data\Dataset.md"><code>tf.data.Dataset</code></a> on their worker. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>An iterator object that should first be `.initialize()`-ed. It may then
either be passed to `strategy.experimental_run()` or you can
`iterator.get_next()` to get the next value to pass to
`strategy.extended.call_for_each_replica()`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	
	
	<h3 class="section">Public properties</h3>

	<div id="extended" class="method">
		<h4>
			<span title="System.object">object</span> <strong>extended</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="extended_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>extended_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="num_replicas_in_sync" class="method">
		<h4>
			<span title="System.int">int</span> <strong>num_replicas_in_sync</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="num_replicas_in_sync_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>num_replicas_in_sync_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>