<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>RandomDataset - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.data.experimental</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.data.experimental/CheckpointInputPipelineHook.htm">CheckpointInputPipelineHook</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/CsvDataset.htm">CsvDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/DistributeOptions.htm">DistributeOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/ICheckpointInputPipelineHook.htm">ICheckpointInputPipelineHook</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/ICsvDataset.htm">ICsvDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IDistributeOptions.htm">IDistributeOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IMapVectorizationOptions.htm">IMapVectorizationOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IOptimizationOptions.htm">IOptimizationOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IOptional.htm">IOptional</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IRandomDataset.htm">IRandomDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IReducer.htm">IReducer</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/ISqlDataset.htm">ISqlDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IStatsAggregator.htm">IStatsAggregator</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IStatsOptions.htm">IStatsOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/ITFRecordWriter.htm">ITFRecordWriter</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/IThreadingOptions.htm">IThreadingOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/MapVectorizationOptions.htm">MapVectorizationOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/OptimizationOptions.htm">OptimizationOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/Optional.htm">Optional</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/RandomDataset.htm" class="current">RandomDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/Reducer.htm">Reducer</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/SqlDataset.htm">SqlDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/StatsAggregator.htm">StatsAggregator</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/StatsOptions.htm">StatsOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/TFRecordWriter.htm">TFRecordWriter</a>
        </li>
				<li>
            <a href="../tensorflow.data.experimental/ThreadingOptions.htm">ThreadingOptions</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> RandomDataset</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.data.experimental</p>
		<p><strong>Parent</strong> <a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.data.experimental/IRandomDataset.htm">IRandomDataset</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">A `Dataset` of pseudorandom values. 
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#apply">apply</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#filter">filter</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#filter_dyn">filter_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#filter_with_legacy_function_dyn">filter_with_legacy_function_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#flat_map_dyn">flat_map_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#interleave">interleave</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#interleave_dyn">interleave_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#reduce">reduce</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#reduce">reduce</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#reduce">reduce</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#reduce">reduce</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#reduce_dyn">reduce_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#unbatch">unbatch</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#unbatch_dyn">unbatch_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#window">window</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#window_dyn">window_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#element_spec">element_spec</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#element_spec_dyn">element_spec_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#output_classes">output_classes</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#output_classes_dyn">output_classes_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#output_shapes">output_shapes</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#output_shapes_dyn">output_shapes_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#output_types">output_types</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#output_types_dyn">output_types_dyn</a></li>
				<li><a href="../tensorflow.data.experimental/RandomDataset.htm#PythonObject">PythonObject</a></li>
			</ul>
		
	</div>
	
	<h3 class="section">Public instance methods</h3>

	<div id="apply" class="method">
		<h4>
			<span title="System.object">object</span> <strong>apply</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> transformation_func)
		</h4>
		<div class="content">Applies a transformation function to this dataset. <p></p> `apply` enables chaining of custom `Dataset` transformations, which are
represented as functions that take one `Dataset` argument and return a
transformed `Dataset`. <p></p> For example: <p></p> ```
dataset = (dataset.map(lambda x: x ** 2)
.apply(group_by_window(key_func, reduce_func, window_size))
.map(lambda x: x ** 3))
``` 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> transformation_func
						</dt>
						<dd>A function that takes one `Dataset` argument and
returns a `Dataset`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="filter" class="method">
		<h4>
			<span title="System.object">object</span> <strong>filter</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> predicate)
		</h4>
		<div class="content">Filters this dataset according to `predicate`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> predicate
						</dt>
						<dd>A function mapping a dataset element to a boolean. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>d = tf.data.Dataset.from_tensor_slices([1, 2, 3]) <p></p> d = d.filter(lambda x: x < 3)  # ==> [1, 2] <p></p> # `tf.math.equal(x, y)` is required for equality comparison
def filter_fn(x):
  return tf.math.equal(x, 1) <p></p> d = d.filter(filter_fn)  # ==> [1] </pre>
</div>
		</div>
	</div>
	<div id="filter_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>filter_dyn</strong>(<span title="System.object">object</span> predicate)
		</h4>
		<div class="content">Filters this dataset according to `predicate`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> predicate
						</dt>
						<dd>A function mapping a dataset element to a boolean. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>d = tf.data.Dataset.from_tensor_slices([1, 2, 3]) <p></p> d = d.filter(lambda x: x < 3)  # ==> [1, 2] <p></p> # `tf.math.equal(x, y)` is required for equality comparison
def filter_fn(x):
  return tf.math.equal(x, 1) <p></p> d = d.filter(filter_fn)  # ==> [1] </pre>
</div>
		</div>
	</div>
	<div id="filter_with_legacy_function_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>filter_with_legacy_function_dyn</strong>(<span title="System.object">object</span> predicate)
		</h4>
		<div class="content">Filters this dataset according to `predicate`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.filter() <p></p> NOTE: This is an escape hatch for existing uses of `filter` that do not work
with V2 functions. New uses are strongly discouraged and existing uses
should migrate to `filter` as this method will be removed in V2. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> predicate
						</dt>
						<dd>A function mapping a nested structure of tensors (having shapes
and types defined by `self.output_shapes` and `self.output_types`) to a
scalar <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> tensor. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="flat_map_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>flat_map_dyn</strong>(<span title="System.object">object</span> map_func)
		</h4>
		<div class="content">Maps `map_func` across this dataset and flattens the result. <p></p> Use `flat_map` if you want to make sure that the order of your dataset
stays the same. For example, to flatten a dataset of batches into a
dataset of their elements:
`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since
`flat_map` produces the same output as
`tf.data.Dataset.interleave(cycle_length=1)` 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> map_func
						</dt>
						<dd>A function mapping a dataset element to a dataset. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>a = Dataset.from_tensor_slices([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) <p></p> a.flat_map(lambda x: Dataset.from_tensor_slices(x + 1)) # ==>
#  [ 2, 3, 4, 5, 6, 7, 8, 9, 10 ] </pre>
</div>
		</div>
	</div>
	<div id="interleave" class="method">
		<h4>
			<span title="System.object">object</span> <strong>interleave</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> map_func, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> cycle_length, <span title="System.int">int</span> block_length, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_parallel_calls)
		</h4>
		<div class="content">Maps `map_func` across this dataset, and interleaves the results. <p></p> For example, you can use `Dataset.interleave()` to process many input files
concurrently:
The `cycle_length` and `block_length` arguments control the order in which
elements are produced. `cycle_length` controls the number of input elements
that are processed concurrently. If you set `cycle_length` to 1, this
transformation will handle one input element at a time, and will produce
identical results to <a href="..\..\tf\data\Dataset\flat_map.md"><code>tf.data.Dataset.flat_map</code></a>. In general,
this transformation will apply `map_func` to `cycle_length` input elements,
open iterators on the returned `Dataset` objects, and cycle through them
producing `block_length` consecutive elements from each iterator, and
consuming the next input element each time it reaches the end of an
iterator.
NOTE: The order of elements yielded by this transformation is
deterministic, as long as `map_func` is a pure function. If
`map_func` contains any stateful operations, the order in which
that state is accessed is undefined. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> map_func
						</dt>
						<dd>A function mapping a dataset element to a dataset. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> cycle_length
						</dt>
						<dd>(Optional.) The number of input elements that will be
processed concurrently. If not specified, the value will be derived from
the number of available CPU cores. If the `num_parallel_calls` argument
is set to <a href="..\..\tf\data\experimental\AUTOTUNE.md"><code>tf.data.experimental.AUTOTUNE</code></a>, the `cycle_length` argument
also identifies the maximum degree of parallelism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> block_length
						</dt>
						<dd>(Optional.) The number of consecutive elements to produce
from each input element before cycling to another input element. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_parallel_calls
						</dt>
						<dd>(Optional.) If specified, the implementation creates a
threadpool, which is used to fetch inputs from cycle elements
asynchronously and in parallel. The default behavior is to fetch inputs
from cycle elements synchronously with no parallelism. If the value
<a href="..\..\tf\data\experimental\AUTOTUNE.md"><code>tf.data.experimental.AUTOTUNE</code></a> is used, then the number of parallel
calls is set dynamically based on available CPU. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Preprocess 4 files concurrently, and interleave blocks of 16 records from
            # each file.
            filenames = ["/var/data/file1.txt", "/var/data/file2.txt",...]
            dataset = (Dataset.from_tensor_slices(filenames)
                      .interleave(lambda x:
                           TextLineDataset(x).map(parse_fn, num_parallel_calls=1),
                           cycle_length=4, block_length=16)) </pre>
</div>
		</div>
	</div>
	<div id="interleave_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>interleave_dyn</strong>(<span title="System.object">object</span> map_func, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> cycle_length, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> block_length, <span title="System.object">object</span> num_parallel_calls)
		</h4>
		<div class="content">Maps `map_func` across this dataset, and interleaves the results. <p></p> For example, you can use `Dataset.interleave()` to process many input files
concurrently:
The `cycle_length` and `block_length` arguments control the order in which
elements are produced. `cycle_length` controls the number of input elements
that are processed concurrently. If you set `cycle_length` to 1, this
transformation will handle one input element at a time, and will produce
identical results to <a href="..\..\tf\data\Dataset\flat_map.md"><code>tf.data.Dataset.flat_map</code></a>. In general,
this transformation will apply `map_func` to `cycle_length` input elements,
open iterators on the returned `Dataset` objects, and cycle through them
producing `block_length` consecutive elements from each iterator, and
consuming the next input element each time it reaches the end of an
iterator.
NOTE: The order of elements yielded by this transformation is
deterministic, as long as `map_func` is a pure function. If
`map_func` contains any stateful operations, the order in which
that state is accessed is undefined. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> map_func
						</dt>
						<dd>A function mapping a dataset element to a dataset. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> cycle_length
						</dt>
						<dd>(Optional.) The number of input elements that will be
processed concurrently. If not specified, the value will be derived from
the number of available CPU cores. If the `num_parallel_calls` argument
is set to <a href="..\..\tf\data\experimental\AUTOTUNE.md"><code>tf.data.experimental.AUTOTUNE</code></a>, the `cycle_length` argument
also identifies the maximum degree of parallelism. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> block_length
						</dt>
						<dd>(Optional.) The number of consecutive elements to produce
from each input element before cycling to another input element. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_parallel_calls
						</dt>
						<dd>(Optional.) If specified, the implementation creates a
threadpool, which is used to fetch inputs from cycle elements
asynchronously and in parallel. The default behavior is to fetch inputs
from cycle elements synchronously with no parallelism. If the value
<a href="..\..\tf\data\experimental\AUTOTUNE.md"><code>tf.data.experimental.AUTOTUNE</code></a> is used, then the number of parallel
calls is set dynamically based on available CPU. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Preprocess 4 files concurrently, and interleave blocks of 16 records from
            # each file.
            filenames = ["/var/data/file1.txt", "/var/data/file2.txt",...]
            dataset = (Dataset.from_tensor_slices(filenames)
                      .interleave(lambda x:
                           TextLineDataset(x).map(parse_fn, num_parallel_calls=1),
                           cycle_length=4, block_length=16)) </pre>
</div>
		</div>
	</div>
	<div id="reduce" class="method">
		<h4>
			<span title="System.object">object</span> <strong>reduce</strong>(<a href="../numpy/int64.htm">int64</a> initial_state, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> reduce_func)
		</h4>
		<div class="content">




		</div>
	</div>
	<div id="reduce" class="method">
		<h4>
			<span title="System.object">object</span> <strong>reduce</strong>(<span title="System.ValueTuple<IGraphNodeBase, object>">ValueTuple&lt;IGraphNodeBase, object&gt;</span> initial_state, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> reduce_func)
		</h4>
		<div class="content">




		</div>
	</div>
	<div id="reduce" class="method">
		<h4>
			<span title="System.object">object</span> <strong>reduce</strong>(<span title="System.int">int</span> initial_state, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> reduce_func)
		</h4>
		<div class="content">




		</div>
	</div>
	<div id="reduce" class="method">
		<h4>
			<span title="System.object">object</span> <strong>reduce</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> initial_state, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> reduce_func)
		</h4>
		<div class="content">




		</div>
	</div>
	<div id="reduce_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>reduce_dyn</strong>(<span title="System.object">object</span> initial_state, <span title="System.object">object</span> reduce_func)
		</h4>
		<div class="content">




		</div>
	</div>
	<div id="unbatch" class="method">
		<h4>
			<a href="../tensorflow.python.data.ops.dataset_ops/_UnbatchDataset.htm">_UnbatchDataset</a> <strong>unbatch</strong>()
		</h4>
		<div class="content">Splits elements of a dataset into multiple elements on the batch dimension. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.unbatch()`. <p></p> For example, if elements of the dataset are shaped `[B, a0, a1,...]`,
where `B` may vary for each input element, then for each element in the
dataset, the unbatched dataset will contain `B` consecutive elements
of shape `[a0, a1,...]`. 



			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.python.data.ops.dataset_ops/_UnbatchDataset.htm">_UnbatchDataset</a></code>
					</dt>
					<dd>A `Dataset` transformation function, which can be passed to
<a href="..\..\..\tf\data\Dataset\apply.md"><code>tf.data.Dataset.apply</code></a>. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># NOTE: The following example uses `{... }` to represent the contents
            # of a dataset.
            a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] } <p></p> a.apply(tf.data.experimental.unbatch()) == {
    'a', 'b', 'c', 'a', 'b', 'a', 'b', 'c', 'd'} </pre>
</div>
		</div>
	</div>
	<div id="unbatch_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>unbatch_dyn</strong>()
		</h4>
		<div class="content">Splits elements of a dataset into multiple elements on the batch dimension. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.unbatch()`. <p></p> For example, if elements of the dataset are shaped `[B, a0, a1,...]`,
where `B` may vary for each input element, then for each element in the
dataset, the unbatched dataset will contain `B` consecutive elements
of shape `[a0, a1,...]`. 



			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `Dataset` transformation function, which can be passed to
<a href="..\..\..\tf\data\Dataset\apply.md"><code>tf.data.Dataset.apply</code></a>. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># NOTE: The following example uses `{... }` to represent the contents
            # of a dataset.
            a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] } <p></p> a.apply(tf.data.experimental.unbatch()) == {
    'a', 'b', 'c', 'a', 'b', 'a', 'b', 'c', 'd'} </pre>
</div>
		</div>
	</div>
	<div id="window" class="method">
		<h4>
			<span title="System.object">object</span> <strong>window</strong>(<span title="System.int">int</span> size, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> shift, <span title="System.int">int</span> stride, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines (nests of) input elements into a dataset of (nests of) windows. <p></p> A "window" is a finite dataset of flat elements of size `size` (or possibly
fewer if there are not enough input elements to fill the window and
`drop_remainder` evaluates to false). <p></p> The `stride` argument determines the stride of the input elements, and the
`shift` argument determines the shift of the window. <p></p> For example, letting {...} to represent a Dataset: <p></p> - `tf.data.Dataset.range(7).window(2)` produces
`{{0, 1}, {2, 3}, {4, 5}, {6}}`
- `tf.data.Dataset.range(7).window(3, 2, 1, True)` produces
`{{0, 1, 2}, {2, 3, 4}, {4, 5, 6}}`
- `tf.data.Dataset.range(7).window(3, 1, 2, True)` produces
`{{0, 2, 4}, {1, 3, 5}, {2, 4, 6}}` <p></p> Note that when the `window` transformation is applied to a dataset of
nested elements, it produces a dataset of nested windows. <p></p> For example: <p></p> - `tf.data.Dataset.from_tensor_slices((range(4), range(4))).window(2)`
produces `{({0, 1}, {0, 1}), ({2, 3}, {2, 3})}`
- `tf.data.Dataset.from_tensor_slices({"a": range(4)}).window(2)`
produces `{{"a": {0, 1}}, {"a": {2, 3}}}` 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of elements
of the input dataset to combine into a window. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> shift
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the
forward shift of the sliding window in each iteration. Defaults to
`size`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stride
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the
stride of the input elements in the sliding window. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether a window should be dropped in case its size is smaller than
`window_size`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="window_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>window_dyn</strong>(<span title="System.object">object</span> size, <span title="System.object">object</span> shift, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> stride, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> drop_remainder)
		</h4>
		<div class="content">Combines (nests of) input elements into a dataset of (nests of) windows. <p></p> A "window" is a finite dataset of flat elements of size `size` (or possibly
fewer if there are not enough input elements to fill the window and
`drop_remainder` evaluates to false). <p></p> The `stride` argument determines the stride of the input elements, and the
`shift` argument determines the shift of the window. <p></p> For example, letting {...} to represent a Dataset: <p></p> - `tf.data.Dataset.range(7).window(2)` produces
`{{0, 1}, {2, 3}, {4, 5}, {6}}`
- `tf.data.Dataset.range(7).window(3, 2, 1, True)` produces
`{{0, 1, 2}, {2, 3, 4}, {4, 5, 6}}`
- `tf.data.Dataset.range(7).window(3, 1, 2, True)` produces
`{{0, 2, 4}, {1, 3, 5}, {2, 4, 6}}` <p></p> Note that when the `window` transformation is applied to a dataset of
nested elements, it produces a dataset of nested windows. <p></p> For example: <p></p> - `tf.data.Dataset.from_tensor_slices((range(4), range(4))).window(2)`
produces `{({0, 1}, {0, 1}), ({2, 3}, {2, 3})}`
- `tf.data.Dataset.from_tensor_slices({"a": range(4)}).window(2)`
produces `{{"a": {0, 1}}, {"a": {2, 3}}}` 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of elements
of the input dataset to combine into a window. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shift
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the
forward shift of the sliding window in each iteration. Defaults to
`size`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> stride
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the
stride of the input elements in the sliding window. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether a window should be dropped in case its size is smaller than
`window_size`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	
	
	<h3 class="section">Public properties</h3>

	<div id="element_spec" class="method">
		<h4>
			<span title="System.object">object</span> <strong>element_spec</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="element_spec_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>element_spec_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="output_classes" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_classes</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="output_classes_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_classes_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="output_shapes" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_shapes</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="output_shapes_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_shapes_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="output_types" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_types</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="output_types_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_types_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>