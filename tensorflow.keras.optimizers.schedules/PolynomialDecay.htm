<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>PolynomialDecay - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.keras.optimizers.schedules</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/ExponentialDecay.htm">ExponentialDecay</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/IExponentialDecay.htm">IExponentialDecay</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/IInverseTimeDecay.htm">IInverseTimeDecay</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/ILearningRateSchedule.htm">ILearningRateSchedule</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/InverseTimeDecay.htm">InverseTimeDecay</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/IPiecewiseConstantDecay.htm">IPiecewiseConstantDecay</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/IPolynomialDecay.htm">IPolynomialDecay</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/LearningRateSchedule.htm">LearningRateSchedule</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/PiecewiseConstantDecay.htm">PiecewiseConstantDecay</a>
        </li>
				<li>
            <a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm" class="current">PolynomialDecay</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> PolynomialDecay</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.keras.optimizers.schedules</p>
		<p><strong>Parent</strong> <a href="../tensorflow.keras.optimizers.schedules/LearningRateSchedule.htm">LearningRateSchedule</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.keras.optimizers.schedules/IPolynomialDecay.htm">IPolynomialDecay</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">A LearningRateSchedule that uses a polynomial decay schedule. 
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#NewDyn">NewDyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#cycle">cycle</a></li>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#decay_steps">decay_steps</a></li>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#end_learning_rate">end_learning_rate</a></li>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#initial_learning_rate">initial_learning_rate</a></li>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#name">name</a></li>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#power">power</a></li>
				<li><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm#PythonObject">PythonObject</a></li>
			</ul>
		
	</div>
	
	
	<h3 class="section">Public static methods</h3>

	<div id="NewDyn" class="method">
		<h4>
			<a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm">PolynomialDecay</a> <strong>NewDyn</strong>(<span title="System.object">object</span> initial_learning_rate, <span title="System.object">object</span> decay_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> end_learning_rate, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> power, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> cycle, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies a polynomial decay to the learning rate. <p></p> It is commonly observed that a monotonically decreasing learning rate, whose
degree of change is carefully chosen, results in a better performing model.
This schedule applies a polynomial decay function to an optimizer step,
given a provided `initial_learning_rate`, to reach an `end_learning_rate`
in the given `decay_steps`. <p></p> It requires a `step` value to compute the decayed learning rate. You
can just pass a TensorFlow variable that you increment at each training
step. <p></p> The schedule is a 1-arg callable that produces a decayed learning rate
when passed the current optimizer step. This can be useful for changing the
learning rate value across different invocations of optimizer functions.
It is computed as:
If `cycle` is True then a multiple of `decay_steps` is used, the first one
that is bigger than `step`.
You can pass this schedule directly into a <a href="..\..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a>
as the learning rate.
Example: Fit a model while decaying from 0.1 to 0.01 in 10000 steps using
sqrt (i.e. power=0.5):
The learning rate schedule is also serializable and deserializable using
<a href="..\..\..\..\tf\keras\optimizers\schedules\serialize.md"><code>tf.keras.optimizers.schedules.serialize</code></a> and
<a href="..\..\..\..\tf\keras\optimizers\schedules\deserialize.md"><code>tf.keras.optimizers.schedules.deserialize</code></a>. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> initial_learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a
Python number.  The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number.
Must be positive.  See the decay computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> end_learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a
Python number.  The minimal end learning rate. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> power
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a
Python number.  The power of the polynomial. Defaults to linear, 1.0. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> cycle
						</dt>
						<dd>A boolean, whether or not it should cycle beyond decay_steps. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name of the operation. Defaults to
'PolynomialDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.keras.optimizers.schedules/PolynomialDecay.htm">PolynomialDecay</a></code>
					</dt>
					<dd>A 1-arg callable learning rate schedule that takes the current optimizer
step and outputs the decayed learning rate, a scalar `Tensor` of the same
type as `initial_learning_rate`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>def decayed_learning_rate(step):
              step = min(step, decay_steps)
              return ((initial_learning_rate - end_learning_rate) *
                      (1 - step / decay_steps) ^ (power)
                     ) + end_learning_rate </pre>
</div>
		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="cycle" class="method">
		<h4>
			<span title="System.bool">bool</span> <strong>cycle</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="decay_steps" class="method">
		<h4>
			<span title="System.int">int</span> <strong>decay_steps</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="end_learning_rate" class="method">
		<h4>
			<span title="System.double">double</span> <strong>end_learning_rate</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="initial_learning_rate" class="method">
		<h4>
			<span title="System.double">double</span> <strong>initial_learning_rate</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="name" class="method">
		<h4>
			<span title="System.object">object</span> <strong>name</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="power" class="method">
		<h4>
			<span title="System.double">double</span> <strong>power</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>