<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>ParameterServerStrategy - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.distribute.experimental</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.distribute.experimental/CentralStorageStrategy.htm">CentralStorageStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/CollectiveCommunication.htm">CollectiveCommunication</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/ICentralStorageStrategy.htm">ICentralStorageStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/ICollectiveCommunication.htm">ICollectiveCommunication</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/IMultiWorkerMirroredStrategy.htm">IMultiWorkerMirroredStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/IParameterServerStrategy.htm">IParameterServerStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/ITPUStrategy.htm">ITPUStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/MultiWorkerMirroredStrategy.htm">MultiWorkerMirroredStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/ParameterServerStrategy.htm" class="current">ParameterServerStrategy</a>
        </li>
				<li>
            <a href="../tensorflow.distribute.experimental/TPUStrategy.htm">TPUStrategy</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> ParameterServerStrategy</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.distribute.experimental</p>
		<p><strong>Parent</strong> <a href="../tensorflow.distribute/Strategy.htm">Strategy</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.distribute.experimental/IParameterServerStrategy.htm">IParameterServerStrategy</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">An asynchronous multi-worker parameter server tf.distribute strategy. <p></p> This strategy requires two jobs: workers and parameter servers. Variables and
updates to those variables will be assigned to parameter servers and other
operations are assigned to workers. <p></p> When each worker has more than one GPU, operations will be replicated on all
GPUs. Even though operations may be replicated, variables are not and each
worker shares a common view for which parameter server a variable is assigned
to. <p></p> By default it uses `TFConfigClusterResolver` to detect configurations for
multi-worker training. This requires a 'TF_CONFIG' environment variable and
the 'TF_CONFIG' must have a cluster spec. <p></p> This class assumes each worker is running the same code independently, but
parameter servers are running a standard server. This means that while each
worker will synchronously compute a single gradient update across all GPUs,
updates between workers proceed asynchronously. Operations that occur only on
the first replica (such as incrementing the global step), will occur on the
first replica *of every worker*. <p></p> It is expected to call `call_for_each_replica(fn,...)` for any
operations which potentially can be replicated across replicas (i.e. multiple
GPUs) even if there is only CPU or one GPU. When defining the `fn`, extra
caution needs to be taken: <p></p> 1) It is generally not recommended to open a device scope under the strategy's
scope. A device scope (i.e. calling <a href="..\..\..\tf\device.md"><code>tf.device</code></a>) will be merged with or
override the device for operations but will not change the device for
variables. <p></p> 2) It is also not recommended to open a colocation scope (i.e. calling
`tf.compat.v1.colocate_with`) under the strategy's scope. For colocating
variables, use `strategy.extended.colocate_vars_with` instead. Colocation of
ops will possibly create device assignment conflicts. <p></p> Note: This strategy only works with the Estimator API. Pass an instance of
this strategy to the `experimental_distribute` argument when you create the
`RunConfig`. This instance of `RunConfig` should then be passed to the
`Estimator` instance on which `train_and_evaluate` is called. <p></p> For Example:
```
strategy = tf.distribute.experimental.ParameterServerStrategy()
run_config = tf.estimator.RunConfig(
experimental_distribute.train_distribute=strategy)
estimator = tf.estimator.Estimator(config=run_config)
tf.estimator.train_and_evaluate(estimator,...) 
			</div>
		
		
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.distribute.experimental/ParameterServerStrategy.htm#extended">extended</a></li>
				<li><a href="../tensorflow.distribute.experimental/ParameterServerStrategy.htm#extended_dyn">extended_dyn</a></li>
				<li><a href="../tensorflow.distribute.experimental/ParameterServerStrategy.htm#num_replicas_in_sync">num_replicas_in_sync</a></li>
				<li><a href="../tensorflow.distribute.experimental/ParameterServerStrategy.htm#num_replicas_in_sync_dyn">num_replicas_in_sync_dyn</a></li>
				<li><a href="../tensorflow.distribute.experimental/ParameterServerStrategy.htm#PythonObject">PythonObject</a></li>
			</ul>
		
	</div>
	
	
	
	<h3 class="section">Public properties</h3>

	<div id="extended" class="method">
		<h4>
			<span title="System.object">object</span> <strong>extended</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="extended_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>extended_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="num_replicas_in_sync" class="method">
		<h4>
			<span title="System.int">int</span> <strong>num_replicas_in_sync</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="num_replicas_in_sync_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>num_replicas_in_sync_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>