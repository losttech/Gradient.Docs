<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>experimental - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.compat.v2.train.experimental</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.compat.v2.train.experimental/experimental.htm" class="current">experimental</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> experimental</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.compat.v2.train.experimental</p>
		</header>
    <div class="sub-header">
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#disable_mixed_precision_graph_rewrite">disable_mixed_precision_graph_rewrite</a></li>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#disable_mixed_precision_graph_rewrite_dyn">disable_mixed_precision_graph_rewrite_dyn</a></li>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#enable_mixed_precision_graph_rewrite">enable_mixed_precision_graph_rewrite</a></li>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#enable_mixed_precision_graph_rewrite">enable_mixed_precision_graph_rewrite</a></li>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#enable_mixed_precision_graph_rewrite">enable_mixed_precision_graph_rewrite</a></li>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#enable_mixed_precision_graph_rewrite">enable_mixed_precision_graph_rewrite</a></li>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#enable_mixed_precision_graph_rewrite_dyn">enable_mixed_precision_graph_rewrite_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#disable_mixed_precision_graph_rewrite_fn">disable_mixed_precision_graph_rewrite_fn</a></li>
				<li><a href="../tensorflow.compat.v2.train.experimental/experimental.htm#enable_mixed_precision_graph_rewrite_fn">enable_mixed_precision_graph_rewrite_fn</a></li>
			</ul>
		
	</div>
	
	
	<h3 class="section">Public static methods</h3>

	<div id="disable_mixed_precision_graph_rewrite" class="method">
		<h4>
			<span title="System.void">void</span> <strong>disable_mixed_precision_graph_rewrite</strong>()
		</h4>
		<div class="content">Disables the mixed precision graph rewrite. <p></p> After this is called, the mixed precision graph rewrite will no longer run for
new Sessions, and so float32 operations will no longer be converted to float16
in such Sessions. However, any existing Sessions will continue to have the
graph rewrite enabled if they were created after
`enable_mixed_precision_graph_rewrite` was called but before
`disable_mixed_precision_graph_rewrite` was called. <p></p> This does not undo the effects of loss scaling. Any optimizers wrapped with a
LossScaleOptimizer will continue to do loss scaling, although this loss
scaling will no longer be useful if the optimizer is used in new Sessions, as
the graph rewrite no longer converts the graph to use float16. <p></p> This function is useful for unit testing. A unit tests can test using the
mixed precision graph rewrite, then disable it so future unit tests continue
using float32. If this is done, unit tests should not share a single session,
as `enable_mixed_precision_graph_rewrite` and
`disable_mixed_precision_graph_rewrite` have no effect on existing sessions. 




		</div>
	</div>
	<div id="disable_mixed_precision_graph_rewrite_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>disable_mixed_precision_graph_rewrite_dyn</strong>()
		</h4>
		<div class="content">Disables the mixed precision graph rewrite. <p></p> After this is called, the mixed precision graph rewrite will no longer run for
new Sessions, and so float32 operations will no longer be converted to float16
in such Sessions. However, any existing Sessions will continue to have the
graph rewrite enabled if they were created after
`enable_mixed_precision_graph_rewrite` was called but before
`disable_mixed_precision_graph_rewrite` was called. <p></p> This does not undo the effects of loss scaling. Any optimizers wrapped with a
LossScaleOptimizer will continue to do loss scaling, although this loss
scaling will no longer be useful if the optimizer is used in new Sessions, as
the graph rewrite no longer converts the graph to use float16. <p></p> This function is useful for unit testing. A unit tests can test using the
mixed precision graph rewrite, then disable it so future unit tests continue
using float32. If this is done, unit tests should not share a single session,
as `enable_mixed_precision_graph_rewrite` and
`disable_mixed_precision_graph_rewrite` have no effect on existing sessions. 




		</div>
	</div>
	<div id="enable_mixed_precision_graph_rewrite" class="method">
		<h4>
			<span title="System.object">object</span> <strong>enable_mixed_precision_graph_rewrite</strong>(<a href="../tensorflow.python.training.tracking.base/Trackable.htm">Trackable</a> opt, <span title="System.double">double</span> loss_scale)
		</h4>
		<div class="content">Enable mixed precision via a graph rewrite. <p></p> Mixed precision is the use of both float32 and float16 data types when
training a model to improve performance. This is achieved via a graph rewrite
operation and a loss-scale optimizer. <p></p> Performing arithmetic operations in float16 takes advantage of specialized
processing units, such as NVIDIA Tensor Cores for much higher arithmetic
throughput. However, due to the smaller representable range, performing the
entire training with float16 can result in gradient underflow, that is, small
gradient values becoming zeroes. Instead, performing only select arithmetic
operations in float16 results in higher throughput and decreased training
time when using compatible hardware accelerators while also reducing memory
usage, typically without sacrificing model accuracy. <p></p> Note: While the mixed precision rewrite changes the datatype of various
layers throughout the model, the same accuracy reached in float32 is
expected. If a `NaN` gradient occurs with dynamic loss scaling, the model
update for that batch is skipped. In this case, the global step count is not
incremented, and the `LossScaleOptimizer` attempts to decrease the loss
scaling value to avoid `NaN` values in subsequent iterations. This approach
has been shown to achieve the same accuracy as float32 and, in most cases,
better training throughput. <p></p> Example:
For a complete example showing the speed-up on training an image
classification task on CIFAR10, check out this
<a href="https://colab.research.google.com/github/NVIDIA/
DeepLearningExamples/blob/master/TensorFlow/docs/amp/notebook_v1.14/
auto_mixed_precision_demo_cifar10.ipynb">Colab notebook</a>. <p></p> Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite
operation before computing gradients. The function additionally returns an
`Optimizer`(`opt`) wrapped with a `LossScaleOptimizer`. This prevents
underflow in the float16 tensors during the backward pass. An optimizer of
type <a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a> or <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> must be passed
to this function, which will then be wrapped to use loss scaling. <p></p> <img src="
http://developer.download.nvidia.com/compute/machine-learning/frameworks/
TF_mixed_precision_training.png" width="500px"> <p></p> The graph rewrite operation changes the `dtype` of certain operations in the
graph from float32 to float16. There are several categories of operations
that are either included or excluded by this rewrite operation. The following
categories of Ops are defined inside corresponding functions under the class
`AutoMixedPrecisionLists` in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/
core/grappler/optimizers/auto_mixed_precision_lists.h">
auto_mixed_precision_lists.h</a>: <p></p> * `ClearList`: Ops that do not have numerically significant adverse effects.
E.g. `ArgMax` and `Floor`.
* `WhiteList`: Ops that are considered numerically safe for execution in
float16, and thus are always converted. E.g. `Conv2D`.
* `BlackList`: Ops that are numerically unsafe to execute in float16 and
can negatively affect downstream nodes. E.g. `Softmax`.
* `GrayList`: Ops that are considered numerically safe for execution in
float16 unless downstream from a BlackList Op. E.g. `Add` and `AvgPool`. <p></p> When this function is used, gradients should only be computed and applied
with the returned optimizer, either by calling `opt.minimize()` or
`opt.compute_gradients()` followed by `opt.apply_gradients()`.
Gradients should not be computed with <a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>.
This is because the returned optimizer will apply loss scaling, and
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a> will not. If you do directly use
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>, your model may not converge due to
float16 underflow problems. <p></p> When eager execution is enabled, the mixed precision graph rewrite is only
enabled within <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, as outside <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, there is no graph. <p></p> For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions
(such as batch size, input size, output size, and channel counts)
should be powers of two if under 256, or  otherwise divisible by 8 if above
256. For more information, check out the
[NVIDIA Deep Learning Performance Guide](
https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html). <p></p> Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with
Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The
parts of the graph on CPUs and TPUs are untouched by the graph rewrite. TPU
support is coming soon. CPUs are not supported, as CPUs do not run float16
operations faster than float32 operations. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.tracking.base/Trackable.htm">Trackable</a></code> opt
						</dt>
						<dd>An instance of a <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> or a
<a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a>. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> loss_scale
						</dt>
						<dd>Either an int/float, the string `"dynamic"`, or an instance of
a <a href="..\..\..\tf\train\experimental\LossScale.md"><code>tf.train.experimental.LossScale</code></a>. The loss scale to use. It is
recommended to keep this as its default value of `"dynamic"`, which will
adjust the scaling automatically to prevent `Inf` or `NaN` values. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A version of `opt` that will use loss scaling to prevent underflow. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>model = tf.keras.models.Sequential([
             ...
            ]) <p></p> opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) <p></p> model.compile(loss="categorical_crossentropy",
            optimizer=opt,
            metrics=["accuracy"]) <p></p> model.fit(x_train, y_train,
        batch_size=batch_size,
        epochs=epochs) </pre>
</div>
		</div>
	</div>
	<div id="enable_mixed_precision_graph_rewrite" class="method">
		<h4>
			<span title="System.object">object</span> <strong>enable_mixed_precision_graph_rewrite</strong>(<a href="../tensorflow.python.training.tracking.base/Trackable.htm">Trackable</a> opt, <span title="System.string">string</span> loss_scale)
		</h4>
		<div class="content">Enable mixed precision via a graph rewrite. <p></p> Mixed precision is the use of both float32 and float16 data types when
training a model to improve performance. This is achieved via a graph rewrite
operation and a loss-scale optimizer. <p></p> Performing arithmetic operations in float16 takes advantage of specialized
processing units, such as NVIDIA Tensor Cores for much higher arithmetic
throughput. However, due to the smaller representable range, performing the
entire training with float16 can result in gradient underflow, that is, small
gradient values becoming zeroes. Instead, performing only select arithmetic
operations in float16 results in higher throughput and decreased training
time when using compatible hardware accelerators while also reducing memory
usage, typically without sacrificing model accuracy. <p></p> Note: While the mixed precision rewrite changes the datatype of various
layers throughout the model, the same accuracy reached in float32 is
expected. If a `NaN` gradient occurs with dynamic loss scaling, the model
update for that batch is skipped. In this case, the global step count is not
incremented, and the `LossScaleOptimizer` attempts to decrease the loss
scaling value to avoid `NaN` values in subsequent iterations. This approach
has been shown to achieve the same accuracy as float32 and, in most cases,
better training throughput. <p></p> Example:
For a complete example showing the speed-up on training an image
classification task on CIFAR10, check out this
<a href="https://colab.research.google.com/github/NVIDIA/
DeepLearningExamples/blob/master/TensorFlow/docs/amp/notebook_v1.14/
auto_mixed_precision_demo_cifar10.ipynb">Colab notebook</a>. <p></p> Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite
operation before computing gradients. The function additionally returns an
`Optimizer`(`opt`) wrapped with a `LossScaleOptimizer`. This prevents
underflow in the float16 tensors during the backward pass. An optimizer of
type <a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a> or <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> must be passed
to this function, which will then be wrapped to use loss scaling. <p></p> <img src="
http://developer.download.nvidia.com/compute/machine-learning/frameworks/
TF_mixed_precision_training.png" width="500px"> <p></p> The graph rewrite operation changes the `dtype` of certain operations in the
graph from float32 to float16. There are several categories of operations
that are either included or excluded by this rewrite operation. The following
categories of Ops are defined inside corresponding functions under the class
`AutoMixedPrecisionLists` in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/
core/grappler/optimizers/auto_mixed_precision_lists.h">
auto_mixed_precision_lists.h</a>: <p></p> * `ClearList`: Ops that do not have numerically significant adverse effects.
E.g. `ArgMax` and `Floor`.
* `WhiteList`: Ops that are considered numerically safe for execution in
float16, and thus are always converted. E.g. `Conv2D`.
* `BlackList`: Ops that are numerically unsafe to execute in float16 and
can negatively affect downstream nodes. E.g. `Softmax`.
* `GrayList`: Ops that are considered numerically safe for execution in
float16 unless downstream from a BlackList Op. E.g. `Add` and `AvgPool`. <p></p> When this function is used, gradients should only be computed and applied
with the returned optimizer, either by calling `opt.minimize()` or
`opt.compute_gradients()` followed by `opt.apply_gradients()`.
Gradients should not be computed with <a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>.
This is because the returned optimizer will apply loss scaling, and
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a> will not. If you do directly use
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>, your model may not converge due to
float16 underflow problems. <p></p> When eager execution is enabled, the mixed precision graph rewrite is only
enabled within <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, as outside <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, there is no graph. <p></p> For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions
(such as batch size, input size, output size, and channel counts)
should be powers of two if under 256, or  otherwise divisible by 8 if above
256. For more information, check out the
[NVIDIA Deep Learning Performance Guide](
https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html). <p></p> Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with
Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The
parts of the graph on CPUs and TPUs are untouched by the graph rewrite. TPU
support is coming soon. CPUs are not supported, as CPUs do not run float16
operations faster than float32 operations. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.tracking.base/Trackable.htm">Trackable</a></code> opt
						</dt>
						<dd>An instance of a <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> or a
<a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a>. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> loss_scale
						</dt>
						<dd>Either an int/float, the string `"dynamic"`, or an instance of
a <a href="..\..\..\tf\train\experimental\LossScale.md"><code>tf.train.experimental.LossScale</code></a>. The loss scale to use. It is
recommended to keep this as its default value of `"dynamic"`, which will
adjust the scaling automatically to prevent `Inf` or `NaN` values. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A version of `opt` that will use loss scaling to prevent underflow. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>model = tf.keras.models.Sequential([
             ...
            ]) <p></p> opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) <p></p> model.compile(loss="categorical_crossentropy",
            optimizer=opt,
            metrics=["accuracy"]) <p></p> model.fit(x_train, y_train,
        batch_size=batch_size,
        epochs=epochs) </pre>
</div>
		</div>
	</div>
	<div id="enable_mixed_precision_graph_rewrite" class="method">
		<h4>
			<span title="System.object">object</span> <strong>enable_mixed_precision_graph_rewrite</strong>(<span title="System.int">int</span> opt, <span title="System.double">double</span> loss_scale)
		</h4>
		<div class="content">Enable mixed precision via a graph rewrite. <p></p> Mixed precision is the use of both float32 and float16 data types when
training a model to improve performance. This is achieved via a graph rewrite
operation and a loss-scale optimizer. <p></p> Performing arithmetic operations in float16 takes advantage of specialized
processing units, such as NVIDIA Tensor Cores for much higher arithmetic
throughput. However, due to the smaller representable range, performing the
entire training with float16 can result in gradient underflow, that is, small
gradient values becoming zeroes. Instead, performing only select arithmetic
operations in float16 results in higher throughput and decreased training
time when using compatible hardware accelerators while also reducing memory
usage, typically without sacrificing model accuracy. <p></p> Note: While the mixed precision rewrite changes the datatype of various
layers throughout the model, the same accuracy reached in float32 is
expected. If a `NaN` gradient occurs with dynamic loss scaling, the model
update for that batch is skipped. In this case, the global step count is not
incremented, and the `LossScaleOptimizer` attempts to decrease the loss
scaling value to avoid `NaN` values in subsequent iterations. This approach
has been shown to achieve the same accuracy as float32 and, in most cases,
better training throughput. <p></p> Example:
For a complete example showing the speed-up on training an image
classification task on CIFAR10, check out this
<a href="https://colab.research.google.com/github/NVIDIA/
DeepLearningExamples/blob/master/TensorFlow/docs/amp/notebook_v1.14/
auto_mixed_precision_demo_cifar10.ipynb">Colab notebook</a>. <p></p> Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite
operation before computing gradients. The function additionally returns an
`Optimizer`(`opt`) wrapped with a `LossScaleOptimizer`. This prevents
underflow in the float16 tensors during the backward pass. An optimizer of
type <a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a> or <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> must be passed
to this function, which will then be wrapped to use loss scaling. <p></p> <img src="
http://developer.download.nvidia.com/compute/machine-learning/frameworks/
TF_mixed_precision_training.png" width="500px"> <p></p> The graph rewrite operation changes the `dtype` of certain operations in the
graph from float32 to float16. There are several categories of operations
that are either included or excluded by this rewrite operation. The following
categories of Ops are defined inside corresponding functions under the class
`AutoMixedPrecisionLists` in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/
core/grappler/optimizers/auto_mixed_precision_lists.h">
auto_mixed_precision_lists.h</a>: <p></p> * `ClearList`: Ops that do not have numerically significant adverse effects.
E.g. `ArgMax` and `Floor`.
* `WhiteList`: Ops that are considered numerically safe for execution in
float16, and thus are always converted. E.g. `Conv2D`.
* `BlackList`: Ops that are numerically unsafe to execute in float16 and
can negatively affect downstream nodes. E.g. `Softmax`.
* `GrayList`: Ops that are considered numerically safe for execution in
float16 unless downstream from a BlackList Op. E.g. `Add` and `AvgPool`. <p></p> When this function is used, gradients should only be computed and applied
with the returned optimizer, either by calling `opt.minimize()` or
`opt.compute_gradients()` followed by `opt.apply_gradients()`.
Gradients should not be computed with <a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>.
This is because the returned optimizer will apply loss scaling, and
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a> will not. If you do directly use
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>, your model may not converge due to
float16 underflow problems. <p></p> When eager execution is enabled, the mixed precision graph rewrite is only
enabled within <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, as outside <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, there is no graph. <p></p> For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions
(such as batch size, input size, output size, and channel counts)
should be powers of two if under 256, or  otherwise divisible by 8 if above
256. For more information, check out the
[NVIDIA Deep Learning Performance Guide](
https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html). <p></p> Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with
Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The
parts of the graph on CPUs and TPUs are untouched by the graph rewrite. TPU
support is coming soon. CPUs are not supported, as CPUs do not run float16
operations faster than float32 operations. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> opt
						</dt>
						<dd>An instance of a <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> or a
<a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a>. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> loss_scale
						</dt>
						<dd>Either an int/float, the string `"dynamic"`, or an instance of
a <a href="..\..\..\tf\train\experimental\LossScale.md"><code>tf.train.experimental.LossScale</code></a>. The loss scale to use. It is
recommended to keep this as its default value of `"dynamic"`, which will
adjust the scaling automatically to prevent `Inf` or `NaN` values. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A version of `opt` that will use loss scaling to prevent underflow. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>model = tf.keras.models.Sequential([
             ...
            ]) <p></p> opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) <p></p> model.compile(loss="categorical_crossentropy",
            optimizer=opt,
            metrics=["accuracy"]) <p></p> model.fit(x_train, y_train,
        batch_size=batch_size,
        epochs=epochs) </pre>
</div>
		</div>
	</div>
	<div id="enable_mixed_precision_graph_rewrite" class="method">
		<h4>
			<span title="System.object">object</span> <strong>enable_mixed_precision_graph_rewrite</strong>(<span title="System.int">int</span> opt, <span title="System.string">string</span> loss_scale)
		</h4>
		<div class="content">Enable mixed precision via a graph rewrite. <p></p> Mixed precision is the use of both float32 and float16 data types when
training a model to improve performance. This is achieved via a graph rewrite
operation and a loss-scale optimizer. <p></p> Performing arithmetic operations in float16 takes advantage of specialized
processing units, such as NVIDIA Tensor Cores for much higher arithmetic
throughput. However, due to the smaller representable range, performing the
entire training with float16 can result in gradient underflow, that is, small
gradient values becoming zeroes. Instead, performing only select arithmetic
operations in float16 results in higher throughput and decreased training
time when using compatible hardware accelerators while also reducing memory
usage, typically without sacrificing model accuracy. <p></p> Note: While the mixed precision rewrite changes the datatype of various
layers throughout the model, the same accuracy reached in float32 is
expected. If a `NaN` gradient occurs with dynamic loss scaling, the model
update for that batch is skipped. In this case, the global step count is not
incremented, and the `LossScaleOptimizer` attempts to decrease the loss
scaling value to avoid `NaN` values in subsequent iterations. This approach
has been shown to achieve the same accuracy as float32 and, in most cases,
better training throughput. <p></p> Example:
For a complete example showing the speed-up on training an image
classification task on CIFAR10, check out this
<a href="https://colab.research.google.com/github/NVIDIA/
DeepLearningExamples/blob/master/TensorFlow/docs/amp/notebook_v1.14/
auto_mixed_precision_demo_cifar10.ipynb">Colab notebook</a>. <p></p> Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite
operation before computing gradients. The function additionally returns an
`Optimizer`(`opt`) wrapped with a `LossScaleOptimizer`. This prevents
underflow in the float16 tensors during the backward pass. An optimizer of
type <a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a> or <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> must be passed
to this function, which will then be wrapped to use loss scaling. <p></p> <img src="
http://developer.download.nvidia.com/compute/machine-learning/frameworks/
TF_mixed_precision_training.png" width="500px"> <p></p> The graph rewrite operation changes the `dtype` of certain operations in the
graph from float32 to float16. There are several categories of operations
that are either included or excluded by this rewrite operation. The following
categories of Ops are defined inside corresponding functions under the class
`AutoMixedPrecisionLists` in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/
core/grappler/optimizers/auto_mixed_precision_lists.h">
auto_mixed_precision_lists.h</a>: <p></p> * `ClearList`: Ops that do not have numerically significant adverse effects.
E.g. `ArgMax` and `Floor`.
* `WhiteList`: Ops that are considered numerically safe for execution in
float16, and thus are always converted. E.g. `Conv2D`.
* `BlackList`: Ops that are numerically unsafe to execute in float16 and
can negatively affect downstream nodes. E.g. `Softmax`.
* `GrayList`: Ops that are considered numerically safe for execution in
float16 unless downstream from a BlackList Op. E.g. `Add` and `AvgPool`. <p></p> When this function is used, gradients should only be computed and applied
with the returned optimizer, either by calling `opt.minimize()` or
`opt.compute_gradients()` followed by `opt.apply_gradients()`.
Gradients should not be computed with <a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>.
This is because the returned optimizer will apply loss scaling, and
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a> will not. If you do directly use
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>, your model may not converge due to
float16 underflow problems. <p></p> When eager execution is enabled, the mixed precision graph rewrite is only
enabled within <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, as outside <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, there is no graph. <p></p> For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions
(such as batch size, input size, output size, and channel counts)
should be powers of two if under 256, or  otherwise divisible by 8 if above
256. For more information, check out the
[NVIDIA Deep Learning Performance Guide](
https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html). <p></p> Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with
Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The
parts of the graph on CPUs and TPUs are untouched by the graph rewrite. TPU
support is coming soon. CPUs are not supported, as CPUs do not run float16
operations faster than float32 operations. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> opt
						</dt>
						<dd>An instance of a <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> or a
<a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a>. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> loss_scale
						</dt>
						<dd>Either an int/float, the string `"dynamic"`, or an instance of
a <a href="..\..\..\tf\train\experimental\LossScale.md"><code>tf.train.experimental.LossScale</code></a>. The loss scale to use. It is
recommended to keep this as its default value of `"dynamic"`, which will
adjust the scaling automatically to prevent `Inf` or `NaN` values. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A version of `opt` that will use loss scaling to prevent underflow. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>model = tf.keras.models.Sequential([
             ...
            ]) <p></p> opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) <p></p> model.compile(loss="categorical_crossentropy",
            optimizer=opt,
            metrics=["accuracy"]) <p></p> model.fit(x_train, y_train,
        batch_size=batch_size,
        epochs=epochs) </pre>
</div>
		</div>
	</div>
	<div id="enable_mixed_precision_graph_rewrite_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>enable_mixed_precision_graph_rewrite_dyn</strong>(<span title="System.object">object</span> opt, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_scale)
		</h4>
		<div class="content">Enable mixed precision via a graph rewrite. <p></p> Mixed precision is the use of both float32 and float16 data types when
training a model to improve performance. This is achieved via a graph rewrite
operation and a loss-scale optimizer. <p></p> Performing arithmetic operations in float16 takes advantage of specialized
processing units, such as NVIDIA Tensor Cores for much higher arithmetic
throughput. However, due to the smaller representable range, performing the
entire training with float16 can result in gradient underflow, that is, small
gradient values becoming zeroes. Instead, performing only select arithmetic
operations in float16 results in higher throughput and decreased training
time when using compatible hardware accelerators while also reducing memory
usage, typically without sacrificing model accuracy. <p></p> Note: While the mixed precision rewrite changes the datatype of various
layers throughout the model, the same accuracy reached in float32 is
expected. If a `NaN` gradient occurs with dynamic loss scaling, the model
update for that batch is skipped. In this case, the global step count is not
incremented, and the `LossScaleOptimizer` attempts to decrease the loss
scaling value to avoid `NaN` values in subsequent iterations. This approach
has been shown to achieve the same accuracy as float32 and, in most cases,
better training throughput. <p></p> Example:
For a complete example showing the speed-up on training an image
classification task on CIFAR10, check out this
<a href="https://colab.research.google.com/github/NVIDIA/
DeepLearningExamples/blob/master/TensorFlow/docs/amp/notebook_v1.14/
auto_mixed_precision_demo_cifar10.ipynb">Colab notebook</a>. <p></p> Calling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite
operation before computing gradients. The function additionally returns an
`Optimizer`(`opt`) wrapped with a `LossScaleOptimizer`. This prevents
underflow in the float16 tensors during the backward pass. An optimizer of
type <a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a> or <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> must be passed
to this function, which will then be wrapped to use loss scaling. <p></p> <img src="
http://developer.download.nvidia.com/compute/machine-learning/frameworks/
TF_mixed_precision_training.png" width="500px"> <p></p> The graph rewrite operation changes the `dtype` of certain operations in the
graph from float32 to float16. There are several categories of operations
that are either included or excluded by this rewrite operation. The following
categories of Ops are defined inside corresponding functions under the class
`AutoMixedPrecisionLists` in
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/
core/grappler/optimizers/auto_mixed_precision_lists.h">
auto_mixed_precision_lists.h</a>: <p></p> * `ClearList`: Ops that do not have numerically significant adverse effects.
E.g. `ArgMax` and `Floor`.
* `WhiteList`: Ops that are considered numerically safe for execution in
float16, and thus are always converted. E.g. `Conv2D`.
* `BlackList`: Ops that are numerically unsafe to execute in float16 and
can negatively affect downstream nodes. E.g. `Softmax`.
* `GrayList`: Ops that are considered numerically safe for execution in
float16 unless downstream from a BlackList Op. E.g. `Add` and `AvgPool`. <p></p> When this function is used, gradients should only be computed and applied
with the returned optimizer, either by calling `opt.minimize()` or
`opt.compute_gradients()` followed by `opt.apply_gradients()`.
Gradients should not be computed with <a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>.
This is because the returned optimizer will apply loss scaling, and
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a> will not. If you do directly use
<a href="..\..\..\tf\gradients.md"><code>tf.gradients</code></a> or <a href="..\..\..\tf\GradientTape.md"><code>tf.GradientTape</code></a>, your model may not converge due to
float16 underflow problems. <p></p> When eager execution is enabled, the mixed precision graph rewrite is only
enabled within <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, as outside <a href="..\..\..\tf\function.md"><code>tf.function</code></a>, there is no graph. <p></p> For NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions
(such as batch size, input size, output size, and channel counts)
should be powers of two if under 256, or  otherwise divisible by 8 if above
256. For more information, check out the
[NVIDIA Deep Learning Performance Guide](
https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html). <p></p> Currently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with
Compute Capability 7.0 and above (Volta, Turing, or newer architectures). The
parts of the graph on CPUs and TPUs are untouched by the graph rewrite. TPU
support is coming soon. CPUs are not supported, as CPUs do not run float16
operations faster than float32 operations. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> opt
						</dt>
						<dd>An instance of a <a href="..\..\..\tf\keras\optimizers\Optimizer.md"><code>tf.keras.optimizers.Optimizer</code></a> or a
<a href="..\..\..\tf\train\Optimizer.md"><code>tf.train.Optimizer</code></a>. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_scale
						</dt>
						<dd>Either an int/float, the string `"dynamic"`, or an instance of
a <a href="..\..\..\tf\train\experimental\LossScale.md"><code>tf.train.experimental.LossScale</code></a>. The loss scale to use. It is
recommended to keep this as its default value of `"dynamic"`, which will
adjust the scaling automatically to prevent `Inf` or `NaN` values. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A version of `opt` that will use loss scaling to prevent underflow. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>model = tf.keras.models.Sequential([
             ...
            ]) <p></p> opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) <p></p> model.compile(loss="categorical_crossentropy",
            optimizer=opt,
            metrics=["accuracy"]) <p></p> model.fit(x_train, y_train,
        batch_size=batch_size,
        epochs=epochs) </pre>
</div>
		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="disable_mixed_precision_graph_rewrite_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>disable_mixed_precision_graph_rewrite_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="enable_mixed_precision_graph_rewrite_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>enable_mixed_precision_graph_rewrite_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>