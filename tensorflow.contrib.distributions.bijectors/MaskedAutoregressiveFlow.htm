<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>MaskedAutoregressiveFlow - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.contrib.distributions.bijectors</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/AbsoluteValue.htm">AbsoluteValue</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Affine.htm">Affine</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/AffineLinearOperator.htm">AffineLinearOperator</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/AffineScalar.htm">AffineScalar</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/BatchNormalization.htm">BatchNormalization</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/bijectors.htm">bijectors</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Chain.htm">Chain</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/CholeskyOuterProduct.htm">CholeskyOuterProduct</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ConditionalBijector.htm">ConditionalBijector</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Exp.htm">Exp</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/FillTriangular.htm">FillTriangular</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Gumbel.htm">Gumbel</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IAbsoluteValue.htm">IAbsoluteValue</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IAffine.htm">IAffine</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IAffineLinearOperator.htm">IAffineLinearOperator</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IAffineScalar.htm">IAffineScalar</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IBatchNormalization.htm">IBatchNormalization</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IChain.htm">IChain</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ICholeskyOuterProduct.htm">ICholeskyOuterProduct</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IConditionalBijector.htm">IConditionalBijector</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IExp.htm">IExp</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IFillTriangular.htm">IFillTriangular</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IGumbel.htm">IGumbel</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IInline.htm">IInline</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IInvert.htm">IInvert</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IKumaraswamy.htm">IKumaraswamy</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IMaskedAutoregressiveFlow.htm">IMaskedAutoregressiveFlow</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IMatrixInverseTriL.htm">IMatrixInverseTriL</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Inline.htm">Inline</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Invert.htm">Invert</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IOrdered.htm">IOrdered</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IPermute.htm">IPermute</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IPowerTransform.htm">IPowerTransform</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IRealNVP.htm">IRealNVP</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IReshape.htm">IReshape</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/IScaleTriL.htm">IScaleTriL</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ISigmoid.htm">ISigmoid</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ISinhArcsinh.htm">ISinhArcsinh</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ISoftmaxCentered.htm">ISoftmaxCentered</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ISoftplus.htm">ISoftplus</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ISoftsign.htm">ISoftsign</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ISquare.htm">ISquare</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ITransformDiagonal.htm">ITransformDiagonal</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Kumaraswamy.htm">Kumaraswamy</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm" class="current">MaskedAutoregressiveFlow</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/MatrixInverseTriL.htm">MatrixInverseTriL</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Ordered.htm">Ordered</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Permute.htm">Permute</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/PowerTransform.htm">PowerTransform</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/RealNVP.htm">RealNVP</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Reshape.htm">Reshape</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/ScaleTriL.htm">ScaleTriL</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Sigmoid.htm">Sigmoid</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/SinhArcsinh.htm">SinhArcsinh</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/SoftmaxCentered.htm">SoftmaxCentered</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Softplus.htm">Softplus</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Softsign.htm">Softsign</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/Square.htm">Square</a>
        </li>
				<li>
            <a href="../tensorflow.contrib.distributions.bijectors/TransformDiagonal.htm">TransformDiagonal</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> MaskedAutoregressiveFlow</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.contrib.distributions.bijectors</p>
		<p><strong>Parent</strong> <a href="../tensorflow.python.ops.distributions.bijector/Bijector.htm">Bijector</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.contrib.distributions.bijectors/IMaskedAutoregressiveFlow.htm">IMaskedAutoregressiveFlow</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">Affine MaskedAutoregressiveFlow bijector for vector-valued events. <p></p> The affine autoregressive flow [(Papamakarios et al., 2016)][3] provides a
relatively simple framework for user-specified (deep) architectures to learn
a distribution over vector-valued events. Regarding terminology, <p></p> "Autoregressive models decompose the joint density as a product of
conditionals, and model each conditional in turn. Normalizing flows
transform a base density (e.g. a standard Gaussian) into the target density
by an invertible transformation with tractable Jacobian."
[(Papamakarios et al., 2016)][3] <p></p> In other words, the "autoregressive property" is equivalent to the
decomposition, `p(x) = prod{ p(x[i] | x[0:i]) : i=0,..., d }`. The provided
`shift_and_log_scale_fn`, `masked_autoregressive_default_template`, achieves
this property by zeroing out weights in its `masked_dense` layers. <p></p> In the `tfp` framework, a "normalizing flow" is implemented as a
`tfp.bijectors.Bijector`. The `forward` "autoregression"
is implemented using a <a href="..\..\..\..\tf\while_loop.md"><code>tf.while_loop</code></a> and a deep neural network (DNN) with
masked weights such that the autoregressive property is automatically met in
the `inverse`. <p></p> A `TransformedDistribution` using `MaskedAutoregressiveFlow(...)` uses the
(expensive) forward-mode calculation to draw samples and the (cheap)
reverse-mode calculation to compute log-probabilities. Conversely, a
`TransformedDistribution` using `Invert(MaskedAutoregressiveFlow(...))` uses
the (expensive) forward-mode calculation to compute log-probabilities and the
(cheap) reverse-mode calculation to compute samples.  See "Example Use"
[below] for more details. <p></p> Given a `shift_and_log_scale_fn`, the forward and inverse transformations are
(a sequence of) affine transformations. A "valid" `shift_and_log_scale_fn`
must compute each `shift` (aka `loc` or "mu" in [Germain et al. (2015)][1])
and `log(scale)` (aka "alpha" in [Germain et al. (2015)][1]) such that each
are broadcastable with the arguments to `forward` and `inverse`, i.e., such
that the calculations in `forward`, `inverse` [below] are possible. <p></p> For convenience, `masked_autoregressive_default_template` is offered as a
possible `shift_and_log_scale_fn` function. It implements the MADE
architecture [(Germain et al., 2015)][1]. MADE is a feed-forward network that
computes a `shift` and `log(scale)` using `masked_dense` layers in a deep
neural network. Weights are masked to ensure the autoregressive property. It
is possible that this architecture is suboptimal for your task. To build
alternative networks, either change the arguments to
`masked_autoregressive_default_template`, use the `masked_dense` function to
roll-out your own, or use some other architecture, e.g., using <a href="..\..\..\..\tf\layers.md"><code>tf.layers</code></a>. <p></p> Warning: no attempt is made to validate that the `shift_and_log_scale_fn`
enforces the "autoregressive property". <p></p> Assuming `shift_and_log_scale_fn` has valid shape and autoregressive
semantics, the forward transformation is
and the inverse transformation is
Notice that the `inverse` does not need a for-loop. This is because in the
forward pass each calculation of `shift` and `log_scale` is based on the `y`
calculated so far (not `x`). In the `inverse`, the `y` is fully known, thus is
equivalent to the scaling used in `forward` after `event_size` passes, i.e.,
the "last" `y` used to compute `shift`, `log_scale`. (Roughly speaking, this
also proves the transform is bijective.) <p></p> #### Examples
#### References <p></p> [1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE:
Masked Autoencoder for Distribution Estimation. In _International
Conference on Machine Learning_, 2015. https://arxiv.org/abs/1502.03509 <p></p> [2]: Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya
Sutskever, and Max Welling. Improving Variational Inference with Inverse
Autoregressive Flow. In _Neural Information Processing Systems_, 2016.
https://arxiv.org/abs/1606.04934 <p></p> [3]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked
Autoregressive Flow for Density Estimation. In _Neural Information
Processing Systems_, 2017. https://arxiv.org/abs/1705.07057 <div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>def forward(x):
              y = zeros_like(x)
              event_size = x.shape[-1]
              for _ in range(event_size):
                shift, log_scale = shift_and_log_scale_fn(y)
                y = x * math_ops.exp(log_scale) + shift
              return y </pre>
</div>
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#NewDyn">NewDyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#dtype">dtype</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#dtype_dyn">dtype_dyn</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#forward_min_event_ndims">forward_min_event_ndims</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#forward_min_event_ndims_dyn">forward_min_event_ndims_dyn</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#graph_parents">graph_parents</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#graph_parents_dyn">graph_parents_dyn</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#inverse_min_event_ndims">inverse_min_event_ndims</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#inverse_min_event_ndims_dyn">inverse_min_event_ndims_dyn</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#is_constant_jacobian">is_constant_jacobian</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#is_constant_jacobian_dyn">is_constant_jacobian_dyn</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#name">name</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#name_dyn">name_dyn</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#PythonObject">PythonObject</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#validate_args">validate_args</a></li>
				<li><a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm#validate_args_dyn">validate_args_dyn</a></li>
			</ul>
		
	</div>
	
	
	<h3 class="section">Public static methods</h3>

	<div id="NewDyn" class="method">
		<h4>
			<a href="../tensorflow.contrib.distributions.bijectors/MaskedAutoregressiveFlow.htm">MaskedAutoregressiveFlow</a> <strong>NewDyn</strong>(<span title="System.object">object</span> shift_and_log_scale_fn, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> is_constant_jacobian, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> validate_args, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> unroll_loop, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Creates the MaskedAutoregressiveFlow bijector. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed after 2018-10-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of <a href="..\..\..\..\tf\contrib\distributions.md"><code>tf.contrib.distributions</code></a>. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> shift_and_log_scale_fn
						</dt>
						<dd>Python `callable` which computes `shift` and
`log_scale` from both the forward domain (`x`) and the inverse domain
(`y`). Calculation must respect the "autoregressive property" (see class
docstring). Suggested default
`masked_autoregressive_default_template(hidden_layers=...)`. Typically
the function contains `tf.Variables` and is wrapped using
`tf.compat.v1.make_template`. Returning `None` for either (both)
`shift`, `log_scale` is equivalent to (but more efficient than)
returning zero. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> is_constant_jacobian
						</dt>
						<dd>Python `bool`. Default: `False`. When `True` the
implementation assumes `log_scale` does not depend on the forward domain
(`x`) or inverse domain (`y`) values. (No validation is made;
`is_constant_jacobian=False` is always safe but possibly computationally
inefficient.) 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> validate_args
						</dt>
						<dd>Python `bool` indicating whether arguments should be
checked for correctness. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> unroll_loop
						</dt>
						<dd>Python `bool` indicating whether the <a href="..\..\..\..\tf\while_loop.md"><code>tf.while_loop</code></a> in
`_forward` should be replaced with a static for loop. Requires that the
final dimension of `x` be known at graph construction time. Defaults to
`False`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>Python `str`, name given to ops managed by this object. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="dtype" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dtype</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="dtype_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dtype_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="forward_min_event_ndims" class="method">
		<h4>
			<span title="System.object">object</span> <strong>forward_min_event_ndims</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="forward_min_event_ndims_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>forward_min_event_ndims_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="graph_parents" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>graph_parents</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="graph_parents_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>graph_parents_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="inverse_min_event_ndims" class="method">
		<h4>
			<span title="System.object">object</span> <strong>inverse_min_event_ndims</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="inverse_min_event_ndims_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>inverse_min_event_ndims_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="is_constant_jacobian" class="method">
		<h4>
			<span title="System.bool">bool</span> <strong>is_constant_jacobian</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="is_constant_jacobian_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>is_constant_jacobian_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="name" class="method">
		<h4>
			<span title="System.object">object</span> <strong>name</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="name_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>name_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="validate_args" class="method">
		<h4>
			<span title="System.bool">bool</span> <strong>validate_args</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="validate_args_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>validate_args_dyn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>