<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>tf.train - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow/AggregationMethod.htm">AggregationMethod</a>
        </li>
				<li>
            <a href="../tensorflow/ConditionalAccumulator.htm">ConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/ConditionalAccumulatorBase.htm">ConditionalAccumulatorBase</a>
        </li>
				<li>
            <a href="../tensorflow/constant_initializer.htm">constant_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/CriticalSection.htm">CriticalSection</a>
        </li>
				<li>
            <a href="../tensorflow/DeviceSpec.htm">DeviceSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Dimension.htm">Dimension</a>
        </li>
				<li>
            <a href="../tensorflow/DType.htm">DType</a>
        </li>
				<li>
            <a href="../tensorflow/FIFOQueue.htm">FIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLenFeature.htm">FixedLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLengthRecordReader.htm">FixedLengthRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLenSequenceFeature.htm">FixedLenSequenceFeature</a>
        </li>
				<li>
            <a href="../tensorflow/glorot_normal_initializer.htm">glorot_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/glorot_uniform_initializer.htm">glorot_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/GradientTape.htm">GradientTape</a>
        </li>
				<li>
            <a href="../tensorflow/Graph.htm">Graph</a>
        </li>
				<li>
            <a href="../tensorflow/Graph._ControlDependenciesController.htm">Graph._ControlDependenciesController</a>
        </li>
				<li>
            <a href="../tensorflow/Graph.I_ControlDependenciesController.htm">Graph.I_ControlDependenciesController</a>
        </li>
				<li>
            <a href="../tensorflow/GraphKeys.htm">GraphKeys</a>
        </li>
				<li>
            <a href="../tensorflow/HeadingAxes.htm">HeadingAxes</a>
        </li>
				<li>
            <a href="../tensorflow/IAggregationMethod.htm">IAggregationMethod</a>
        </li>
				<li>
            <a href="../tensorflow/IConditionalAccumulator.htm">IConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/IConditionalAccumulatorBase.htm">IConditionalAccumulatorBase</a>
        </li>
				<li>
            <a href="../tensorflow/Iconstant_initializer.htm">Iconstant_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/ICriticalSection.htm">ICriticalSection</a>
        </li>
				<li>
            <a href="../tensorflow/IdentityReader.htm">IdentityReader</a>
        </li>
				<li>
            <a href="../tensorflow/IDeviceSpec.htm">IDeviceSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IDimension.htm">IDimension</a>
        </li>
				<li>
            <a href="../tensorflow/IDType.htm">IDType</a>
        </li>
				<li>
            <a href="../tensorflow/IFIFOQueue.htm">IFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLenFeature.htm">IFixedLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLengthRecordReader.htm">IFixedLengthRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLenSequenceFeature.htm">IFixedLenSequenceFeature</a>
        </li>
				<li>
            <a href="../tensorflow/Iglorot_normal_initializer.htm">Iglorot_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Iglorot_uniform_initializer.htm">Iglorot_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IGradientTape.htm">IGradientTape</a>
        </li>
				<li>
            <a href="../tensorflow/IGraph.htm">IGraph</a>
        </li>
				<li>
            <a href="../tensorflow/IGraphKeys.htm">IGraphKeys</a>
        </li>
				<li>
            <a href="../tensorflow/IIdentityReader.htm">IIdentityReader</a>
        </li>
				<li>
            <a href="../tensorflow/IIndexedSlices.htm">IIndexedSlices</a>
        </li>
				<li>
            <a href="../tensorflow/IIndexedSlicesSpec.htm">IIndexedSlicesSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IInteractiveSession.htm">IInteractiveSession</a>
        </li>
				<li>
            <a href="../tensorflow/ILazyLoader.htm">ILazyLoader</a>
        </li>
				<li>
            <a href="../tensorflow/ILMDBReader.htm">ILMDBReader</a>
        </li>
				<li>
            <a href="../tensorflow/IModule.htm">IModule</a>
        </li>
				<li>
            <a href="../tensorflow/Iname_scope.htm">Iname_scope</a>
        </li>
				<li>
            <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a>
        </li>
				<li>
            <a href="../tensorflow/IndexedSlicesSpec.htm">IndexedSlicesSpec</a>
        </li>
				<li>
            <a href="../tensorflow/InteractiveSession.htm">InteractiveSession</a>
        </li>
				<li>
            <a href="../tensorflow/Iones_initializer.htm">Iones_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IOperation.htm">IOperation</a>
        </li>
				<li>
            <a href="../tensorflow/IOpError.htm">IOpError</a>
        </li>
				<li>
            <a href="../tensorflow/IOptionalSpec.htm">IOptionalSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Iorthogonal_initializer.htm">Iorthogonal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IPaddingFIFOQueue.htm">IPaddingFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IPriorityQueue.htm">IPriorityQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IQueueBase.htm">IQueueBase</a>
        </li>
				<li>
            <a href="../tensorflow/IRaggedTensor.htm">IRaggedTensor</a>
        </li>
				<li>
            <a href="../tensorflow/IRaggedTensorSpec.htm">IRaggedTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Irandom_normal_initializer.htm">Irandom_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Irandom_uniform_initializer.htm">Irandom_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IRandomShuffleQueue.htm">IRandomShuffleQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IReaderBase.htm">IReaderBase</a>
        </li>
				<li>
            <a href="../tensorflow/IRegisterGradient.htm">IRegisterGradient</a>
        </li>
				<li>
            <a href="../tensorflow/ISession.htm">ISession</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseConditionalAccumulator.htm">ISparseConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseFeature.htm">ISparseFeature</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensor.htm">ISparseTensor</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensorSpec.htm">ISparseTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensorValue.htm">ISparseTensorValue</a>
        </li>
				<li>
            <a href="../tensorflow/ITensor.htm">ITensor</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorArray.htm">ITensorArray</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorArraySpec.htm">ITensorArraySpec</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorShape.htm">ITensorShape</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorSpec.htm">ITensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/ITextLineReader.htm">ITextLineReader</a>
        </li>
				<li>
            <a href="../tensorflow/ITFRecordReader.htm">ITFRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/Itruncated_normal_initializer.htm">Itruncated_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/ITypeSpec.htm">ITypeSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IUnconnectedGradients.htm">IUnconnectedGradients</a>
        </li>
				<li>
            <a href="../tensorflow/Iuniform_unit_scaling_initializer.htm">Iuniform_unit_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IVariable.htm">IVariable</a>
        </li>
				<li>
            <a href="../tensorflow/Ivariable_scope.htm">Ivariable_scope</a>
        </li>
				<li>
            <a href="../tensorflow/IVariableScope.htm">IVariableScope</a>
        </li>
				<li>
            <a href="../tensorflow/Ivariance_scaling_initializer.htm">Ivariance_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IVarLenFeature.htm">IVarLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/IWholeFileReader.htm">IWholeFileReader</a>
        </li>
				<li>
            <a href="../tensorflow/Izeros_initializer.htm">Izeros_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/LazyLoader.htm">LazyLoader</a>
        </li>
				<li>
            <a href="../tensorflow/LMDBReader.htm">LMDBReader</a>
        </li>
				<li>
            <a href="../tensorflow/Module.htm">Module</a>
        </li>
				<li>
            <a href="../tensorflow/name_scope.htm">name_scope</a>
        </li>
				<li>
            <a href="../tensorflow/ones_initializer.htm">ones_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Operation.htm">Operation</a>
        </li>
				<li>
            <a href="../tensorflow/Operation._InputList.htm">Operation._InputList</a>
        </li>
				<li>
            <a href="../tensorflow/Operation.I_InputList.htm">Operation.I_InputList</a>
        </li>
				<li>
            <a href="../tensorflow/OpError.htm">OpError</a>
        </li>
				<li>
            <a href="../tensorflow/OptionalSpec.htm">OptionalSpec</a>
        </li>
				<li>
            <a href="../tensorflow/orthogonal_initializer.htm">orthogonal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/PaddingFIFOQueue.htm">PaddingFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/PriorityQueue.htm">PriorityQueue</a>
        </li>
				<li>
            <a href="../tensorflow/QueueBase.htm">QueueBase</a>
        </li>
				<li>
            <a href="../tensorflow/RaggedTensor.htm">RaggedTensor</a>
        </li>
				<li>
            <a href="../tensorflow/RaggedTensorSpec.htm">RaggedTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/random_normal_initializer.htm">random_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/random_uniform_initializer.htm">random_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/RandomShuffleQueue.htm">RandomShuffleQueue</a>
        </li>
				<li>
            <a href="../tensorflow/ReaderBase.htm">ReaderBase</a>
        </li>
				<li>
            <a href="../tensorflow/RegisterGradient.htm">RegisterGradient</a>
        </li>
				<li>
            <a href="../tensorflow/Session.htm">Session</a>
        </li>
				<li>
            <a href="../tensorflow/SparseConditionalAccumulator.htm">SparseConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/SparseFeature.htm">SparseFeature</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensor.htm">SparseTensor</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensorSpec.htm">SparseTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensorValue.htm">SparseTensorValue</a>
        </li>
				<li>
            <a href="../tensorflow/Tensor.htm">Tensor</a>
        </li>
				<li>
            <a href="../tensorflow/Tensor`1.htm">Tensor&lt;T&gt;</a>
        </li>
				<li>
            <a href="../tensorflow/TensorArray.htm">TensorArray</a>
        </li>
				<li>
            <a href="../tensorflow/TensorArraySpec.htm">TensorArraySpec</a>
        </li>
				<li>
            <a href="../tensorflow/TensorDimension.htm">TensorDimension</a>
        </li>
				<li>
            <a href="../tensorflow/TensorDimensionSlice.htm">TensorDimensionSlice</a>
        </li>
				<li>
            <a href="../tensorflow/TensorShape.htm">TensorShape</a>
        </li>
				<li>
            <a href="../tensorflow/TensorSpec.htm">TensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/TextLineReader.htm">TextLineReader</a>
        </li>
				<li>
            <a href="../tensorflow/tf.htm">tf</a>
        </li>
				<li>
            <a href="../tensorflow/tf.audio.htm">tf.audio</a>
        </li>
				<li>
            <a href="../tensorflow/tf.autograph.htm">tf.autograph</a>
        </li>
				<li>
            <a href="../tensorflow/tf.autograph.experimental.htm">tf.autograph.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.bitwise.htm">tf.bitwise</a>
        </li>
				<li>
            <a href="../tensorflow/tf.compat.htm">tf.compat</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.htm">tf.config</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.experimental.htm">tf.config.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.optimizer.htm">tf.config.optimizer</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.threading.htm">tf.config.threading</a>
        </li>
				<li>
            <a href="../tensorflow/tf.data.htm">tf.data</a>
        </li>
				<li>
            <a href="../tensorflow/tf.data.experimental.htm">tf.data.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.debugging.htm">tf.debugging</a>
        </li>
				<li>
            <a href="../tensorflow/tf.distribute.htm">tf.distribute</a>
        </li>
				<li>
            <a href="../tensorflow/tf.distributions.htm">tf.distributions</a>
        </li>
				<li>
            <a href="../tensorflow/tf.errors.htm">tf.errors</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.htm">tf.estimator</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.experimental.htm">tf.estimator.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.export.htm">tf.estimator.export</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.inputs.htm">tf.estimator.inputs</a>
        </li>
				<li>
            <a href="../tensorflow/tf.experimental.htm">tf.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.feature_column.htm">tf.feature_column</a>
        </li>
				<li>
            <a href="../tensorflow/tf.gfile.htm">tf.gfile</a>
        </li>
				<li>
            <a href="../tensorflow/tf.graph_util.htm">tf.graph_util</a>
        </li>
				<li>
            <a href="../tensorflow/tf.image.htm">tf.image</a>
        </li>
				<li>
            <a href="../tensorflow/tf.initializers.htm">tf.initializers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.io.htm">tf.io</a>
        </li>
				<li>
            <a href="../tensorflow/tf.io.gfile.htm">tf.io.gfile</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.htm">tf.keras</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.activations.htm">tf.keras.activations</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.htm">tf.keras.applications</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.densenet.htm">tf.keras.applications.densenet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.imagenet_utils.htm">tf.keras.applications.imagenet_utils</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.inception_resnet_v2.htm">tf.keras.applications.inception_resnet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.inception_v3.htm">tf.keras.applications.inception_v3</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.mobilenet.htm">tf.keras.applications.mobilenet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.mobilenet_v2.htm">tf.keras.applications.mobilenet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.nasnet.htm">tf.keras.applications.nasnet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.resnet.htm">tf.keras.applications.resnet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.resnet_v2.htm">tf.keras.applications.resnet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.vgg16.htm">tf.keras.applications.vgg16</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.vgg19.htm">tf.keras.applications.vgg19</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.xception.htm">tf.keras.applications.xception</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.backend.htm">tf.keras.backend</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.constraints.htm">tf.keras.constraints</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.htm">tf.keras.datasets</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.boston_housing.htm">tf.keras.datasets.boston_housing</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.cifar10.htm">tf.keras.datasets.cifar10</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.cifar100.htm">tf.keras.datasets.cifar100</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.fashion_mnist.htm">tf.keras.datasets.fashion_mnist</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.imdb.htm">tf.keras.datasets.imdb</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.mnist.htm">tf.keras.datasets.mnist</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.reuters.htm">tf.keras.datasets.reuters</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.estimator.htm">tf.keras.estimator</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.experimental.htm">tf.keras.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.initializers.htm">tf.keras.initializers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.layers.htm">tf.keras.layers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.losses.htm">tf.keras.losses</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.metrics.htm">tf.keras.metrics</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.mixed_precision.htm">tf.keras.mixed_precision</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.mixed_precision.experimental.htm">tf.keras.mixed_precision.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.models.htm">tf.keras.models</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.optimizers.htm">tf.keras.optimizers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.optimizers.schedules.htm">tf.keras.optimizers.schedules</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.preprocessing.htm">tf.keras.preprocessing</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.preprocessing.image.htm">tf.keras.preprocessing.image</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.regularizers.htm">tf.keras.regularizers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.utils.htm">tf.keras.utils</a>
        </li>
				<li>
            <a href="../tensorflow/tf.layers.htm">tf.layers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.layers.experimental.htm">tf.layers.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.linalg.htm">tf.linalg</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.htm">tf.lite</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.htm">tf.lite.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.htm">tf.lite.experimental.microfrontend</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.python.htm">tf.lite.experimental.microfrontend.python</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.python.ops.htm">tf.lite.experimental.microfrontend.python.ops</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.nn.htm">tf.lite.experimental.nn</a>
        </li>
				<li>
            <a href="../tensorflow/tf.logging.htm">tf.logging</a>
        </li>
				<li>
            <a href="../tensorflow/tf.losses.htm">tf.losses</a>
        </li>
				<li>
            <a href="../tensorflow/tf.math.htm">tf.math</a>
        </li>
				<li>
            <a href="../tensorflow/tf.metrics.htm">tf.metrics</a>
        </li>
				<li>
            <a href="../tensorflow/tf.nest.htm">tf.nest</a>
        </li>
				<li>
            <a href="../tensorflow/tf.nn.htm">tf.nn</a>
        </li>
				<li>
            <a href="../tensorflow/tf.profiler.htm">tf.profiler</a>
        </li>
				<li>
            <a href="../tensorflow/tf.quantization.htm">tf.quantization</a>
        </li>
				<li>
            <a href="../tensorflow/tf.ragged.htm">tf.ragged</a>
        </li>
				<li>
            <a href="../tensorflow/tf.random.htm">tf.random</a>
        </li>
				<li>
            <a href="../tensorflow/tf.random.experimental.htm">tf.random.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.resource_loader.htm">tf.resource_loader</a>
        </li>
				<li>
            <a href="../tensorflow/tf.saved_model.htm">tf.saved_model</a>
        </li>
				<li>
            <a href="../tensorflow/tf.saved_model.main_op.htm">tf.saved_model.main_op</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sets.htm">tf.sets</a>
        </li>
				<li>
            <a href="../tensorflow/tf.signal.htm">tf.signal</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sparse.htm">tf.sparse</a>
        </li>
				<li>
            <a href="../tensorflow/tf.strings.htm">tf.strings</a>
        </li>
				<li>
            <a href="../tensorflow/tf.summary.htm">tf.summary</a>
        </li>
				<li>
            <a href="../tensorflow/tf.summary.experimental.htm">tf.summary.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sysconfig.htm">tf.sysconfig</a>
        </li>
				<li>
            <a href="../tensorflow/tf.test.htm">tf.test</a>
        </li>
				<li>
            <a href="../tensorflow/tf.tpu.htm">tf.tpu</a>
        </li>
				<li>
            <a href="../tensorflow/tf.tpu.experimental.htm">tf.tpu.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.train.htm" class="current">tf.train</a>
        </li>
				<li>
            <a href="../tensorflow/tf.train.experimental.htm">tf.train.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.user_ops.htm">tf.user_ops</a>
        </li>
				<li>
            <a href="../tensorflow/tf.xla.htm">tf.xla</a>
        </li>
				<li>
            <a href="../tensorflow/tf.xla.experimental.htm">tf.xla.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/TFRecordReader.htm">TFRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/truncated_normal_initializer.htm">truncated_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/TypeSpec.htm">TypeSpec</a>
        </li>
				<li>
            <a href="../tensorflow/UnconnectedGradients.htm">UnconnectedGradients</a>
        </li>
				<li>
            <a href="../tensorflow/uniform_unit_scaling_initializer.htm">uniform_unit_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Variable.htm">Variable</a>
        </li>
				<li>
            <a href="../tensorflow/variable_scope.htm">variable_scope</a>
        </li>
				<li>
            <a href="../tensorflow/VariableAggregation.htm">VariableAggregation</a>
        </li>
				<li>
            <a href="../tensorflow/VariableScope.htm">VariableScope</a>
        </li>
				<li>
            <a href="../tensorflow/VariableSynchronization.htm">VariableSynchronization</a>
        </li>
				<li>
            <a href="../tensorflow/variance_scaling_initializer.htm">variance_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/VarLenFeature.htm">VarLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/WholeFileReader.htm">WholeFileReader</a>
        </li>
				<li>
            <a href="../tensorflow/zeros_initializer.htm">zeros_initializer</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> tf.train</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow</p>
		</header>
    <div class="sub-header">
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow/tf.train.htm#add_queue_runner">add_queue_runner</a></li>
				<li><a href="../tensorflow/tf.train.htm#add_queue_runner_dyn">add_queue_runner_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#assert_global_step">assert_global_step</a></li>
				<li><a href="../tensorflow/tf.train.htm#assert_global_step">assert_global_step</a></li>
				<li><a href="../tensorflow/tf.train.htm#basic_train_loop">basic_train_loop</a></li>
				<li><a href="../tensorflow/tf.train.htm#basic_train_loop_dyn">basic_train_loop_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch">batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch">batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_dyn">batch_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_join">batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_join">batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_join">batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_join">batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_join_dyn">batch_join_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#checkpoint_exists">checkpoint_exists</a></li>
				<li><a href="../tensorflow/tf.train.htm#checkpoint_exists">checkpoint_exists</a></li>
				<li><a href="../tensorflow/tf.train.htm#checkpoint_exists">checkpoint_exists</a></li>
				<li><a href="../tensorflow/tf.train.htm#checkpoint_exists">checkpoint_exists</a></li>
				<li><a href="../tensorflow/tf.train.htm#checkpoints_iterator">checkpoints_iterator</a></li>
				<li><a href="../tensorflow/tf.train.htm#cosine_decay">cosine_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#cosine_decay_dyn">cosine_decay_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#cosine_decay_restarts">cosine_decay_restarts</a></li>
				<li><a href="../tensorflow/tf.train.htm#cosine_decay_restarts">cosine_decay_restarts</a></li>
				<li><a href="../tensorflow/tf.train.htm#cosine_decay_restarts_dyn">cosine_decay_restarts_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#do_quantize_training_on_graphdef">do_quantize_training_on_graphdef</a></li>
				<li><a href="../tensorflow/tf.train.htm#do_quantize_training_on_graphdef_dyn">do_quantize_training_on_graphdef_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#exponential_decay">exponential_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#exponential_decay">exponential_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#exponential_decay">exponential_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#exponential_decay_dyn">exponential_decay_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#export_meta_graph">export_meta_graph</a></li>
				<li><a href="../tensorflow/tf.train.htm#export_meta_graph">export_meta_graph</a></li>
				<li><a href="../tensorflow/tf.train.htm#export_meta_graph_dyn">export_meta_graph_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#generate_checkpoint_state_proto">generate_checkpoint_state_proto</a></li>
				<li><a href="../tensorflow/tf.train.htm#generate_checkpoint_state_proto">generate_checkpoint_state_proto</a></li>
				<li><a href="../tensorflow/tf.train.htm#generate_checkpoint_state_proto">generate_checkpoint_state_proto</a></li>
				<li><a href="../tensorflow/tf.train.htm#generate_checkpoint_state_proto">generate_checkpoint_state_proto</a></li>
				<li><a href="../tensorflow/tf.train.htm#generate_checkpoint_state_proto_dyn">generate_checkpoint_state_proto_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_checkpoint_mtimes">get_checkpoint_mtimes</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_checkpoint_mtimes_dyn">get_checkpoint_mtimes_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_checkpoint_state">get_checkpoint_state</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_checkpoint_state">get_checkpoint_state</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_checkpoint_state_dyn">get_checkpoint_state_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_global_step">get_global_step</a></li>
				<li><a href="../tensorflow/tf.train.htm#global_step">global_step</a></li>
				<li><a href="../tensorflow/tf.train.htm#global_step">global_step</a></li>
				<li><a href="../tensorflow/tf.train.htm#global_step">global_step</a></li>
				<li><a href="../tensorflow/tf.train.htm#global_step">global_step</a></li>
				<li><a href="../tensorflow/tf.train.htm#global_step_dyn">global_step_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#import_meta_graph">import_meta_graph</a></li>
				<li><a href="../tensorflow/tf.train.htm#import_meta_graph">import_meta_graph</a></li>
				<li><a href="../tensorflow/tf.train.htm#import_meta_graph">import_meta_graph</a></li>
				<li><a href="../tensorflow/tf.train.htm#import_meta_graph_dyn">import_meta_graph_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#input_producer">input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#input_producer">input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#input_producer">input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#input_producer">input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#input_producer_dyn">input_producer_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#inverse_time_decay">inverse_time_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#inverse_time_decay_dyn">inverse_time_decay_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#latest_checkpoint">latest_checkpoint</a></li>
				<li><a href="../tensorflow/tf.train.htm#latest_checkpoint">latest_checkpoint</a></li>
				<li><a href="../tensorflow/tf.train.htm#latest_checkpoint_dyn">latest_checkpoint_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#limit_epochs">limit_epochs</a></li>
				<li><a href="../tensorflow/tf.train.htm#limit_epochs">limit_epochs</a></li>
				<li><a href="../tensorflow/tf.train.htm#limit_epochs_dyn">limit_epochs_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#linear_cosine_decay">linear_cosine_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#linear_cosine_decay">linear_cosine_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#linear_cosine_decay_dyn">linear_cosine_decay_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#list_variables">list_variables</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch">maybe_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch">maybe_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch">maybe_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch">maybe_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch">maybe_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch">maybe_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch_dyn">maybe_batch_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch_join">maybe_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch_join">maybe_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch_join">maybe_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch_join_dyn">maybe_batch_join_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch">maybe_shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch">maybe_shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch">maybe_shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch">maybe_shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch">maybe_shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch">maybe_shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch_dyn">maybe_shuffle_batch_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch_join">maybe_shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch_join">maybe_shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch_join">maybe_shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch_join_dyn">maybe_shuffle_batch_join_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession">MonitoredTrainingSession</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession_dyn">MonitoredTrainingSession_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#natural_exp_decay">natural_exp_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#natural_exp_decay_dyn">natural_exp_decay_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#noisy_linear_cosine_decay">noisy_linear_cosine_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#noisy_linear_cosine_decay">noisy_linear_cosine_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#noisy_linear_cosine_decay_dyn">noisy_linear_cosine_decay_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#piecewise_constant">piecewise_constant</a></li>
				<li><a href="../tensorflow/tf.train.htm#piecewise_constant_dyn">piecewise_constant_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#polynomial_decay">polynomial_decay</a></li>
				<li><a href="../tensorflow/tf.train.htm#polynomial_decay_dyn">polynomial_decay_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#range_input_producer">range_input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#range_input_producer_dyn">range_input_producer_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#remove_checkpoint">remove_checkpoint</a></li>
				<li><a href="../tensorflow/tf.train.htm#remove_checkpoint">remove_checkpoint</a></li>
				<li><a href="../tensorflow/tf.train.htm#remove_checkpoint">remove_checkpoint</a></li>
				<li><a href="../tensorflow/tf.train.htm#remove_checkpoint_dyn">remove_checkpoint_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter">replica_device_setter</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter_dyn">replica_device_setter_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_fprint">sdca_fprint</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_fprint_dyn">sdca_fprint_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer">sdca_optimizer</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer_dyn">sdca_optimizer_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_shrink_l1">sdca_shrink_l1</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_shrink_l1">sdca_shrink_l1</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_shrink_l1_dyn">sdca_shrink_l1_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch">shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch">shuffle_batch</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_dyn">shuffle_batch_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join">shuffle_batch_join</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join_dyn">shuffle_batch_join_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#slice_input_producer">slice_input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#slice_input_producer_dyn">slice_input_producer_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#start_queue_runners">start_queue_runners</a></li>
				<li><a href="../tensorflow/tf.train.htm#start_queue_runners">start_queue_runners</a></li>
				<li><a href="../tensorflow/tf.train.htm#start_queue_runners">start_queue_runners</a></li>
				<li><a href="../tensorflow/tf.train.htm#start_queue_runners">start_queue_runners</a></li>
				<li><a href="../tensorflow/tf.train.htm#start_queue_runners">start_queue_runners</a></li>
				<li><a href="../tensorflow/tf.train.htm#start_queue_runners_dyn">start_queue_runners_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#string_input_producer">string_input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#string_input_producer">string_input_producer</a></li>
				<li><a href="../tensorflow/tf.train.htm#string_input_producer_dyn">string_input_producer_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#summary_iterator">summary_iterator</a></li>
				<li><a href="../tensorflow/tf.train.htm#summary_iterator">summary_iterator</a></li>
				<li><a href="../tensorflow/tf.train.htm#summary_iterator_dyn">summary_iterator_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#update_checkpoint_state">update_checkpoint_state</a></li>
				<li><a href="../tensorflow/tf.train.htm#update_checkpoint_state_dyn">update_checkpoint_state_dyn</a></li>
				<li><a href="../tensorflow/tf.train.htm#warm_start">warm_start</a></li>
				<li><a href="../tensorflow/tf.train.htm#warm_start">warm_start</a></li>
				<li><a href="../tensorflow/tf.train.htm#warm_start_dyn">warm_start_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow/tf.train.htm#add_queue_runner_fn">add_queue_runner_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#assert_global_step_fn">assert_global_step_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#basic_train_loop_fn">basic_train_loop_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_fn">batch_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#batch_join_fn">batch_join_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#checkpoint_exists_fn">checkpoint_exists_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#checkpoints_iterator_fn">checkpoints_iterator_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#cosine_decay_fn">cosine_decay_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#cosine_decay_restarts_fn">cosine_decay_restarts_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#create_global_step_fn">create_global_step_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#do_quantize_training_on_graphdef_fn">do_quantize_training_on_graphdef_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#exponential_decay_fn">exponential_decay_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#export_meta_graph_fn">export_meta_graph_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#generate_checkpoint_state_proto_fn">generate_checkpoint_state_proto_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_checkpoint_mtimes_fn">get_checkpoint_mtimes_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_checkpoint_state_fn">get_checkpoint_state_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_global_step__fn">get_global_step__fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#get_or_create_global_step_fn">get_or_create_global_step_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#global_step_fn">global_step_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#import_meta_graph_fn">import_meta_graph_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#init_from_checkpoint_fn">init_from_checkpoint_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#input_producer_fn">input_producer_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#inverse_time_decay_fn">inverse_time_decay_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#latest_checkpoint_fn">latest_checkpoint_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#limit_epochs_fn">limit_epochs_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#linear_cosine_decay_fn">linear_cosine_decay_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#list_variables_fn">list_variables_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#load_checkpoint_fn">load_checkpoint_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#load_variable_fn">load_variable_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch_fn">maybe_batch_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_batch_join_fn">maybe_batch_join_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch_fn">maybe_shuffle_batch_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#maybe_shuffle_batch_join_fn">maybe_shuffle_batch_join_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#MonitoredTrainingSession_fn">MonitoredTrainingSession_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#natural_exp_decay_fn">natural_exp_decay_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#noisy_linear_cosine_decay_fn">noisy_linear_cosine_decay_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#piecewise_constant_fn">piecewise_constant_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#polynomial_decay_fn">polynomial_decay_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#range_input_producer_fn">range_input_producer_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#remove_checkpoint_fn">remove_checkpoint_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#replica_device_setter_fn">replica_device_setter_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_fprint_fn">sdca_fprint_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_optimizer_fn">sdca_optimizer_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#sdca_shrink_l1_fn">sdca_shrink_l1_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_fn">shuffle_batch_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#shuffle_batch_join_fn">shuffle_batch_join_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#slice_input_producer_fn">slice_input_producer_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#start_queue_runners_fn">start_queue_runners_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#string_input_producer_fn">string_input_producer_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#summary_iterator_fn">summary_iterator_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#update_checkpoint_state_fn">update_checkpoint_state_fn</a></li>
				<li><a href="../tensorflow/tf.train.htm#warm_start_fn">warm_start_fn</a></li>
			</ul>
		
	</div>
	
	
	<h3 class="section">Public static methods</h3>

	<div id="add_queue_runner" class="method">
		<h4>
			<span title="System.void">void</span> <strong>add_queue_runner</strong>(<a href="../tensorflow.train/QueueRunner.htm">QueueRunner</a> qr, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Adds a `QueueRunner` to a collection in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> When building a complex model that uses many queues it is often difficult to
gather all the queue runners that need to be run.  This convenience function
allows you to add a queue runner to a well known collection in the graph. <p></p> The companion method `start_queue_runners()` can be used to start threads for
all the collected queue runners. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.train/QueueRunner.htm">QueueRunner</a></code> qr
						</dt>
						<dd>A `QueueRunner`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to add
the queue runner to.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="add_queue_runner_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>add_queue_runner_dyn</strong>(<span title="System.object">object</span> qr, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Adds a `QueueRunner` to a collection in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> When building a complex model that uses many queues it is often difficult to
gather all the queue runners that need to be run.  This convenience function
allows you to add a queue runner to a well known collection in the graph. <p></p> The companion method `start_queue_runners()` can be used to start threads for
all the collected queue runners. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> qr
						</dt>
						<dd>A `QueueRunner`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to add
the queue runner to.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="assert_global_step" class="method">
		<h4>
			<span title="System.void">void</span> <strong>assert_global_step</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> global_step_tensor)
		</h4>
		<div class="content">Asserts `global_step_tensor` is a scalar int `Variable` or `Tensor`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> global_step_tensor
						</dt>
						<dd>`Tensor` to test. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="assert_global_step" class="method">
		<h4>
			<span title="System.void">void</span> <strong>assert_global_step</strong>(<span title="System.object">object</span> global_step_tensor)
		</h4>
		<div class="content">Asserts `global_step_tensor` is a scalar int `Variable` or `Tensor`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> global_step_tensor
						</dt>
						<dd>`Tensor` to test. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="basic_train_loop" class="method">
		<h4>
			<span title="System.void">void</span> <strong>basic_train_loop</strong>(<a href="../tensorflow.train/Supervisor.htm">Supervisor</a> supervisor, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> train_step_fn, <span title="System.Nullable<ValueTuple<Supervisor, string>>">Nullable&lt;ValueTuple&lt;Supervisor, string&gt;&gt;</span> args, <span title="System.Collections.Generic.IDictionary<string, string>">IDictionary&lt;string, string&gt;</span> kwargs, <span title="System.string">string</span> master)
		</h4>
		<div class="content">Basic loop to train a model. <p></p> Calls `train_step_fn` in a loop to train a model.  The function is called as:
It is passed a `tf.compat.v1.Session` in addition to `args` and `kwargs`.  The
function
typically runs one training step in the session. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.train/Supervisor.htm">Supervisor</a></code> supervisor
						</dt>
						<dd>`tf.compat.v1.train.Supervisor` to run the training services. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> train_step_fn
						</dt>
						<dd>Callable to execute one training step.  Called repeatedly as
`train_step_fn(session, *args **kwargs)`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<Supervisor, string>>">Nullable&lt;ValueTuple&lt;Supervisor, string&gt;&gt;</span></code> args
						</dt>
						<dd>Optional positional arguments passed to `train_step_fn`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, string>">IDictionary&lt;string, string&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyword arguments passed to `train_step_fn`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>Master to use to create the training session.  Defaults to `""`
which causes the session to be created in the local process. 
						</dd>
				</dl>
			</div>

<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>train_step_fn(session, *args, **kwargs) </pre>
</div>
		</div>
	</div>
	<div id="basic_train_loop_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>basic_train_loop_dyn</strong>(<span title="System.object">object</span> supervisor, <span title="System.object">object</span> train_step_fn, <span title="System.object">object</span> args, <span title="System.object">object</span> kwargs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> master)
		</h4>
		<div class="content">Basic loop to train a model. <p></p> Calls `train_step_fn` in a loop to train a model.  The function is called as:
It is passed a `tf.compat.v1.Session` in addition to `args` and `kwargs`.  The
function
typically runs one training step in the session. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> supervisor
						</dt>
						<dd>`tf.compat.v1.train.Supervisor` to run the training services. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> train_step_fn
						</dt>
						<dd>Callable to execute one training step.  Called repeatedly as
`train_step_fn(session, *args **kwargs)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> args
						</dt>
						<dd>Optional positional arguments passed to `train_step_fn`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kwargs
						</dt>
						<dd>Optional keyword arguments passed to `train_step_fn`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> master
						</dt>
						<dd>Master to use to create the training session.  Defaults to `""`
which causes the session to be created in the local process. 
						</dd>
				</dl>
			</div>

<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>train_step_fn(session, *args, **kwargs) </pre>
</div>
		</div>
	</div>
	<div id="batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> tensors, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches of tensors in `tensors`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The argument `tensors` can be a list or a dictionary of tensors.
The value returned by the function will be of the same type
as `tensors`. <p></p> This function is implemented using a queue. A `QueueRunner` for the
queue is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> If `enqueue_many` is `False`, `tensors` is assumed to represent a single
example.  An input tensor with shape `[x, y, z]` will be output as a tensor
with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors` is assumed to represent a batch of
examples, where the first dimension is indexed by example, and all members of
`tensors` should have the same size in the first dimension.  If an input
tensor has shape `[*, x, y, z]`, the output will have shape `[batch_size, x,
y, z]`.  The `capacity` argument controls the how long the prefetching is
allowed to grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have shape `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors` (except if
the input is a list of one element, then it returns a tensor, not a list). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch</strong>(<span title="System.Collections.Generic.IDictionary<string, string>">IDictionary&lt;string, string&gt;</span> tensors, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches of tensors in `tensors`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The argument `tensors` can be a list or a dictionary of tensors.
The value returned by the function will be of the same type
as `tensors`. <p></p> This function is implemented using a queue. A `QueueRunner` for the
queue is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> If `enqueue_many` is `False`, `tensors` is assumed to represent a single
example.  An input tensor with shape `[x, y, z]` will be output as a tensor
with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors` is assumed to represent a batch of
examples, where the first dimension is indexed by example, and all members of
`tensors` should have the same size in the first dimension.  If an input
tensor has shape `[*, x, y, z]`, the output will have shape `[batch_size, x,
y, z]`.  The `capacity` argument controls the how long the prefetching is
allowed to grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have shape `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, string>">IDictionary&lt;string, string&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors` (except if
the input is a list of one element, then it returns a tensor, not a list). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_dyn</strong>(<span title="System.object">object</span> tensors, <span title="System.object">object</span> batch_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> num_threads, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dynamic_pad, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Creates batches of tensors in `tensors`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The argument `tensors` can be a list or a dictionary of tensors.
The value returned by the function will be of the same type
as `tensors`. <p></p> This function is implemented using a queue. A `QueueRunner` for the
queue is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> If `enqueue_many` is `False`, `tensors` is assumed to represent a single
example.  An input tensor with shape `[x, y, z]` will be output as a tensor
with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors` is assumed to represent a batch of
examples, where the first dimension is indexed by example, and all members of
`tensors` should have the same size in the first dimension.  If an input
tensor has shape `[*, x, y, z]`, the output will have shape `[batch_size, x,
y, z]`.  The `capacity` argument controls the how long the prefetching is
allowed to grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have shape `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors` (except if
the input is a list of one element, then it returns a tensor, not a list). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> batch_size, <span title="System.int">int</span> capacity, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Runs a list of tensors to fill a queue to create batches of examples. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.batch()`. <p></p> WARNING: This function is nondeterministic, since it starts a separate thread
for each tensor. <p></p> Enqueues a different list of tensors in different threads.
Implemented using a queue -- a `QueueRunner` for the queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> `len(tensors_list)` threads will be started,
with thread `i` enqueuing the tensors from
`tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first
dimension if `enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example. An input tensor `x` will be output as a
tensor with shape `[batch_size] + x.shape`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  The slices of any input tensor
`x` are treated as examples, and the output tensors will have shape
`[batch_size] + x.shape[1:]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors_list` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have value `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> batch_size, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Runs a list of tensors to fill a queue to create batches of examples. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.batch()`. <p></p> WARNING: This function is nondeterministic, since it starts a separate thread
for each tensor. <p></p> Enqueues a different list of tensors in different threads.
Implemented using a queue -- a `QueueRunner` for the queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> `len(tensors_list)` threads will be started,
with thread `i` enqueuing the tensors from
`tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first
dimension if `enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example. An input tensor `x` will be output as a
tensor with shape `[batch_size] + x.shape`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  The slices of any input tensor
`x` are treated as examples, and the output tensors will have shape
`[batch_size] + x.shape[1:]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors_list` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have value `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Runs a list of tensors to fill a queue to create batches of examples. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.batch()`. <p></p> WARNING: This function is nondeterministic, since it starts a separate thread
for each tensor. <p></p> Enqueues a different list of tensors in different threads.
Implemented using a queue -- a `QueueRunner` for the queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> `len(tensors_list)` threads will be started,
with thread `i` enqueuing the tensors from
`tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first
dimension if `enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example. An input tensor `x` will be output as a
tensor with shape `[batch_size] + x.shape`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  The slices of any input tensor
`x` are treated as examples, and the output tensors will have shape
`[batch_size] + x.shape[1:]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors_list` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have value `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Runs a list of tensors to fill a queue to create batches of examples. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.batch()`. <p></p> WARNING: This function is nondeterministic, since it starts a separate thread
for each tensor. <p></p> Enqueues a different list of tensors in different threads.
Implemented using a queue -- a `QueueRunner` for the queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> `len(tensors_list)` threads will be started,
with thread `i` enqueuing the tensors from
`tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first
dimension if `enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example. An input tensor `x` will be output as a
tensor with shape `[batch_size] + x.shape`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  The slices of any input tensor
`x` are treated as examples, and the output tensors will have shape
`[batch_size] + x.shape[1:]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors_list` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have value `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_join_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_join_dyn</strong>(<span title="System.object">object</span> tensors_list, <span title="System.object">object</span> batch_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dynamic_pad, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Runs a list of tensors to fill a queue to create batches of examples. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.batch()`. <p></p> WARNING: This function is nondeterministic, since it starts a separate thread
for each tensor. <p></p> Enqueues a different list of tensors in different threads.
Implemented using a queue -- a `QueueRunner` for the queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. <p></p> `len(tensors_list)` threads will be started,
with thread `i` enqueuing the tensors from
`tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first
dimension if `enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example. An input tensor `x` will be output as a
tensor with shape `[batch_size] + x.shape`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  The slices of any input tensor
`x` are treated as examples, and the output tensors will have shape
`[batch_size] + x.shape[1:]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> *N.B.:* If `dynamic_pad` is `False`, you must ensure that either
(i) the `shapes` argument is passed, or (ii) all of the tensors in
`tensors_list` must have fully-defined shapes. `ValueError` will be
raised if neither of these conditions holds. <p></p> If `dynamic_pad` is `True`, it is sufficient that the *rank* of the
tensors is known, but individual dimensions may have value `None`.
In this case, for each enqueue the dimensions with value `None`
may have a variable length; upon dequeue, the output tensors will be padded
on the right to the maximum shape of the tensors in the current minibatch.
For numbers, this padding takes value 0.  For strings, this padding is
the empty string.  See `PaddingFIFOQueue` for more info. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="checkpoint_exists" class="method">
		<h4>
			<span title="System.bool">bool</span> <strong>checkpoint_exists</strong>(<span title="System.Byte[]">Byte[]</span> checkpoint_prefix)
		</h4>
		<div class="content">Checks whether a V1 or V2 checkpoint exists with the specified prefix. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix. <p></p> This is the recommended way to check if a checkpoint exists, since it takes
into account the naming difference between V1 and V2 formats. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_prefix
						</dt>
						<dd>the prefix of a V1 or V2 checkpoint, with V2 taking
priority.  Typically the result of `Saver.save()` or that of
`tf.train.latest_checkpoint()`, regardless of sharded/non-sharded or
V1/V2. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.bool">bool</span></code>
					</dt>
					<dd>A bool, true if a checkpoint referred to by `checkpoint_prefix` exists. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="checkpoint_exists" class="method">
		<h4>
			<span title="System.bool">bool</span> <strong>checkpoint_exists</strong>(<span title="System.string">string</span> checkpoint_prefix)
		</h4>
		<div class="content">Checks whether a V1 or V2 checkpoint exists with the specified prefix. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix. <p></p> This is the recommended way to check if a checkpoint exists, since it takes
into account the naming difference between V1 and V2 formats. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_prefix
						</dt>
						<dd>the prefix of a V1 or V2 checkpoint, with V2 taking
priority.  Typically the result of `Saver.save()` or that of
`tf.train.latest_checkpoint()`, regardless of sharded/non-sharded or
V1/V2. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.bool">bool</span></code>
					</dt>
					<dd>A bool, true if a checkpoint referred to by `checkpoint_prefix` exists. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="checkpoint_exists" class="method">
		<h4>
			<span title="System.bool">bool</span> <strong>checkpoint_exists</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> checkpoint_prefix)
		</h4>
		<div class="content">Checks whether a V1 or V2 checkpoint exists with the specified prefix. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix. <p></p> This is the recommended way to check if a checkpoint exists, since it takes
into account the naming difference between V1 and V2 formats. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> checkpoint_prefix
						</dt>
						<dd>the prefix of a V1 or V2 checkpoint, with V2 taking
priority.  Typically the result of `Saver.save()` or that of
`tf.train.latest_checkpoint()`, regardless of sharded/non-sharded or
V1/V2. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.bool">bool</span></code>
					</dt>
					<dd>A bool, true if a checkpoint referred to by `checkpoint_prefix` exists. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="checkpoint_exists" class="method">
		<h4>
			<span title="System.bool">bool</span> <strong>checkpoint_exists</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> checkpoint_prefix)
		</h4>
		<div class="content">Checks whether a V1 or V2 checkpoint exists with the specified prefix. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix. <p></p> This is the recommended way to check if a checkpoint exists, since it takes
into account the naming difference between V1 and V2 formats. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> checkpoint_prefix
						</dt>
						<dd>the prefix of a V1 or V2 checkpoint, with V2 taking
priority.  Typically the result of `Saver.save()` or that of
`tf.train.latest_checkpoint()`, regardless of sharded/non-sharded or
V1/V2. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.bool">bool</span></code>
					</dt>
					<dd>A bool, true if a checkpoint referred to by `checkpoint_prefix` exists. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="checkpoints_iterator" class="method">
		<h4>
			<span title="System.Collections.Generic.IEnumerator<object>">IEnumerator&lt;object&gt;</span> <strong>checkpoints_iterator</strong>(<span title="System.string">string</span> checkpoint_dir, <span title="System.int">int</span> min_interval_secs, <span title="System.double">double</span> timeout, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> timeout_fn)
		</h4>
		<div class="content">Continuously yield new checkpoint files as they appear. <p></p> The iterator only checks for new checkpoints when control flow has been
reverted to it. This means it can miss checkpoints if your code takes longer
to run between iterations than `min_interval_secs` or the interval at which
new checkpoints are written. <p></p> The `timeout` argument is the maximum number of seconds to block waiting for
a new checkpoint.  It is used in combination with the `timeout_fn` as
follows: <p></p> * If the timeout expires and no `timeout_fn` was specified, the iterator
stops yielding.
* If a `timeout_fn` was specified, that function is called and if it returns
a true boolean value the iterator stops yielding.
* If the function returns a false boolean value then the iterator resumes the
wait for new checkpoints.  At this point the timeout logic applies again. <p></p> This behavior gives control to callers on what to do if checkpoints do not
come fast enough or stop being generated.  For example, if callers have a way
to detect that the training has stopped and know that no new checkpoints
will be generated, they can provide a `timeout_fn` that returns `True` when
the training has stopped.  If they know that the training is still going on
they return `False` instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>The directory in which checkpoints are saved. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_interval_secs
						</dt>
						<dd>The minimum number of seconds between yielding
checkpoints. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> timeout
						</dt>
						<dd>The maximum number of seconds to wait between checkpoints. If left
as `None`, then the process will wait indefinitely. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> timeout_fn
						</dt>
						<dd>Optional function to call after a timeout.  If the function
returns True, then it means that no new checkpoints will be generated and
the iterator will exit.  The function is called with no arguments. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="cosine_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_decay</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> alpha, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies cosine decay to the learning rate. <p></p> See [Loshchilov & Hutter, ICLR2016], SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a cosine decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> alpha
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number. Minimum
learning rate value as a fraction of learning_rate. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String. Optional name of the operation.  Defaults to 'CosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
            decayed = (1 - alpha) * cosine_decay + alpha
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="cosine_decay_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_decay_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> decay_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> alpha, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies cosine decay to the learning rate. <p></p> See [Loshchilov & Hutter, ICLR2016], SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a cosine decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> alpha
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number. Minimum
learning rate value as a fraction of learning_rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String. Optional name of the operation.  Defaults to 'CosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
            decayed = (1 - alpha) * cosine_decay + alpha
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="cosine_decay_restarts" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_decay_restarts</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> first_decay_steps, <span title="System.double">double</span> t_mul, <span title="System.double">double</span> m_mul, <span title="System.double">double</span> alpha, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies cosine decay with restarts to the learning rate. <p></p> See [Loshchilov & Hutter, ICLR2016], SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a cosine decay function with
restarts to a provided initial learning rate.  It requires a `global_step`
value to compute the decayed learning rate.  You can just pass a TensorFlow
variable that you increment at each training step. <p></p> The function returns the decayed learning rate while taking into account
possible warm restarts. The learning rate multiplier first decays
from 1 to `alpha` for `first_decay_steps` steps. Then, a warm
restart is performed. Each new warm restart runs for `t_mul` times more steps
and with `m_mul` times smaller initial learning rate. <p></p> Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> first_decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number.
Number of steps to decay over. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> t_mul
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number. Used to
derive the number of iterations in the i-th period 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> m_mul
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
Used to derive the initial learning rate of the i-th period: 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> alpha
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number. Minimum
learning rate value as a fraction of the learning_rate. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String. Optional name of the operation.  Defaults to 'SGDRDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>first_decay_steps = 1000
            lr_decayed = cosine_decay_restarts(learning_rate, global_step,
                                               first_decay_steps) </pre>
</div>
		</div>
	</div>
	<div id="cosine_decay_restarts" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_decay_restarts</strong>(<span title="System.double">double</span> learning_rate, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> global_step, <span title="System.int">int</span> first_decay_steps, <span title="System.double">double</span> t_mul, <span title="System.double">double</span> m_mul, <span title="System.double">double</span> alpha, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies cosine decay with restarts to the learning rate. <p></p> See [Loshchilov & Hutter, ICLR2016], SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a cosine decay function with
restarts to a provided initial learning rate.  It requires a `global_step`
value to compute the decayed learning rate.  You can just pass a TensorFlow
variable that you increment at each training step. <p></p> The function returns the decayed learning rate while taking into account
possible warm restarts. The learning rate multiplier first decays
from 1 to `alpha` for `first_decay_steps` steps. Then, a warm
restart is performed. Each new warm restart runs for `t_mul` times more steps
and with `m_mul` times smaller initial learning rate. <p></p> Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> first_decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number.
Number of steps to decay over. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> t_mul
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number. Used to
derive the number of iterations in the i-th period 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> m_mul
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
Used to derive the initial learning rate of the i-th period: 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> alpha
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number. Minimum
learning rate value as a fraction of the learning_rate. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String. Optional name of the operation.  Defaults to 'SGDRDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>first_decay_steps = 1000
            lr_decayed = cosine_decay_restarts(learning_rate, global_step,
                                               first_decay_steps) </pre>
</div>
		</div>
	</div>
	<div id="cosine_decay_restarts_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_decay_restarts_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> first_decay_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> t_mul, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> m_mul, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> alpha, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies cosine decay with restarts to the learning rate. <p></p> See [Loshchilov & Hutter, ICLR2016], SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a cosine decay function with
restarts to a provided initial learning rate.  It requires a `global_step`
value to compute the decayed learning rate.  You can just pass a TensorFlow
variable that you increment at each training step. <p></p> The function returns the decayed learning rate while taking into account
possible warm restarts. The learning rate multiplier first decays
from 1 to `alpha` for `first_decay_steps` steps. Then, a warm
restart is performed. Each new warm restart runs for `t_mul` times more steps
and with `m_mul` times smaller initial learning rate. <p></p> Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> first_decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number.
Number of steps to decay over. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> t_mul
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number. Used to
derive the number of iterations in the i-th period 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> m_mul
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
Used to derive the initial learning rate of the i-th period: 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> alpha
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number. Minimum
learning rate value as a fraction of the learning_rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String. Optional name of the operation.  Defaults to 'SGDRDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>first_decay_steps = 1000
            lr_decayed = cosine_decay_restarts(learning_rate, global_step,
                                               first_decay_steps) </pre>
</div>
		</div>
	</div>
	<div id="do_quantize_training_on_graphdef" class="method">
		<h4>
			<span title="System.object">object</span> <strong>do_quantize_training_on_graphdef</strong>(<span title="System.object">object</span> input_graph, <span title="System.int">int</span> num_bits)
		</h4>
		<div class="content">A general quantization scheme is being developed in <a href="..\..\tf\contrib\quantize.md"><code>tf.contrib.quantize</code></a>. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
GraphDef quantized training rewriter is deprecated in the long term <p></p> Consider using that instead, though since it is in the tf.contrib namespace,
it is not subject to backward compatibility guarantees. 




		</div>
	</div>
	<div id="do_quantize_training_on_graphdef_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>do_quantize_training_on_graphdef_dyn</strong>(<span title="System.object">object</span> input_graph, <span title="System.object">object</span> num_bits)
		</h4>
		<div class="content">A general quantization scheme is being developed in <a href="..\..\tf\contrib\quantize.md"><code>tf.contrib.quantize</code></a>. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
GraphDef quantized training rewriter is deprecated in the long term <p></p> Consider using that instead, though since it is in the tf.contrib namespace,
it is not subject to backward compatibility guarantees. 




		</div>
	</div>
	<div id="exponential_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>exponential_decay</strong>(<span title="System.double">double</span> learning_rate, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> decay_rate, <span title="System.bool">bool</span> staircase, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies exponential decay to the learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an exponential decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
If the argument `staircase` is `True`, then `global_step / decay_steps` is an
integer division and the decayed learning rate follows a staircase function. <p></p> Example: decay every 100000 steps with a base of 0.96: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation.  Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Must
be positive.  See the decay computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> decay_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The decay rate. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> staircase
						</dt>
						<dd>Boolean.  If `True` decay the learning rate at discrete intervals 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'ExponentialDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate *
                                    decay_rate ^ (global_step / decay_steps) </pre>
</div>
		</div>
	</div>
	<div id="exponential_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>exponential_decay</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> decay_rate, <span title="System.bool">bool</span> staircase, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies exponential decay to the learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an exponential decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
If the argument `staircase` is `True`, then `global_step / decay_steps` is an
integer division and the decayed learning rate follows a staircase function. <p></p> Example: decay every 100000 steps with a base of 0.96: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation.  Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Must
be positive.  See the decay computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> decay_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The decay rate. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> staircase
						</dt>
						<dd>Boolean.  If `True` decay the learning rate at discrete intervals 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'ExponentialDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate *
                                    decay_rate ^ (global_step / decay_steps) </pre>
</div>
		</div>
	</div>
	<div id="exponential_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>exponential_decay</strong>(<span title="System.double">double</span> learning_rate, <a href="../tensorflow.python.ops.resource_variable_ops/ResourceVariable.htm">ResourceVariable</a> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> decay_rate, <span title="System.bool">bool</span> staircase, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies exponential decay to the learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an exponential decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
If the argument `staircase` is `True`, then `global_step / decay_steps` is an
integer division and the decayed learning rate follows a staircase function. <p></p> Example: decay every 100000 steps with a base of 0.96: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><a href="../tensorflow.python.ops.resource_variable_ops/ResourceVariable.htm">ResourceVariable</a></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation.  Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Must
be positive.  See the decay computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> decay_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The decay rate. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> staircase
						</dt>
						<dd>Boolean.  If `True` decay the learning rate at discrete intervals 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'ExponentialDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate *
                                    decay_rate ^ (global_step / decay_steps) </pre>
</div>
		</div>
	</div>
	<div id="exponential_decay_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>exponential_decay_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> decay_steps, <span title="System.object">object</span> decay_rate, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> staircase, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies exponential decay to the learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an exponential decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
If the argument `staircase` is `True`, then `global_step / decay_steps` is an
integer division and the decayed learning rate follows a staircase function. <p></p> Example: decay every 100000 steps with a base of 0.96: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation.  Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Must
be positive.  See the decay computation above. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The decay rate. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> staircase
						</dt>
						<dd>Boolean.  If `True` decay the learning rate at discrete intervals 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'ExponentialDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate *
                                    decay_rate ^ (global_step / decay_steps) </pre>
</div>
		</div>
	</div>
	<div id="export_meta_graph" class="method">
		<h4>
			<span title="System.object">object</span> <strong>export_meta_graph</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> filename, <span title="System.object">object</span> meta_info_def, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> graph_def, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> saver_def, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> collection_list, <span title="System.bool">bool</span> as_text, <a href="../tensorflow/Graph.htm">Graph</a> graph, <span title="System.object">object</span> export_scope, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> clear_devices, <span title="System.bool">bool</span> clear_extraneous_savers, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Returns `MetaGraphDef` proto. <p></p> Optionally writes it to filename. <p></p> This function exports the graph, saver, and collection objects into
`MetaGraphDef` protocol buffer with the intention of it being imported
at a later time or location to restart training, run inference, or be
a subgraph. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> filename
						</dt>
						<dd>Optional filename including the path for writing the generated
`MetaGraphDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> meta_info_def
						</dt>
						<dd>`MetaInfoDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> graph_def
						</dt>
						<dd>`GraphDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> saver_def
						</dt>
						<dd>`SaverDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> collection_list
						</dt>
						<dd>List of string keys to collect. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> as_text
						</dt>
						<dd>If `True`, writes the `MetaGraphDef` as an ASCII proto. 
						</dd>
						<dt>
							<code><a href="../tensorflow/Graph.htm">Graph</a></code> graph
						</dt>
						<dd>The `Graph` to export. If `None`, use the default graph. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> export_scope
						</dt>
						<dd>Optional `string`. Name scope under which to extract the
subgraph. The scope name will be striped from the node definitions for
easy import later into new name scopes. If `None`, the whole graph is
exported. graph_def and export_scope cannot both be specified. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during export. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> clear_extraneous_savers
						</dt>
						<dd>Remove any Saver-related information from the graph
(both Save/Restore ops and SaverDefs) that are not associated with the
provided SaverDef. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of filename and with `_debug` added before the
file extend. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyed arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `MetaGraphDef` proto. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="export_meta_graph" class="method">
		<h4>
			<span title="System.object">object</span> <strong>export_meta_graph</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> filename, <span title="System.object">object</span> meta_info_def, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> graph_def, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> saver_def, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> collection_list, <span title="System.bool">bool</span> as_text, <a href="../tensorflow/Graph.htm">Graph</a> graph, <span title="System.object">object</span> export_scope, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> clear_devices, <span title="System.bool">bool</span> clear_extraneous_savers, <a href="../tensorflow.train/Saver.htm">Saver</a> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Returns `MetaGraphDef` proto. <p></p> Optionally writes it to filename. <p></p> This function exports the graph, saver, and collection objects into
`MetaGraphDef` protocol buffer with the intention of it being imported
at a later time or location to restart training, run inference, or be
a subgraph. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> filename
						</dt>
						<dd>Optional filename including the path for writing the generated
`MetaGraphDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> meta_info_def
						</dt>
						<dd>`MetaInfoDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> graph_def
						</dt>
						<dd>`GraphDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> saver_def
						</dt>
						<dd>`SaverDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> collection_list
						</dt>
						<dd>List of string keys to collect. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> as_text
						</dt>
						<dd>If `True`, writes the `MetaGraphDef` as an ASCII proto. 
						</dd>
						<dt>
							<code><a href="../tensorflow/Graph.htm">Graph</a></code> graph
						</dt>
						<dd>The `Graph` to export. If `None`, use the default graph. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> export_scope
						</dt>
						<dd>Optional `string`. Name scope under which to extract the
subgraph. The scope name will be striped from the node definitions for
easy import later into new name scopes. If `None`, the whole graph is
exported. graph_def and export_scope cannot both be specified. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during export. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> clear_extraneous_savers
						</dt>
						<dd>Remove any Saver-related information from the graph
(both Save/Restore ops and SaverDefs) that are not associated with the
provided SaverDef. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Saver.htm">Saver</a></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of filename and with `_debug` added before the
file extend. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyed arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `MetaGraphDef` proto. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="export_meta_graph_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>export_meta_graph_dyn</strong>(<span title="System.object">object</span> filename, <span title="System.object">object</span> meta_info_def, <span title="System.object">object</span> graph_def, <span title="System.object">object</span> saver_def, <span title="System.object">object</span> collection_list, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> as_text, <span title="System.object">object</span> graph, <span title="System.object">object</span> export_scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> clear_devices, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> clear_extraneous_savers, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strip_default_attrs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_debug_info, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Returns `MetaGraphDef` proto. <p></p> Optionally writes it to filename. <p></p> This function exports the graph, saver, and collection objects into
`MetaGraphDef` protocol buffer with the intention of it being imported
at a later time or location to restart training, run inference, or be
a subgraph. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> filename
						</dt>
						<dd>Optional filename including the path for writing the generated
`MetaGraphDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> meta_info_def
						</dt>
						<dd>`MetaInfoDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> graph_def
						</dt>
						<dd>`GraphDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> saver_def
						</dt>
						<dd>`SaverDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> collection_list
						</dt>
						<dd>List of string keys to collect. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> as_text
						</dt>
						<dd>If `True`, writes the `MetaGraphDef` as an ASCII proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> graph
						</dt>
						<dd>The `Graph` to export. If `None`, use the default graph. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> export_scope
						</dt>
						<dd>Optional `string`. Name scope under which to extract the
subgraph. The scope name will be striped from the node definitions for
easy import later into new name scopes. If `None`, the whole graph is
exported. graph_def and export_scope cannot both be specified. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during export. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> clear_extraneous_savers
						</dt>
						<dd>Remove any Saver-related information from the graph
(both Save/Restore ops and SaverDefs) that are not associated with the
provided SaverDef. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of filename and with `_debug` added before the
file extend. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyed arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `MetaGraphDef` proto. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="generate_checkpoint_state_proto" class="method">
		<h4>
			<span title="System.object">object</span> <strong>generate_checkpoint_state_proto</strong>(<span title="System.string">string</span> save_dir, <span title="System.Byte[]">Byte[]</span> model_checkpoint_path, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> all_model_checkpoint_paths, <span title="System.object">object</span> all_model_checkpoint_timestamps, <span title="System.Nullable<double>">Nullable&lt;double&gt;</span> last_preserved_timestamp)
		</h4>
		<div class="content">Generates a checkpoint state proto. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> save_dir
						</dt>
						<dd>Directory where the model was saved. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> model_checkpoint_path
						</dt>
						<dd>The checkpoint file. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> all_model_checkpoint_paths
						</dt>
						<dd>List of strings.  Paths to all not-yet-deleted
checkpoints, sorted from oldest to newest.  If this is a non-empty list,
the last element must be equal to model_checkpoint_path.  These paths
are also saved in the CheckpointState proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_timestamps
						</dt>
						<dd>A list of floats, indicating the number of
seconds since the Epoch when each checkpoint was generated. 
						</dd>
						<dt>
							<code><span title="System.Nullable<double>">Nullable&lt;double&gt;</span></code> last_preserved_timestamp
						</dt>
						<dd>A float, indicating the number of seconds since
the Epoch when the last preserved checkpoint was written, e.g. due to a
`keep_checkpoint_every_n_hours` parameter (see
<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for an implementation). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>CheckpointState proto with model_checkpoint_path and
all_model_checkpoint_paths updated to either absolute paths or
relative paths to the current save_dir. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="generate_checkpoint_state_proto" class="method">
		<h4>
			<span title="System.object">object</span> <strong>generate_checkpoint_state_proto</strong>(<span title="System.string">string</span> save_dir, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> model_checkpoint_path, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> all_model_checkpoint_paths, <span title="System.object">object</span> all_model_checkpoint_timestamps, <span title="System.Nullable<double>">Nullable&lt;double&gt;</span> last_preserved_timestamp)
		</h4>
		<div class="content">Generates a checkpoint state proto. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> save_dir
						</dt>
						<dd>Directory where the model was saved. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> model_checkpoint_path
						</dt>
						<dd>The checkpoint file. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> all_model_checkpoint_paths
						</dt>
						<dd>List of strings.  Paths to all not-yet-deleted
checkpoints, sorted from oldest to newest.  If this is a non-empty list,
the last element must be equal to model_checkpoint_path.  These paths
are also saved in the CheckpointState proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_timestamps
						</dt>
						<dd>A list of floats, indicating the number of
seconds since the Epoch when each checkpoint was generated. 
						</dd>
						<dt>
							<code><span title="System.Nullable<double>">Nullable&lt;double&gt;</span></code> last_preserved_timestamp
						</dt>
						<dd>A float, indicating the number of seconds since
the Epoch when the last preserved checkpoint was written, e.g. due to a
`keep_checkpoint_every_n_hours` parameter (see
<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for an implementation). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>CheckpointState proto with model_checkpoint_path and
all_model_checkpoint_paths updated to either absolute paths or
relative paths to the current save_dir. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="generate_checkpoint_state_proto" class="method">
		<h4>
			<span title="System.object">object</span> <strong>generate_checkpoint_state_proto</strong>(<span title="System.string">string</span> save_dir, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> model_checkpoint_path, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> all_model_checkpoint_paths, <span title="System.object">object</span> all_model_checkpoint_timestamps, <span title="System.Nullable<double>">Nullable&lt;double&gt;</span> last_preserved_timestamp)
		</h4>
		<div class="content">Generates a checkpoint state proto. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> save_dir
						</dt>
						<dd>Directory where the model was saved. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> model_checkpoint_path
						</dt>
						<dd>The checkpoint file. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> all_model_checkpoint_paths
						</dt>
						<dd>List of strings.  Paths to all not-yet-deleted
checkpoints, sorted from oldest to newest.  If this is a non-empty list,
the last element must be equal to model_checkpoint_path.  These paths
are also saved in the CheckpointState proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_timestamps
						</dt>
						<dd>A list of floats, indicating the number of
seconds since the Epoch when each checkpoint was generated. 
						</dd>
						<dt>
							<code><span title="System.Nullable<double>">Nullable&lt;double&gt;</span></code> last_preserved_timestamp
						</dt>
						<dd>A float, indicating the number of seconds since
the Epoch when the last preserved checkpoint was written, e.g. due to a
`keep_checkpoint_every_n_hours` parameter (see
<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for an implementation). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>CheckpointState proto with model_checkpoint_path and
all_model_checkpoint_paths updated to either absolute paths or
relative paths to the current save_dir. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="generate_checkpoint_state_proto" class="method">
		<h4>
			<span title="System.object">object</span> <strong>generate_checkpoint_state_proto</strong>(<span title="System.string">string</span> save_dir, <span title="System.string">string</span> model_checkpoint_path, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> all_model_checkpoint_paths, <span title="System.object">object</span> all_model_checkpoint_timestamps, <span title="System.Nullable<double>">Nullable&lt;double&gt;</span> last_preserved_timestamp)
		</h4>
		<div class="content">Generates a checkpoint state proto. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> save_dir
						</dt>
						<dd>Directory where the model was saved. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> model_checkpoint_path
						</dt>
						<dd>The checkpoint file. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> all_model_checkpoint_paths
						</dt>
						<dd>List of strings.  Paths to all not-yet-deleted
checkpoints, sorted from oldest to newest.  If this is a non-empty list,
the last element must be equal to model_checkpoint_path.  These paths
are also saved in the CheckpointState proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_timestamps
						</dt>
						<dd>A list of floats, indicating the number of
seconds since the Epoch when each checkpoint was generated. 
						</dd>
						<dt>
							<code><span title="System.Nullable<double>">Nullable&lt;double&gt;</span></code> last_preserved_timestamp
						</dt>
						<dd>A float, indicating the number of seconds since
the Epoch when the last preserved checkpoint was written, e.g. due to a
`keep_checkpoint_every_n_hours` parameter (see
<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for an implementation). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>CheckpointState proto with model_checkpoint_path and
all_model_checkpoint_paths updated to either absolute paths or
relative paths to the current save_dir. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="generate_checkpoint_state_proto_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>generate_checkpoint_state_proto_dyn</strong>(<span title="System.object">object</span> save_dir, <span title="System.object">object</span> model_checkpoint_path, <span title="System.object">object</span> all_model_checkpoint_paths, <span title="System.object">object</span> all_model_checkpoint_timestamps, <span title="System.object">object</span> last_preserved_timestamp)
		</h4>
		<div class="content">Generates a checkpoint state proto. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> save_dir
						</dt>
						<dd>Directory where the model was saved. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> model_checkpoint_path
						</dt>
						<dd>The checkpoint file. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_paths
						</dt>
						<dd>List of strings.  Paths to all not-yet-deleted
checkpoints, sorted from oldest to newest.  If this is a non-empty list,
the last element must be equal to model_checkpoint_path.  These paths
are also saved in the CheckpointState proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_timestamps
						</dt>
						<dd>A list of floats, indicating the number of
seconds since the Epoch when each checkpoint was generated. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> last_preserved_timestamp
						</dt>
						<dd>A float, indicating the number of seconds since
the Epoch when the last preserved checkpoint was written, e.g. due to a
`keep_checkpoint_every_n_hours` parameter (see
<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for an implementation). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>CheckpointState proto with model_checkpoint_path and
all_model_checkpoint_paths updated to either absolute paths or
relative paths to the current save_dir. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_checkpoint_mtimes" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>get_checkpoint_mtimes</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> checkpoint_prefixes)
		</h4>
		<div class="content">Returns the mtimes (modification timestamps) of the checkpoints. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes. <p></p> Globs for the checkpoints pointed to by `checkpoint_prefixes`.  If the files
exist, collect their mtime.  Both V2 and V1 checkpoints are considered, in
that priority. <p></p> This is the recommended way to get the mtimes, since it takes into account
the naming difference between V1 and V2 formats. <p></p> Note: If not all checkpoints exist, the length of the returned mtimes list
will be smaller than the length of `checkpoint_prefixes` list, so mapping
checkpoints to corresponding mtimes will not be possible. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> checkpoint_prefixes
						</dt>
						<dd>a list of checkpoint paths, typically the results of
`Saver.save()` or those of `tf.train.latest_checkpoint()`, regardless of
sharded/non-sharded or V1/V2. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span></code>
					</dt>
					<dd>A list of mtimes (in microseconds) of the found checkpoints. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_checkpoint_mtimes_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_checkpoint_mtimes_dyn</strong>(<span title="System.object">object</span> checkpoint_prefixes)
		</h4>
		<div class="content">Returns the mtimes (modification timestamps) of the checkpoints. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file utilities to get mtimes. <p></p> Globs for the checkpoints pointed to by `checkpoint_prefixes`.  If the files
exist, collect their mtime.  Both V2 and V1 checkpoints are considered, in
that priority. <p></p> This is the recommended way to get the mtimes, since it takes into account
the naming difference between V1 and V2 formats. <p></p> Note: If not all checkpoints exist, the length of the returned mtimes list
will be smaller than the length of `checkpoint_prefixes` list, so mapping
checkpoints to corresponding mtimes will not be possible. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> checkpoint_prefixes
						</dt>
						<dd>a list of checkpoint paths, typically the results of
`Saver.save()` or those of `tf.train.latest_checkpoint()`, regardless of
sharded/non-sharded or V1/V2. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list of mtimes (in microseconds) of the found checkpoints. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_checkpoint_state" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_checkpoint_state</strong>(<span title="System.Byte[]">Byte[]</span> checkpoint_dir, <span title="System.string">string</span> latest_filename)
		</h4>
		<div class="content">Returns CheckpointState proto from the "checkpoint" file. <p></p> If the "checkpoint" file contains a valid CheckpointState
proto, returns it. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>The directory of checkpoints. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name of the checkpoint file.  Default to
'checkpoint'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A CheckpointState if the state was available, None
otherwise. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_checkpoint_state" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_checkpoint_state</strong>(<span title="System.string">string</span> checkpoint_dir, <span title="System.string">string</span> latest_filename)
		</h4>
		<div class="content">Returns CheckpointState proto from the "checkpoint" file. <p></p> If the "checkpoint" file contains a valid CheckpointState
proto, returns it. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>The directory of checkpoints. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name of the checkpoint file.  Default to
'checkpoint'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A CheckpointState if the state was available, None
otherwise. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_checkpoint_state_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_checkpoint_state_dyn</strong>(<span title="System.object">object</span> checkpoint_dir, <span title="System.object">object</span> latest_filename)
		</h4>
		<div class="content">Returns CheckpointState proto from the "checkpoint" file. <p></p> If the "checkpoint" file contains a valid CheckpointState
proto, returns it. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> checkpoint_dir
						</dt>
						<dd>The directory of checkpoints. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> latest_filename
						</dt>
						<dd>Optional name of the checkpoint file.  Default to
'checkpoint'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A CheckpointState if the state was available, None
otherwise. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_global_step" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_global_step</strong>(<a href="../tensorflow/Graph.htm">Graph</a> graph)
		</h4>
		<div class="content">Get the global step tensor. <p></p> The global step tensor must be an integer variable. We first try to find it
in the collection `GLOBAL_STEP`, or by name `global_step:0`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/Graph.htm">Graph</a></code> graph
						</dt>
						<dd>The graph to find the global step in. If missing, use default graph. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The global step variable, or `None` if none was found. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="global_step" class="method">
		<h4>
			<span title="System.int">int</span> <strong>global_step</strong>(<a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a> sess, <span title="System.object">object</span> global_step_tensor)
		</h4>
		<div class="content">Small helper to get the global step. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a></code> sess
						</dt>
						<dd>A TensorFlow `Session` object. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step_tensor
						</dt>
						<dd>`Tensor` or the `name` of the operation that contains
the global step. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.int">int</span></code>
					</dt>
					<dd>The global step value. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Create a variable to hold the global_step.
            global_step_tensor = tf.Variable(10, trainable=False, name='global_step')
            # Create a session.
            sess = tf.compat.v1.Session()
            # Initialize the variable
            sess.run(global_step_tensor.initializer)
            # Get the variable value.
            print('global_step: %s' % tf.compat.v1.train.global_step(sess,
            global_step_tensor)) <p></p> global_step: 10 </pre>
</div>
		</div>
	</div>
	<div id="global_step" class="method">
		<h4>
			<span title="System.int">int</span> <strong>global_step</strong>(<a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a> sess, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> global_step_tensor)
		</h4>
		<div class="content">Small helper to get the global step. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a></code> sess
						</dt>
						<dd>A TensorFlow `Session` object. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> global_step_tensor
						</dt>
						<dd>`Tensor` or the `name` of the operation that contains
the global step. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.int">int</span></code>
					</dt>
					<dd>The global step value. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Create a variable to hold the global_step.
            global_step_tensor = tf.Variable(10, trainable=False, name='global_step')
            # Create a session.
            sess = tf.compat.v1.Session()
            # Initialize the variable
            sess.run(global_step_tensor.initializer)
            # Get the variable value.
            print('global_step: %s' % tf.compat.v1.train.global_step(sess,
            global_step_tensor)) <p></p> global_step: 10 </pre>
</div>
		</div>
	</div>
	<div id="global_step" class="method">
		<h4>
			<span title="System.int">int</span> <strong>global_step</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.object">object</span> global_step_tensor)
		</h4>
		<div class="content">Small helper to get the global step. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A TensorFlow `Session` object. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step_tensor
						</dt>
						<dd>`Tensor` or the `name` of the operation that contains
the global step. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.int">int</span></code>
					</dt>
					<dd>The global step value. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Create a variable to hold the global_step.
            global_step_tensor = tf.Variable(10, trainable=False, name='global_step')
            # Create a session.
            sess = tf.compat.v1.Session()
            # Initialize the variable
            sess.run(global_step_tensor.initializer)
            # Get the variable value.
            print('global_step: %s' % tf.compat.v1.train.global_step(sess,
            global_step_tensor)) <p></p> global_step: 10 </pre>
</div>
		</div>
	</div>
	<div id="global_step" class="method">
		<h4>
			<span title="System.int">int</span> <strong>global_step</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> global_step_tensor)
		</h4>
		<div class="content">Small helper to get the global step. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A TensorFlow `Session` object. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> global_step_tensor
						</dt>
						<dd>`Tensor` or the `name` of the operation that contains
the global step. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.int">int</span></code>
					</dt>
					<dd>The global step value. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Create a variable to hold the global_step.
            global_step_tensor = tf.Variable(10, trainable=False, name='global_step')
            # Create a session.
            sess = tf.compat.v1.Session()
            # Initialize the variable
            sess.run(global_step_tensor.initializer)
            # Get the variable value.
            print('global_step: %s' % tf.compat.v1.train.global_step(sess,
            global_step_tensor)) <p></p> global_step: 10 </pre>
</div>
		</div>
	</div>
	<div id="global_step_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>global_step_dyn</strong>(<span title="System.object">object</span> sess, <span title="System.object">object</span> global_step_tensor)
		</h4>
		<div class="content">Small helper to get the global step. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> sess
						</dt>
						<dd>A TensorFlow `Session` object. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step_tensor
						</dt>
						<dd>`Tensor` or the `name` of the operation that contains
the global step. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The global step value. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Create a variable to hold the global_step.
            global_step_tensor = tf.Variable(10, trainable=False, name='global_step')
            # Create a session.
            sess = tf.compat.v1.Session()
            # Initialize the variable
            sess.run(global_step_tensor.initializer)
            # Get the variable value.
            print('global_step: %s' % tf.compat.v1.train.global_step(sess,
            global_step_tensor)) <p></p> global_step: 10 </pre>
</div>
		</div>
	</div>
	<div id="import_meta_graph" class="method">
		<h4>
			<a href="../tensorflow.train/Saver.htm">Saver</a> <strong>import_meta_graph</strong>(<span title="System.int">int</span> meta_graph_or_file, <span title="System.bool">bool</span> clear_devices, <span title="System.string">string</span> import_scope, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Recreates a Graph saved in a `MetaGraphDef` proto. <p></p> This function takes a `MetaGraphDef` protocol buffer as input. If
the argument is a file containing a `MetaGraphDef` protocol buffer ,
it constructs a protocol buffer from the file content. The function
then adds all the nodes from the `graph_def` field to the
current graph, recreates all the collections, and returns a saver
constructed from the `saver_def` field. <p></p> In combination with `export_meta_graph()`, this function can be used to <p></p> * Serialize a graph along with other Python objects such as `QueueRunner`,
`Variable` into a `MetaGraphDef`. <p></p> * Restart training from a saved graph and checkpoints. <p></p> * Run inference from a saved graph and checkpoints.
Later we can continue training from this saved `meta_graph` without building
the model from scratch.
NOTE: Restarting training from saved `meta_graph` only works if the
device assignments have not changed. <p></p> Example:
Variables, placeholders, and independent operations can also be stored, as
shown in the following example.
Later this model can be restored and contents loaded. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> meta_graph_or_file
						</dt>
						<dd>`MetaGraphDef` protocol buffer or filename (including
the path) containing a `MetaGraphDef`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during import. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> import_scope
						</dt>
						<dd>Optional `string`. Name scope to add. Only used when
initializing from protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyed arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/Saver.htm">Saver</a></code>
					</dt>
					<dd>A saver constructed from `saver_def` in `MetaGraphDef` or None. <p></p> A None value is returned if no variables exist in the `MetaGraphDef`
(i.e., there are no variables to restore). 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>...
            # Create a saver.
            saver = tf.compat.v1.train.Saver(...variables...)
            # Remember the training_op we want to run by adding it to a collection.
            tf.compat.v1.add_to_collection('train_op', train_op)
            sess = tf.compat.v1.Session()
            for step in xrange(1000000):
                sess.run(train_op)
                if step % 1000 == 0:
                    # Saves checkpoint, which by default also exports a meta_graph
                    # named 'my-model-global_step.meta'.
                    saver.save(sess, 'my-model', global_step=step) </pre>
</div>
		</div>
	</div>
	<div id="import_meta_graph" class="method">
		<h4>
			<a href="../tensorflow.train/Saver.htm">Saver</a> <strong>import_meta_graph</strong>(<span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> meta_graph_or_file, <span title="System.bool">bool</span> clear_devices, <span title="System.string">string</span> import_scope, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Recreates a Graph saved in a `MetaGraphDef` proto. <p></p> This function takes a `MetaGraphDef` protocol buffer as input. If
the argument is a file containing a `MetaGraphDef` protocol buffer ,
it constructs a protocol buffer from the file content. The function
then adds all the nodes from the `graph_def` field to the
current graph, recreates all the collections, and returns a saver
constructed from the `saver_def` field. <p></p> In combination with `export_meta_graph()`, this function can be used to <p></p> * Serialize a graph along with other Python objects such as `QueueRunner`,
`Variable` into a `MetaGraphDef`. <p></p> * Restart training from a saved graph and checkpoints. <p></p> * Run inference from a saved graph and checkpoints.
Later we can continue training from this saved `meta_graph` without building
the model from scratch.
NOTE: Restarting training from saved `meta_graph` only works if the
device assignments have not changed. <p></p> Example:
Variables, placeholders, and independent operations can also be stored, as
shown in the following example.
Later this model can be restored and contents loaded. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> meta_graph_or_file
						</dt>
						<dd>`MetaGraphDef` protocol buffer or filename (including
the path) containing a `MetaGraphDef`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during import. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> import_scope
						</dt>
						<dd>Optional `string`. Name scope to add. Only used when
initializing from protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyed arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/Saver.htm">Saver</a></code>
					</dt>
					<dd>A saver constructed from `saver_def` in `MetaGraphDef` or None. <p></p> A None value is returned if no variables exist in the `MetaGraphDef`
(i.e., there are no variables to restore). 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>...
            # Create a saver.
            saver = tf.compat.v1.train.Saver(...variables...)
            # Remember the training_op we want to run by adding it to a collection.
            tf.compat.v1.add_to_collection('train_op', train_op)
            sess = tf.compat.v1.Session()
            for step in xrange(1000000):
                sess.run(train_op)
                if step % 1000 == 0:
                    # Saves checkpoint, which by default also exports a meta_graph
                    # named 'my-model-global_step.meta'.
                    saver.save(sess, 'my-model', global_step=step) </pre>
</div>
		</div>
	</div>
	<div id="import_meta_graph" class="method">
		<h4>
			<a href="../tensorflow.train/Saver.htm">Saver</a> <strong>import_meta_graph</strong>(<span title="System.string">string</span> meta_graph_or_file, <span title="System.bool">bool</span> clear_devices, <span title="System.string">string</span> import_scope, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Recreates a Graph saved in a `MetaGraphDef` proto. <p></p> This function takes a `MetaGraphDef` protocol buffer as input. If
the argument is a file containing a `MetaGraphDef` protocol buffer ,
it constructs a protocol buffer from the file content. The function
then adds all the nodes from the `graph_def` field to the
current graph, recreates all the collections, and returns a saver
constructed from the `saver_def` field. <p></p> In combination with `export_meta_graph()`, this function can be used to <p></p> * Serialize a graph along with other Python objects such as `QueueRunner`,
`Variable` into a `MetaGraphDef`. <p></p> * Restart training from a saved graph and checkpoints. <p></p> * Run inference from a saved graph and checkpoints.
Later we can continue training from this saved `meta_graph` without building
the model from scratch.
NOTE: Restarting training from saved `meta_graph` only works if the
device assignments have not changed. <p></p> Example:
Variables, placeholders, and independent operations can also be stored, as
shown in the following example.
Later this model can be restored and contents loaded. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_or_file
						</dt>
						<dd>`MetaGraphDef` protocol buffer or filename (including
the path) containing a `MetaGraphDef`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during import. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> import_scope
						</dt>
						<dd>Optional `string`. Name scope to add. Only used when
initializing from protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyed arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/Saver.htm">Saver</a></code>
					</dt>
					<dd>A saver constructed from `saver_def` in `MetaGraphDef` or None. <p></p> A None value is returned if no variables exist in the `MetaGraphDef`
(i.e., there are no variables to restore). 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>...
            # Create a saver.
            saver = tf.compat.v1.train.Saver(...variables...)
            # Remember the training_op we want to run by adding it to a collection.
            tf.compat.v1.add_to_collection('train_op', train_op)
            sess = tf.compat.v1.Session()
            for step in xrange(1000000):
                sess.run(train_op)
                if step % 1000 == 0:
                    # Saves checkpoint, which by default also exports a meta_graph
                    # named 'my-model-global_step.meta'.
                    saver.save(sess, 'my-model', global_step=step) </pre>
</div>
		</div>
	</div>
	<div id="import_meta_graph_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>import_meta_graph_dyn</strong>(<span title="System.object">object</span> meta_graph_or_file, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> clear_devices, <span title="System.object">object</span> import_scope, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Recreates a Graph saved in a `MetaGraphDef` proto. <p></p> This function takes a `MetaGraphDef` protocol buffer as input. If
the argument is a file containing a `MetaGraphDef` protocol buffer ,
it constructs a protocol buffer from the file content. The function
then adds all the nodes from the `graph_def` field to the
current graph, recreates all the collections, and returns a saver
constructed from the `saver_def` field. <p></p> In combination with `export_meta_graph()`, this function can be used to <p></p> * Serialize a graph along with other Python objects such as `QueueRunner`,
`Variable` into a `MetaGraphDef`. <p></p> * Restart training from a saved graph and checkpoints. <p></p> * Run inference from a saved graph and checkpoints.
Later we can continue training from this saved `meta_graph` without building
the model from scratch.
NOTE: Restarting training from saved `meta_graph` only works if the
device assignments have not changed. <p></p> Example:
Variables, placeholders, and independent operations can also be stored, as
shown in the following example.
Later this model can be restored and contents loaded. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> meta_graph_or_file
						</dt>
						<dd>`MetaGraphDef` protocol buffer or filename (including
the path) containing a `MetaGraphDef`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during import. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> import_scope
						</dt>
						<dd>Optional `string`. Name scope to add. Only used when
initializing from protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Optional keyed arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A saver constructed from `saver_def` in `MetaGraphDef` or None. <p></p> A None value is returned if no variables exist in the `MetaGraphDef`
(i.e., there are no variables to restore). 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>...
            # Create a saver.
            saver = tf.compat.v1.train.Saver(...variables...)
            # Remember the training_op we want to run by adding it to a collection.
            tf.compat.v1.add_to_collection('train_op', train_op)
            sess = tf.compat.v1.Session()
            for step in xrange(1000000):
                sess.run(train_op)
                if step % 1000 == 0:
                    # Saves checkpoint, which by default also exports a meta_graph
                    # named 'my-model-global_step.meta'.
                    saver.save(sess, 'my-model', global_step=step) </pre>
</div>
		</div>
	</div>
	<div id="input_producer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>input_producer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> input_tensor, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> element_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> summary_name, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output the rows of `input_tensor` to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> input_tensor
						</dt>
						<dd>A tensor with the rows to produce. Must be at least
one-dimensional. Must either have a fully-defined shape, or
`element_shape` must be defined. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> element_shape
						</dt>
						<dd>(Optional.) A `TensorShape` representing the shape of a
row of `input_tensor`, if it cannot be inferred. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>(Optional.) An integer. If specified `input_producer` produces
each row of `input_tensor` `num_epochs` times before generating an
`OutOfRange` error. If not specified, `input_producer` can cycle through
the rows of `input_tensor` an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>(Optional.) A boolean. If true, the rows are randomly shuffled
within each epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>(Optional.) An integer. The seed to use if `shuffle` is true. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>(Optional.) The capacity of the queue to be used for buffering
the input. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional.) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> summary_name
						</dt>
						<dd>(Optional.) If set, a scalar summary for the current queue
size will be generated, using this name as part of the tag. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> name
						</dt>
						<dd>(Optional.) A name for queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>(Optional.) Cancel op for the queue 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output rows.  A `QueueRunner` for the queue is
added to the current `QUEUE_RUNNER` collection of the current
graph. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="input_producer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>input_producer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> input_tensor, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> element_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> summary_name, <span title="System.string">string</span> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output the rows of `input_tensor` to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> input_tensor
						</dt>
						<dd>A tensor with the rows to produce. Must be at least
one-dimensional. Must either have a fully-defined shape, or
`element_shape` must be defined. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> element_shape
						</dt>
						<dd>(Optional.) A `TensorShape` representing the shape of a
row of `input_tensor`, if it cannot be inferred. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>(Optional.) An integer. If specified `input_producer` produces
each row of `input_tensor` `num_epochs` times before generating an
`OutOfRange` error. If not specified, `input_producer` can cycle through
the rows of `input_tensor` an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>(Optional.) A boolean. If true, the rows are randomly shuffled
within each epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>(Optional.) An integer. The seed to use if `shuffle` is true. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>(Optional.) The capacity of the queue to be used for buffering
the input. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional.) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> summary_name
						</dt>
						<dd>(Optional.) If set, a scalar summary for the current queue
size will be generated, using this name as part of the tag. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional.) A name for queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>(Optional.) Cancel op for the queue 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output rows.  A `QueueRunner` for the queue is
added to the current `QUEUE_RUNNER` collection of the current
graph. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="input_producer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>input_producer</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> input_tensor, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> element_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> summary_name, <a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output the rows of `input_tensor` to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> input_tensor
						</dt>
						<dd>A tensor with the rows to produce. Must be at least
one-dimensional. Must either have a fully-defined shape, or
`element_shape` must be defined. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> element_shape
						</dt>
						<dd>(Optional.) A `TensorShape` representing the shape of a
row of `input_tensor`, if it cannot be inferred. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>(Optional.) An integer. If specified `input_producer` produces
each row of `input_tensor` `num_epochs` times before generating an
`OutOfRange` error. If not specified, `input_producer` can cycle through
the rows of `input_tensor` an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>(Optional.) A boolean. If true, the rows are randomly shuffled
within each epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>(Optional.) An integer. The seed to use if `shuffle` is true. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>(Optional.) The capacity of the queue to be used for buffering
the input. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional.) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> summary_name
						</dt>
						<dd>(Optional.) If set, a scalar summary for the current queue
size will be generated, using this name as part of the tag. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> name
						</dt>
						<dd>(Optional.) A name for queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>(Optional.) Cancel op for the queue 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output rows.  A `QueueRunner` for the queue is
added to the current `QUEUE_RUNNER` collection of the current
graph. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="input_producer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>input_producer</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> input_tensor, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> element_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> summary_name, <span title="System.string">string</span> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output the rows of `input_tensor` to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> input_tensor
						</dt>
						<dd>A tensor with the rows to produce. Must be at least
one-dimensional. Must either have a fully-defined shape, or
`element_shape` must be defined. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> element_shape
						</dt>
						<dd>(Optional.) A `TensorShape` representing the shape of a
row of `input_tensor`, if it cannot be inferred. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>(Optional.) An integer. If specified `input_producer` produces
each row of `input_tensor` `num_epochs` times before generating an
`OutOfRange` error. If not specified, `input_producer` can cycle through
the rows of `input_tensor` an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>(Optional.) A boolean. If true, the rows are randomly shuffled
within each epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>(Optional.) An integer. The seed to use if `shuffle` is true. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>(Optional.) The capacity of the queue to be used for buffering
the input. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional.) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> summary_name
						</dt>
						<dd>(Optional.) If set, a scalar summary for the current queue
size will be generated, using this name as part of the tag. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional.) A name for queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>(Optional.) Cancel op for the queue 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output rows.  A `QueueRunner` for the queue is
added to the current `QUEUE_RUNNER` collection of the current
graph. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="input_producer_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>input_producer_dyn</strong>(<span title="System.object">object</span> input_tensor, <span title="System.object">object</span> element_shape, <span title="System.object">object</span> num_epochs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> shuffle, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> summary_name, <span title="System.object">object</span> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output the rows of `input_tensor` to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> input_tensor
						</dt>
						<dd>A tensor with the rows to produce. Must be at least
one-dimensional. Must either have a fully-defined shape, or
`element_shape` must be defined. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> element_shape
						</dt>
						<dd>(Optional.) A `TensorShape` representing the shape of a
row of `input_tensor`, if it cannot be inferred. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_epochs
						</dt>
						<dd>(Optional.) An integer. If specified `input_producer` produces
each row of `input_tensor` `num_epochs` times before generating an
`OutOfRange` error. If not specified, `input_producer` can cycle through
the rows of `input_tensor` an unlimited number of times. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> shuffle
						</dt>
						<dd>(Optional.) A boolean. If true, the rows are randomly shuffled
within each epoch. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>(Optional.) An integer. The seed to use if `shuffle` is true. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>(Optional.) The capacity of the queue to be used for buffering
the input. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional.) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_name
						</dt>
						<dd>(Optional.) If set, a scalar summary for the current queue
size will be generated, using this name as part of the tag. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional.) A name for queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>(Optional.) Cancel op for the queue 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output rows.  A `QueueRunner` for the queue is
added to the current `QUEUE_RUNNER` collection of the current
graph. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="inverse_time_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>inverse_time_decay</strong>(<span title="System.double">double</span> learning_rate, <a href="../tensorflow.python.ops.resource_variable_ops/ResourceVariable.htm">ResourceVariable</a> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> decay_rate, <span title="System.bool">bool</span> staircase, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies inverse time decay to the initial learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an inverse decay function
to a provided initial learning rate.  It requires an `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
or, if `staircase` is `True`, as:
Example: decay 1/t with a rate of 0.5: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><a href="../tensorflow.python.ops.resource_variable_ops/ResourceVariable.htm">ResourceVariable</a></code> global_step
						</dt>
						<dd>A Python number. Global step to use for the decay computation.
Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>How often to apply decay. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> decay_rate
						</dt>
						<dd>A Python number.  The decay rate. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> staircase
						</dt>
						<dd>Whether to apply decay in a discrete staircase, as opposed to
continuous, fashion. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'InverseTimeDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate / (1 + decay_rate * global_step /
            decay_step) </pre>
</div>
		</div>
	</div>
	<div id="inverse_time_decay_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>inverse_time_decay_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> decay_steps, <span title="System.object">object</span> decay_rate, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> staircase, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies inverse time decay to the initial learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an inverse decay function
to a provided initial learning rate.  It requires an `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
or, if `staircase` is `True`, as:
Example: decay 1/t with a rate of 0.5: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A Python number. Global step to use for the decay computation.
Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>How often to apply decay. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_rate
						</dt>
						<dd>A Python number.  The decay rate. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> staircase
						</dt>
						<dd>Whether to apply decay in a discrete staircase, as opposed to
continuous, fashion. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'InverseTimeDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate / (1 + decay_rate * global_step /
            decay_step) </pre>
</div>
		</div>
	</div>
	<div id="latest_checkpoint" class="method">
		<h4>
			<span title="System.object">object</span> <strong>latest_checkpoint</strong>(<span title="System.string">string</span> checkpoint_dir, <span title="System.string">string</span> latest_filename)
		</h4>
		<div class="content">Finds the filename of latest saved checkpoint file. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>Directory where the variables were saved. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that
contains the list of most recent checkpoint filenames.
See the corresponding argument to `Saver.save()`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the latest checkpoint or `None` if no checkpoint was found. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="latest_checkpoint" class="method">
		<h4>
			<span title="System.object">object</span> <strong>latest_checkpoint</strong>(<span title="System.Byte[]">Byte[]</span> checkpoint_dir, <span title="System.string">string</span> latest_filename)
		</h4>
		<div class="content">Finds the filename of latest saved checkpoint file. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>Directory where the variables were saved. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that
contains the list of most recent checkpoint filenames.
See the corresponding argument to `Saver.save()`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the latest checkpoint or `None` if no checkpoint was found. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="latest_checkpoint_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>latest_checkpoint_dyn</strong>(<span title="System.object">object</span> checkpoint_dir, <span title="System.object">object</span> latest_filename)
		</h4>
		<div class="content">Finds the filename of latest saved checkpoint file. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> checkpoint_dir
						</dt>
						<dd>Directory where the variables were saved. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that
contains the list of most recent checkpoint filenames.
See the corresponding argument to `Saver.save()`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the latest checkpoint or `None` if no checkpoint was found. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="limit_epochs" class="method">
		<h4>
			<span title="System.object">object</span> <strong>limit_epochs</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> tensor, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Returns tensor `num_epochs` times and then raises an `OutOfRange` error. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`. <p></p> Note: creates local counter `epochs`. Use `local_variables_initializer()` to
initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> tensor
						</dt>
						<dd>Any `Tensor`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>A positive integer (optional).  If specified, limits the number
of steps the output tensor may be evaluated. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>tensor or `OutOfRange`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="limit_epochs" class="method">
		<h4>
			<span title="System.object">object</span> <strong>limit_epochs</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensor, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Returns tensor `num_epochs` times and then raises an `OutOfRange` error. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`. <p></p> Note: creates local counter `epochs`. Use `local_variables_initializer()` to
initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensor
						</dt>
						<dd>Any `Tensor`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>A positive integer (optional).  If specified, limits the number
of steps the output tensor may be evaluated. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>tensor or `OutOfRange`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="limit_epochs_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>limit_epochs_dyn</strong>(<span title="System.object">object</span> tensor, <span title="System.object">object</span> num_epochs, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Returns tensor `num_epochs` times and then raises an `OutOfRange` error. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`. <p></p> Note: creates local counter `epochs`. Use `local_variables_initializer()` to
initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensor
						</dt>
						<dd>Any `Tensor`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_epochs
						</dt>
						<dd>A positive integer (optional).  If specified, limits the number
of steps the output tensor may be evaluated. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>tensor or `OutOfRange`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="linear_cosine_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>linear_cosine_decay</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> num_periods, <span title="System.double">double</span> alpha, <span title="System.double">double</span> beta, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies linear cosine decay to the learning rate. <p></p> See [Bello et al., ICML2017] Neural Optimizer Search with RL.
https://arxiv.org/abs/1709.07417 <p></p> For the idea of warm starts here controlled by `num_periods`,
see [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> Note that linear cosine decay is more aggressive than cosine decay and
larger initial learning rates can typically be used. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a linear cosine decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> num_periods
						</dt>
						<dd>Number of periods in the cosine part of the decay. See
computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> alpha
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> beta
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'LinearCosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            linear_decay = (decay_steps - global_step) / decay_steps)
            cosine_decay = 0.5 * (
                1 + cos(pi * 2 * num_periods * global_step / decay_steps))
            decayed = (alpha + linear_decay) * cosine_decay + beta
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="linear_cosine_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>linear_cosine_decay</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> decay_steps, <span title="System.int">int</span> num_periods, <span title="System.double">double</span> alpha, <span title="System.double">double</span> beta, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies linear cosine decay to the learning rate. <p></p> See [Bello et al., ICML2017] Neural Optimizer Search with RL.
https://arxiv.org/abs/1709.07417 <p></p> For the idea of warm starts here controlled by `num_periods`,
see [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> Note that linear cosine decay is more aggressive than cosine decay and
larger initial learning rates can typically be used. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a linear cosine decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_periods
						</dt>
						<dd>Number of periods in the cosine part of the decay. See
computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> alpha
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> beta
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'LinearCosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            linear_decay = (decay_steps - global_step) / decay_steps)
            cosine_decay = 0.5 * (
                1 + cos(pi * 2 * num_periods * global_step / decay_steps))
            decayed = (alpha + linear_decay) * cosine_decay + beta
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="linear_cosine_decay_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>linear_cosine_decay_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> decay_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> num_periods, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> alpha, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies linear cosine decay to the learning rate. <p></p> See [Bello et al., ICML2017] Neural Optimizer Search with RL.
https://arxiv.org/abs/1709.07417 <p></p> For the idea of warm starts here controlled by `num_periods`,
see [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> Note that linear cosine decay is more aggressive than cosine decay and
larger initial learning rates can typically be used. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a linear cosine decay function
to a provided initial learning rate.  It requires a `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> num_periods
						</dt>
						<dd>Number of periods in the cosine part of the decay. See
computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> alpha
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'LinearCosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            linear_decay = (decay_steps - global_step) / decay_steps)
            cosine_decay = 0.5 * (
                1 + cos(pi * 2 * num_periods * global_step / decay_steps))
            decayed = (alpha + linear_decay) * cosine_decay + beta
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="list_variables" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<ValueTuple<object, object>>">IList&lt;ValueTuple&lt;object, object&gt;&gt;</span> <strong>list_variables</strong>(<span title="System.Byte[]">Byte[]</span> ckpt_dir_or_file)
		</h4>
		<div class="content">Returns list of all variables in the checkpoint. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> ckpt_dir_or_file
						</dt>
						<dd>Directory with checkpoints file or path to checkpoint. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<ValueTuple<object, object>>">IList&lt;ValueTuple&lt;object, object&gt;&gt;</span></code>
					</dt>
					<dd>List of tuples `(name, shape)`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Conditionally creates batches of tensors based on `keep_input`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> tensors, <span title="System.bool">bool</span> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Conditionally creates batches of tensors based on `keep_input`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors, <span title="System.bool">bool</span> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Conditionally creates batches of tensors based on `keep_input`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> tensors, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Conditionally creates batches of tensors based on `keep_input`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> tensors, <span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Conditionally creates batches of tensors based on `keep_input`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors, <span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> num_threads, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Conditionally creates batches of tensors based on `keep_input`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch_dyn</strong>(<span title="System.object">object</span> tensors, <span title="System.object">object</span> keep_input, <span title="System.object">object</span> batch_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> num_threads, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dynamic_pad, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Conditionally creates batches of tensors based on `keep_input`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensors`.  The batching will
be nondeterministic if `num_threads > 1`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensors` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors_list, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Runs a list of tensors to conditionally fill a queue to create batches. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors_list, <span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Runs a list of tensors to conditionally fill a queue to create batches. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors_list, <span title="System.bool">bool</span> keep_input, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> dynamic_pad, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Runs a list of tensors to conditionally fill a queue to create batches. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_batch_join_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_batch_join_dyn</strong>(<span title="System.object">object</span> tensors_list, <span title="System.object">object</span> keep_input, <span title="System.object">object</span> batch_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dynamic_pad, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Runs a list of tensors to conditionally fill a queue to create batches. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`). <p></p> See docstring in `batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list_list[i]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dynamic_pad
						</dt>
						<dd>Boolean.  Allow variable dimensions in input shapes.
The given dimensions are padded upon dequeue so that tensors within a
batch have the same shapes. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span> keep_input, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> keep_input, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.bool">bool</span> keep_input, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span> keep_input, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> keep_input, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.bool">bool</span> keep_input, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch_dyn</strong>(<span title="System.object">object</span> tensors, <span title="System.object">object</span> batch_size, <span title="System.object">object</span> capacity, <span title="System.object">object</span> min_after_dequeue, <span title="System.object">object</span> keep_input, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> num_threads, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> keep_input, <span title="System.object">object</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span> keep_input, <span title="System.object">object</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<bool>">IEnumerable&lt;bool&gt;</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.bool">bool</span> keep_input, <span title="System.object">object</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="maybe_shuffle_batch_join_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>maybe_shuffle_batch_join_dyn</strong>(<span title="System.object">object</span> tensors_list, <span title="System.object">object</span> batch_size, <span title="System.object">object</span> capacity, <span title="System.object">object</span> min_after_dequeue, <span title="System.object">object</span> keep_input, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling conditionally-enqueued tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).filter(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> See docstring in `shuffle_batch_join` for more details. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> keep_input
						</dt>
						<dd>A `bool` Tensor.  This tensor controls whether the input is
added to the queue or not.  If it is a scalar and evaluates `True`, then
`tensors` are all added to the queue. If it is a vector and `enqueue_many`
is `True`, then each example is added to the queue only if the
corresponding value in `keep_input` is `True`. This tensor essentially
acts as a filtering mechanism. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.Byte[]">Byte[]</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <span title="System.int">int</span> save_summaries_steps, <span title="System.double">double</span> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.double">double</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession" class="method">
		<h4>
			<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> <strong>MonitoredTrainingSession</strong>(<span title="System.string">string</span> master, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> is_chief, <span title="System.string">string</span> checkpoint_dir, <a href="../tensorflow.train/Scaffold.htm">Scaffold</a> scaffold, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> hooks, <span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span> chief_only_hooks, <span title="System.int">int</span> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <span title="System.int">int</span> stop_grace_period_secs, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> log_step_count_steps, <span title="System.int">int</span> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Scaffold.htm">Scaffold</a></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<SessionRunHook>">IEnumerable&lt;SessionRunHook&gt;</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="MonitoredTrainingSession_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>MonitoredTrainingSession_dyn</strong>(<a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> master, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> is_chief, <span title="System.object">object</span> checkpoint_dir, <span title="System.object">object</span> scaffold, <span title="System.object">object</span> hooks, <span title="System.object">object</span> chief_only_hooks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_summaries_secs, <span title="System.object">object</span> config, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> stop_grace_period_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> log_step_count_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> max_wait_secs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_checkpoint_steps, <span title="System.object">object</span> summary_dir)
		</h4>
		<div class="content">Creates a `MonitoredSession` for training. <p></p> For a chief, this utility sets proper session initializer/restorer. It also
creates hooks related to checkpoint and summary saving. For workers, this
utility sets proper session creator which waits for the chief to
initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for
more
information. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> master
						</dt>
						<dd>`String` the TensorFlow master to use. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> is_chief
						</dt>
						<dd>If `True`, it will take care of initialization and recovery the
underlying TensorFlow session. If `False`, it will wait on a chief to
initialize or recover the TensorFlow session. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> checkpoint_dir
						</dt>
						<dd>A string.  Optional path to a directory where to restore
variables. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scaffold
						</dt>
						<dd>A `Scaffold` used for gathering or building supportive ops. If not
specified, a default one is created. It's used to finalize the graph. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> hooks
						</dt>
						<dd>Optional list of `SessionRunHook` objects. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> chief_only_hooks
						</dt>
						<dd>list of `SessionRunHook` objects. Activate these hooks if
`is_chief==True`, ignore otherwise. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_secs
						</dt>
						<dd>The frequency, in seconds, that a checkpoint is saved
using a default checkpoint saver. If both `save_checkpoint_steps` and
`save_checkpoint_secs` are set to `None`, then the default checkpoint
saver isn't used. If both are provided, then only `save_checkpoint_secs`
is used. Default 600. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
summaries are written to disk using a default summary saver. If both
`save_summaries_steps` and `save_summaries_secs` are set to `None`, then
the default summary saver isn't used. Default 100. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_summaries_secs
						</dt>
						<dd>The frequency, in secs, that the summaries are written
to disk using a default summary saver.  If both `save_summaries_steps` and
`save_summaries_secs` are set to `None`, then the default summary saver
isn't used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> config
						</dt>
						<dd>an instance of `tf.compat.v1.ConfigProto` proto used to configure
the session. It's the `config` argument of constructor of
`tf.compat.v1.Session`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> stop_grace_period_secs
						</dt>
						<dd>Number of seconds given to threads to stop after
`close()` has been called. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> log_step_count_steps
						</dt>
						<dd>The frequency, in number of global steps, that the
global step/sec is logged. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> max_wait_secs
						</dt>
						<dd>Maximum time workers should wait for the session to become
available. This should be kept relatively short to help detect incorrect
code, but sometimes may need to be increased if the chief takes a while to
start up. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_checkpoint_steps
						</dt>
						<dd>The frequency, in number of global steps, that a
checkpoint is saved using a default checkpoint saver. If both
`save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then
the default checkpoint saver isn't used. If both are provided, then only
`save_checkpoint_secs` is used. Default not enabled. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> summary_dir
						</dt>
						<dd>A string.  Optional path to a directory where to save
summaries. If None, checkpoint_dir is used instead. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `MonitoredSession` object. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="natural_exp_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>natural_exp_decay</strong>(<span title="System.double">double</span> learning_rate, <a href="../tensorflow.python.ops.resource_variable_ops/ResourceVariable.htm">ResourceVariable</a> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> decay_rate, <span title="System.bool">bool</span> staircase, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies natural exponential decay to the initial learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an exponential decay function
to a provided initial learning rate.  It requires an `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
or, if `staircase` is `True`, as:
Example: decay exponentially with a base of 0.96: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><a href="../tensorflow.python.ops.resource_variable_ops/ResourceVariable.htm">ResourceVariable</a></code> global_step
						</dt>
						<dd>A Python number. Global step to use for the decay computation.
Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>How often to apply decay. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> decay_rate
						</dt>
						<dd>A Python number.  The decay rate. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> staircase
						</dt>
						<dd>Whether to apply decay in a discrete staircase, as opposed to
continuous, fashion. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'ExponentialTimeDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate * exp(-decay_rate * global_step /
            decay_step) </pre>
</div>
		</div>
	</div>
	<div id="natural_exp_decay_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>natural_exp_decay_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> decay_steps, <span title="System.object">object</span> decay_rate, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> staircase, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies natural exponential decay to the initial learning rate. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies an exponential decay function
to a provided initial learning rate.  It requires an `global_step` value to
compute the decayed learning rate.  You can just pass a TensorFlow variable
that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
or, if `staircase` is `True`, as:
Example: decay exponentially with a base of 0.96: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A Python number. Global step to use for the decay computation.
Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>How often to apply decay. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_rate
						</dt>
						<dd>A Python number.  The decay rate. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> staircase
						</dt>
						<dd>Whether to apply decay in a discrete staircase, as opposed to
continuous, fashion. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'ExponentialTimeDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>decayed_learning_rate = learning_rate * exp(-decay_rate * global_step /
            decay_step) </pre>
</div>
		</div>
	</div>
	<div id="noisy_linear_cosine_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>noisy_linear_cosine_decay</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> initial_variance, <span title="System.double">double</span> variance_decay, <span title="System.int">int</span> num_periods, <span title="System.double">double</span> alpha, <span title="System.double">double</span> beta, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies noisy linear cosine decay to the learning rate. <p></p> See [Bello et al., ICML2017] Neural Optimizer Search with RL.
https://arxiv.org/abs/1709.07417 <p></p> For the idea of warm starts here controlled by `num_periods`,
see [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> Note that linear cosine decay is more aggressive than cosine decay and
larger initial learning rates can typically be used. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a noisy linear
cosine decay function to a provided initial learning rate.
It requires a `global_step` value to compute the decayed learning rate.
You can just pass a TensorFlow variable that you increment at each
training step. <p></p> The function returns the decayed learning rate.  It is computed as:
where eps_t is 0-centered gaussian noise with variance
initial_variance / (1 + global_step) ** variance_decay <p></p> Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> initial_variance
						</dt>
						<dd>initial variance for the noise. See computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> variance_decay
						</dt>
						<dd>decay for the noise's variance. See computation above. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_periods
						</dt>
						<dd>Number of periods in the cosine part of the decay. See
computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> alpha
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> beta
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'NoisyLinearCosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            linear_decay = (decay_steps - global_step) / decay_steps)
            cosine_decay = 0.5 * (
                1 + cos(pi * 2 * num_periods * global_step / decay_steps))
            decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="noisy_linear_cosine_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>noisy_linear_cosine_decay</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> initial_variance, <span title="System.double">double</span> variance_decay, <span title="System.double">double</span> num_periods, <span title="System.double">double</span> alpha, <span title="System.double">double</span> beta, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies noisy linear cosine decay to the learning rate. <p></p> See [Bello et al., ICML2017] Neural Optimizer Search with RL.
https://arxiv.org/abs/1709.07417 <p></p> For the idea of warm starts here controlled by `num_periods`,
see [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> Note that linear cosine decay is more aggressive than cosine decay and
larger initial learning rates can typically be used. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a noisy linear
cosine decay function to a provided initial learning rate.
It requires a `global_step` value to compute the decayed learning rate.
You can just pass a TensorFlow variable that you increment at each
training step. <p></p> The function returns the decayed learning rate.  It is computed as:
where eps_t is 0-centered gaussian noise with variance
initial_variance / (1 + global_step) ** variance_decay <p></p> Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> initial_variance
						</dt>
						<dd>initial variance for the noise. See computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> variance_decay
						</dt>
						<dd>decay for the noise's variance. See computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> num_periods
						</dt>
						<dd>Number of periods in the cosine part of the decay. See
computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> alpha
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> beta
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'NoisyLinearCosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            linear_decay = (decay_steps - global_step) / decay_steps)
            cosine_decay = 0.5 * (
                1 + cos(pi * 2 * num_periods * global_step / decay_steps))
            decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="noisy_linear_cosine_decay_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>noisy_linear_cosine_decay_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> decay_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> initial_variance, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> variance_decay, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> num_periods, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> alpha, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies noisy linear cosine decay to the learning rate. <p></p> See [Bello et al., ICML2017] Neural Optimizer Search with RL.
https://arxiv.org/abs/1709.07417 <p></p> For the idea of warm starts here controlled by `num_periods`,
see [Loshchilov & Hutter, ICLR2016] SGDR: Stochastic Gradient Descent
with Warm Restarts. https://arxiv.org/abs/1608.03983 <p></p> Note that linear cosine decay is more aggressive than cosine decay and
larger initial learning rates can typically be used. <p></p> When training a model, it is often recommended to lower the learning rate as
the training progresses.  This function applies a noisy linear
cosine decay function to a provided initial learning rate.
It requires a `global_step` value to compute the decayed learning rate.
You can just pass a TensorFlow variable that you increment at each
training step. <p></p> The function returns the decayed learning rate.  It is computed as:
where eps_t is 0-centered gaussian noise with variance
initial_variance / (1 + global_step) ** variance_decay <p></p> Example usage: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` Tensor or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Number
of steps to decay over. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> initial_variance
						</dt>
						<dd>initial variance for the noise. See computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> variance_decay
						</dt>
						<dd>decay for the noise's variance. See computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> num_periods
						</dt>
						<dd>Number of periods in the cosine part of the decay. See
computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> alpha
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta
						</dt>
						<dd>See computation above. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name of the operation.  Defaults to
'NoisyLinearCosineDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            linear_decay = (decay_steps - global_step) / decay_steps)
            cosine_decay = 0.5 * (
                1 + cos(pi * 2 * num_periods * global_step / decay_steps))
            decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
            decayed_learning_rate = learning_rate * decayed </pre>
</div>
		</div>
	</div>
	<div id="piecewise_constant" class="method">
		<h4>
			<span title="System.object">object</span> <strong>piecewise_constant</strong>(<a href="../tensorflow/Variable.htm">Variable</a> x, <span title="System.Collections.Generic.IEnumerable<double>">IEnumerable&lt;double&gt;</span> boundaries, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> values, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Piecewise constant from boundaries and interval values. <p></p> Example: use a learning rate that's 1.0 for the first 100001 steps, 0.5
for the next 10000 steps, and 0.1 for any additional steps. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/Variable.htm">Variable</a></code> x
						</dt>
						<dd>A 0-D scalar `Tensor`. Must be one of the following types: `float32`,
`float64`, `uint8`, `int8`, `int16`, `int32`, `int64`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<double>">IEnumerable&lt;double&gt;</span></code> boundaries
						</dt>
						<dd>A list of `Tensor`s or `int`s or `float`s with strictly
increasing entries, and with all elements having the same type as `x`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> values
						</dt>
						<dd>A list of `Tensor`s or `float`s or `int`s that specifies the values
for the intervals defined by `boundaries`. It should have one more element
than `boundaries`, and all elements should have the same type. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string. Optional name of the operation. Defaults to
'PiecewiseConstant'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A 0-D Tensor. Its value is `values[0]` when `x <= boundaries[0]`,
`values[1]` when `x > boundaries[0]` and `x <= boundaries[1]`,...,
and values[-1] when `x > boundaries[-1]`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = tf.Variable(0, trainable=False)
            boundaries = [100000, 110000]
            values = [1.0, 0.5, 0.1]
            learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries,
            values) <p></p> # Later, whenever we perform an optimization step, we increment global_step. </pre>
</div>
		</div>
	</div>
	<div id="piecewise_constant_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>piecewise_constant_dyn</strong>(<span title="System.object">object</span> x, <span title="System.object">object</span> boundaries, <span title="System.object">object</span> values, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Piecewise constant from boundaries and interval values. <p></p> Example: use a learning rate that's 1.0 for the first 100001 steps, 0.5
for the next 10000 steps, and 0.1 for any additional steps. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> x
						</dt>
						<dd>A 0-D scalar `Tensor`. Must be one of the following types: `float32`,
`float64`, `uint8`, `int8`, `int16`, `int32`, `int64`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> boundaries
						</dt>
						<dd>A list of `Tensor`s or `int`s or `float`s with strictly
increasing entries, and with all elements having the same type as `x`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> values
						</dt>
						<dd>A list of `Tensor`s or `float`s or `int`s that specifies the values
for the intervals defined by `boundaries`. It should have one more element
than `boundaries`, and all elements should have the same type. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string. Optional name of the operation. Defaults to
'PiecewiseConstant'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A 0-D Tensor. Its value is `values[0]` when `x <= boundaries[0]`,
`values[1]` when `x > boundaries[0]` and `x <= boundaries[1]`,...,
and values[-1] when `x > boundaries[-1]`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = tf.Variable(0, trainable=False)
            boundaries = [100000, 110000]
            values = [1.0, 0.5, 0.1]
            learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries,
            values) <p></p> # Later, whenever we perform an optimization step, we increment global_step. </pre>
</div>
		</div>
	</div>
	<div id="polynomial_decay" class="method">
		<h4>
			<span title="System.object">object</span> <strong>polynomial_decay</strong>(<span title="System.double">double</span> learning_rate, <span title="System.int">int</span> global_step, <span title="System.int">int</span> decay_steps, <span title="System.double">double</span> end_learning_rate, <span title="System.double">double</span> power, <span title="System.bool">bool</span> cycle, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies a polynomial decay to the learning rate. <p></p> It is commonly observed that a monotonically decreasing learning rate, whose
degree of change is carefully chosen, results in a better performing model.
This function applies a polynomial decay function to a provided initial
`learning_rate` to reach an `end_learning_rate` in the given `decay_steps`. <p></p> It requires a `global_step` value to compute the decayed learning rate.  You
can just pass a TensorFlow variable that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
If `cycle` is True then a multiple of `decay_steps` is used, the first one
that is bigger than `global_steps`.
Example: decay from 0.1 to 0.01 in 10000 steps using sqrt (i.e. power=0.5): 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.double">double</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation.  Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Must
be positive.  See the decay computation above. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> end_learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python
number.  The minimal end learning rate. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> power
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.  The
power of the polynomial. Defaults to linear, 1.0. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> cycle
						</dt>
						<dd>A boolean, whether or not it should cycle beyond decay_steps. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String.  Optional name of the operation. Defaults to
'PolynomialDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            decayed_learning_rate = (learning_rate - end_learning_rate) *
                                    (1 - global_step / decay_steps) ^ (power) +
                                    end_learning_rate </pre>
</div>
		</div>
	</div>
	<div id="polynomial_decay_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>polynomial_decay_dyn</strong>(<span title="System.object">object</span> learning_rate, <span title="System.object">object</span> global_step, <span title="System.object">object</span> decay_steps, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> end_learning_rate, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> power, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> cycle, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies a polynomial decay to the learning rate. <p></p> It is commonly observed that a monotonically decreasing learning rate, whose
degree of change is carefully chosen, results in a better performing model.
This function applies a polynomial decay function to a provided initial
`learning_rate` to reach an `end_learning_rate` in the given `decay_steps`. <p></p> It requires a `global_step` value to compute the decayed learning rate.  You
can just pass a TensorFlow variable that you increment at each training step. <p></p> The function returns the decayed learning rate.  It is computed as:
If `cycle` is True then a multiple of `decay_steps` is used, the first one
that is bigger than `global_steps`.
Example: decay from 0.1 to 0.01 in 10000 steps using sqrt (i.e. power=0.5): 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.
The initial learning rate. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Global
step to use for the decay computation.  Must not be negative. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> decay_steps
						</dt>
						<dd>A scalar `int32` or `int64` `Tensor` or a Python number. Must
be positive.  See the decay computation above. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> end_learning_rate
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python
number.  The minimal end learning rate. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> power
						</dt>
						<dd>A scalar `float32` or `float64` `Tensor` or a Python number.  The
power of the polynomial. Defaults to linear, 1.0. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> cycle
						</dt>
						<dd>A boolean, whether or not it should cycle beyond decay_steps. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name of the operation. Defaults to
'PolynomialDecay'. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` of the same type as `learning_rate`.  The decayed
learning rate. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>global_step = min(global_step, decay_steps)
            decayed_learning_rate = (learning_rate - end_learning_rate) *
                                    (1 - global_step / decay_steps) ^ (power) +
                                    end_learning_rate </pre>
</div>
		</div>
	</div>
	<div id="range_input_producer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>range_input_producer</strong>(<span title="System.int">int</span> limit, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Produces the integers from 0 to limit-1 in a queue. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> limit
						</dt>
						<dd>An int32 scalar tensor. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>An integer (optional). If specified, `range_input_producer`
produces each integer `num_epochs` times before generating an
OutOfRange error. If not specified, `range_input_producer` can cycle
through the integers an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>Boolean. If true, the integers are randomly shuffled within each
epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>An integer (optional). Seed used if shuffle == True. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. Sets the queue capacity. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A Queue with the output integers.  A `QueueRunner` for the Queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="range_input_producer_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>range_input_producer_dyn</strong>(<span title="System.object">object</span> limit, <span title="System.object">object</span> num_epochs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> shuffle, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Produces the integers from 0 to limit-1 in a queue. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> limit
						</dt>
						<dd>An int32 scalar tensor. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_epochs
						</dt>
						<dd>An integer (optional). If specified, `range_input_producer`
produces each integer `num_epochs` times before generating an
OutOfRange error. If not specified, `range_input_producer` can cycle
through the integers an unlimited number of times. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> shuffle
						</dt>
						<dd>Boolean. If true, the integers are randomly shuffled within each
epoch. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>An integer (optional). Seed used if shuffle == True. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>An integer. Sets the queue capacity. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A Queue with the output integers.  A `QueueRunner` for the Queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="remove_checkpoint" class="method">
		<h4>
			<span title="System.void">void</span> <strong>remove_checkpoint</strong>(<span title="System.Byte[]">Byte[]</span> checkpoint_prefix, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> checkpoint_format_version, <span title="System.string">string</span> meta_graph_suffix)
		</h4>
		<div class="content">Removes a checkpoint given by `checkpoint_prefix`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> checkpoint_prefix
						</dt>
						<dd>The prefix of a V1 or V2 checkpoint. Typically the result
of `Saver.save()` or that of `tf.train.latest_checkpoint()`, regardless of
sharded/non-sharded or V1/V2. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> checkpoint_format_version
						</dt>
						<dd>`SaverDef.CheckpointFormatVersion`, defaults to
`SaverDef.V2`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="remove_checkpoint" class="method">
		<h4>
			<span title="System.void">void</span> <strong>remove_checkpoint</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> checkpoint_prefix, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> checkpoint_format_version, <span title="System.string">string</span> meta_graph_suffix)
		</h4>
		<div class="content">Removes a checkpoint given by `checkpoint_prefix`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> checkpoint_prefix
						</dt>
						<dd>The prefix of a V1 or V2 checkpoint. Typically the result
of `Saver.save()` or that of `tf.train.latest_checkpoint()`, regardless of
sharded/non-sharded or V1/V2. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> checkpoint_format_version
						</dt>
						<dd>`SaverDef.CheckpointFormatVersion`, defaults to
`SaverDef.V2`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="remove_checkpoint" class="method">
		<h4>
			<span title="System.void">void</span> <strong>remove_checkpoint</strong>(<span title="System.string">string</span> checkpoint_prefix, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> checkpoint_format_version, <span title="System.string">string</span> meta_graph_suffix)
		</h4>
		<div class="content">Removes a checkpoint given by `checkpoint_prefix`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> checkpoint_prefix
						</dt>
						<dd>The prefix of a V1 or V2 checkpoint. Typically the result
of `Saver.save()` or that of `tf.train.latest_checkpoint()`, regardless of
sharded/non-sharded or V1/V2. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> checkpoint_format_version
						</dt>
						<dd>`SaverDef.CheckpointFormatVersion`, defaults to
`SaverDef.V2`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="remove_checkpoint_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>remove_checkpoint_dyn</strong>(<span title="System.object">object</span> checkpoint_prefix, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> checkpoint_format_version, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> meta_graph_suffix)
		</h4>
		<div class="content">Removes a checkpoint given by `checkpoint_prefix`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> checkpoint_prefix
						</dt>
						<dd>The prefix of a V1 or V2 checkpoint. Typically the result
of `Saver.save()` or that of `tf.train.latest_checkpoint()`, regardless of
sharded/non-sharded or V1/V2. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> checkpoint_format_version
						</dt>
						<dd>`SaverDef.CheckpointFormatVersion`, defaults to
`SaverDef.V2`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.training/GreedyLoadBalancingStrategy.htm">GreedyLoadBalancingStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.training/GreedyLoadBalancingStrategy.htm">GreedyLoadBalancingStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.boosted_trees.training.functions.gbdt_batch/_OpRoundRobinStrategy.htm">_OpRoundRobinStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.boosted_trees.training.functions.gbdt_batch/_OpRoundRobinStrategy.htm">_OpRoundRobinStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.training/RandomStrategy.htm">RandomStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.training/RandomStrategy.htm">RandomStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <span title="System.ValueTuple<IDictionary<object, object>, PythonClassContainer>">ValueTuple&lt;IDictionary&lt;object, object&gt;, PythonClassContainer&gt;</span> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.training/GreedyLoadBalancingStrategy.htm">GreedyLoadBalancingStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<IDictionary<object, object>, PythonClassContainer>">ValueTuple&lt;IDictionary&lt;object, object&gt;, PythonClassContainer&gt;</span></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.training/GreedyLoadBalancingStrategy.htm">GreedyLoadBalancingStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <span title="System.ValueTuple<IDictionary<object, object>, PythonClassContainer>">ValueTuple&lt;IDictionary&lt;object, object&gt;, PythonClassContainer&gt;</span> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.training/RandomStrategy.htm">RandomStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<IDictionary<object, object>, PythonClassContainer>">ValueTuple&lt;IDictionary&lt;object, object&gt;, PythonClassContainer&gt;</span></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.training/RandomStrategy.htm">RandomStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.boosted_trees.training.functions.gbdt_batch/_OpRoundRobinStrategy.htm">_OpRoundRobinStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.boosted_trees.training.functions.gbdt_batch/_OpRoundRobinStrategy.htm">_OpRoundRobinStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <span title="System.ValueTuple<IDictionary<object, object>, PythonClassContainer>">ValueTuple&lt;IDictionary&lt;object, object&gt;, PythonClassContainer&gt;</span> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.boosted_trees.training.functions.gbdt_batch/_OpRoundRobinStrategy.htm">_OpRoundRobinStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<IDictionary<object, object>, PythonClassContainer>">ValueTuple&lt;IDictionary&lt;object, object&gt;, PythonClassContainer&gt;</span></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.boosted_trees.training.functions.gbdt_batch/_OpRoundRobinStrategy.htm">_OpRoundRobinStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.training/GreedyLoadBalancingStrategy.htm">GreedyLoadBalancingStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.training/GreedyLoadBalancingStrategy.htm">GreedyLoadBalancingStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter</strong>(<span title="System.int">int</span> ps_tasks, <span title="System.string">string</span> ps_device, <span title="System.string">string</span> worker_device, <span title="System.bool">bool</span> merge_devices, <a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a> cluster, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> ps_ops, <a href="../tensorflow.contrib.training/RandomStrategy.htm">RandomStrategy</a> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.training/RandomStrategy.htm">RandomStrategy</a></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="replica_device_setter_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>replica_device_setter_dyn</strong>(<a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> ps_tasks, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> ps_device, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> worker_device, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> merge_devices, <span title="System.object">object</span> cluster, <span title="System.object">object</span> ps_ops, <span title="System.object">object</span> ps_strategy)
		</h4>
		<div class="content">Return a `device function` to use when building a Graph for replicas. <p></p> Device Functions are used in `with tf.device(device_function):` statement to
automatically assign devices to `Operation` objects as they are constructed,
Device constraints are added from the inner-most context first, working
outwards. The merging behavior adds constraints to fields that are yet unset
by a more inner context. Currently the fields are (job, task, cpu/gpu). <p></p> If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op.
Otherwise, the value of `ps_tasks` is derived from `cluster`. <p></p> By default, only Variable ops are placed on ps tasks, and the placement
strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used
to do more intelligent placement, such as
<a href="..\..\tf\contrib\training\GreedyLoadBalancingStrategy.md"><code>tf.contrib.training.GreedyLoadBalancingStrategy</code></a>. <p></p> For example, 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> ps_tasks
						</dt>
						<dd>Number of tasks in the `ps` job.  Ignored if `cluster` is
provided. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> ps_device
						</dt>
						<dd>String.  Device of the `ps` job.  If empty no `ps` job is used.
Defaults to `ps`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> worker_device
						</dt>
						<dd>String.  Device of the `worker` job.  If empty no `worker`
job is used. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> merge_devices
						</dt>
						<dd>`Boolean`. If `True`, merges or only sets a device if the
device constraint is completely unset. merges device specification rather
than overriding them. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cluster
						</dt>
						<dd>`ClusterDef` proto or `ClusterSpec`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> ps_ops
						</dt>
						<dd>List of strings representing `Operation` types that need to be
placed on `ps` devices.  If `None`, defaults to `STANDARD_PS_OPS`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> ps_strategy
						</dt>
						<dd>A callable invoked for every ps `Operation` (i.e. matched by
`ps_ops`), that takes the `Operation` and returns the ps task index to
use.  If `None`, defaults to a round-robin strategy across all `ps`
devices. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A function to pass to `tf.device()`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
            # jobs on hosts worker0, worker1 and worker2.
            cluster_spec = {
                "ps": ["ps0:2222", "ps1:2222"],
                "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
            with
            tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
              # Build your graph
              v1 = tf.Variable(...)  # assigned to /job:ps/task:0
              v2 = tf.Variable(...)  # assigned to /job:ps/task:1
              v3 = tf.Variable(...)  # assigned to /job:ps/task:0
            # Run compute </pre>
</div>
		</div>
	</div>
	<div id="sdca_fprint" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>sdca_fprint</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> input, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Computes fingerprints of the input strings. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> input
						</dt>
						<dd>A `Tensor` of type `string`.
vector of strings to compute fingerprints on. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A `Tensor` of type `int64`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_fprint_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_fprint_dyn</strong>(<span title="System.object">object</span> input, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Computes fingerprints of the input strings. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> input
						</dt>
						<dd>A `Tensor` of type `string`.
vector of strings to compute fingerprints on. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `Tensor` of type `int64`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.int">int</span> loss_type, <span title="System.double">double</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.int">int</span> loss_type, <span title="System.int">int</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.bool">bool</span> loss_type, <span title="System.string">string</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.bool">bool</span> loss_type, <span title="System.double">double</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.bool">bool</span> loss_type, <span title="System.bool">bool</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.double">double</span> loss_type, <span title="System.int">int</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.int">int</span> loss_type, <span title="System.bool">bool</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.double">double</span> loss_type, <span title="System.string">string</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.int">int</span> loss_type, <span title="System.string">string</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.string">string</span> loss_type, <span title="System.bool">bool</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.bool">bool</span> loss_type, <span title="System.int">int</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.string">string</span> loss_type, <span title="System.double">double</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.double">double</span> loss_type, <span title="System.bool">bool</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.string">string</span> loss_type, <span title="System.string">string</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.double">double</span> loss_type, <span title="System.double">double</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_example_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_indices, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_feature_values, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_features, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_labels, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> sparse_indices, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> sparse_weights, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> dense_weights, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> example_state_data, <span title="System.string">string</span> loss_type, <span title="System.int">int</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.int">int</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_optimizer_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_optimizer_dyn</strong>(<span title="System.object">object</span> sparse_example_indices, <span title="System.object">object</span> sparse_feature_indices, <span title="System.object">object</span> sparse_feature_values, <span title="System.object">object</span> dense_features, <span title="System.object">object</span> example_weights, <span title="System.object">object</span> example_labels, <span title="System.object">object</span> sparse_indices, <span title="System.object">object</span> sparse_weights, <span title="System.object">object</span> dense_weights, <span title="System.object">object</span> example_state_data, <span title="System.object">object</span> loss_type, <span title="System.object">object</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> num_loss_partitions, <span title="System.object">object</span> num_inner_iterations, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> adaptative, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for <p></p> linear models with L1 + L2 regularization. As global optimization objective is
strongly-convex, the optimizer optimizes the dual objective at each step. The
optimizer applies each update one example at a time. Examples are sampled
uniformly, and the optimizer is learning rate free and enjoys linear convergence
rate. <p></p> [Proximal Stochastic Dual Coordinate Ascent](http://arxiv.org/pdf/1211.2717v1.pdf).<br>
Shai Shalev-Shwartz, Tong Zhang. 2012 <p></p> $$Loss Objective = \sum f_{i} (wx_{i}) + (l2 / 2) * |w|^2 + l1 * |w|$$ <p></p> [Adding vs. Averaging in Distributed Primal-Dual Optimization](http://arxiv.org/abs/1502.03508).<br>
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan,
Peter Richtarik, Martin Takac. 2015 <p></p> [Stochastic Dual Coordinate Ascent with Adaptive Probabilities](https://arxiv.org/abs/1502.08053).<br>
Dominik Csiba, Zheng Qu, Peter Richtarik. 2015 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> sparse_example_indices
						</dt>
						<dd>A list of `Tensor` objects with type `int64`.
a list of vectors which contain example indices. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> sparse_feature_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors which contain feature indices. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> sparse_feature_values
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of vectors which contains feature value
associated with each feature group. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> dense_features
						</dt>
						<dd>A list of `Tensor` objects with type `float32`.
a list of matrices which contains the dense feature values. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> example_weights
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the weight associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> example_labels
						</dt>
						<dd>A `Tensor` of type `float32`.
a vector which contains the label/target associated with each
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> sparse_indices
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `int64`.
a list of vectors where each value is the indices which has
corresponding weights in sparse_weights. This field maybe omitted for the
dense approach. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> sparse_weights
						</dt>
						<dd>A list with the same length as `sparse_example_indices` of `Tensor` objects with type `float32`.
a list of vectors where each value is the weight associated with
a sparse feature group. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> dense_weights
						</dt>
						<dd>A list with the same length as `dense_features` of `Tensor` objects with type `float32`.
a list of vectors where the values are the weights associated
with a dense feature group. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> example_state_data
						</dt>
						<dd>A `Tensor` of type `float32`.
a list of vectors containing the example state data. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> loss_type
						</dt>
						<dd>A `string` from: `"logistic_loss", "squared_loss", "hinge_loss", "smooth_hinge_loss", "poisson_loss"`.
Type of the primal loss. Currently SdcaSolver supports logistic,
squared and hinge losses. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`. Symmetric l2 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_loss_partitions
						</dt>
						<dd>An `int` that is `>= 1`.
Number of partitions of the global loss function. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_inner_iterations
						</dt>
						<dd>An `int` that is `>= 1`.
Number of iterations per mini-batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> adaptative
						</dt>
						<dd>An optional `bool`. Defaults to `True`.
Whether to use Adaptive SDCA for the inner loop. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A tuple of `Tensor` objects (out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_shrink_l1" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_shrink_l1</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> weights, <span title="System.int">int</span> l1, <span title="System.double">double</span> l2, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies L1 regularization shrink step on the parameters. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> weights
						</dt>
						<dd>A list of `Tensor` objects with type mutable `float32`.
a list of vectors where each value is the weight associated with a
feature group. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> l2
						</dt>
						<dd>A `float`.
Symmetric l2 regularization strength. Should be a positive float. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The created Operation. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_shrink_l1" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_shrink_l1</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> weights, <span title="System.int">int</span> l1, <span title="System.int">int</span> l2, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies L1 regularization shrink step on the parameters. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> weights
						</dt>
						<dd>A list of `Tensor` objects with type mutable `float32`.
a list of vectors where each value is the weight associated with a
feature group. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> l2
						</dt>
						<dd>A `float`.
Symmetric l2 regularization strength. Should be a positive float. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The created Operation. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sdca_shrink_l1_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sdca_shrink_l1_dyn</strong>(<span title="System.object">object</span> weights, <span title="System.object">object</span> l1, <span title="System.object">object</span> l2, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies L1 regularization shrink step on the parameters. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> weights
						</dt>
						<dd>A list of `Tensor` objects with type mutable `float32`.
a list of vectors where each value is the weight associated with a
feature group. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l1
						</dt>
						<dd>A `float`. Symmetric l1 regularization strength. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> l2
						</dt>
						<dd>A `float`.
Symmetric l2 regularization strength. Should be a positive float. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A name for the operation (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The created Operation. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`. <p></p> This function adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors`. <p></p> If `enqueue_many` is `False`, `tensors` is assumed to represent a
single example.  An input tensor with shape `[x, y, z]` will be output
as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors` is assumed to represent a
batch of examples, where the first dimension is indexed by example,
and all members of `tensors` should have the same size in the
first dimension.  If an input tensor has shape `[*, x, y, z]`, the
output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself.
*N.B.:* You must ensure that either (i) the `shapes` argument is
passed, or (ii) all of the tensors in `tensors` must have
fully-defined shapes. `ValueError` will be raised if neither of
these conditions holds. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Creates batches of 32 images and 32 labels.
            image_batch, label_batch = tf.compat.v1.train.shuffle_batch(
                  [single_image, single_label],
                  batch_size=32,
                  num_threads=4,
                  capacity=50000,
                  min_after_dequeue=10000) </pre>
</div>
		</div>
	</div>
	<div id="shuffle_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch</strong>(<span title="System.Collections.Generic.IDictionary<string, string>">IDictionary&lt;string, string&gt;</span> tensors, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.int">int</span> num_threads, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`. <p></p> This function adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors`. <p></p> If `enqueue_many` is `False`, `tensors` is assumed to represent a
single example.  An input tensor with shape `[x, y, z]` will be output
as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors` is assumed to represent a
batch of examples, where the first dimension is indexed by example,
and all members of `tensors` should have the same size in the
first dimension.  If an input tensor has shape `[*, x, y, z]`, the
output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself.
*N.B.:* You must ensure that either (i) the `shapes` argument is
passed, or (ii) all of the tensors in `tensors` must have
fully-defined shapes. `ValueError` will be raised if neither of
these conditions holds. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, string>">IDictionary&lt;string, string&gt;</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Creates batches of 32 images and 32 labels.
            image_batch, label_batch = tf.compat.v1.train.shuffle_batch(
                  [single_image, single_label],
                  batch_size=32,
                  num_threads=4,
                  capacity=50000,
                  min_after_dequeue=10000) </pre>
</div>
		</div>
	</div>
	<div id="shuffle_batch_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_dyn</strong>(<span title="System.object">object</span> tensors, <span title="System.object">object</span> batch_size, <span title="System.object">object</span> capacity, <span title="System.object">object</span> min_after_dequeue, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> num_threads, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Creates batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`. <p></p> This function adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors`. <p></p> If `enqueue_many` is `False`, `tensors` is assumed to represent a
single example.  An input tensor with shape `[x, y, z]` will be output
as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors` is assumed to represent a
batch of examples, where the first dimension is indexed by example,
and all members of `tensors` should have the same size in the
first dimension.  If an input tensor has shape `[*, x, y, z]`, the
output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself.
*N.B.:* You must ensure that either (i) the `shapes` argument is
passed, or (ii) all of the tensors in `tensors` must have
fully-defined shapes. `ValueError` will be raised if neither of
these conditions holds. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>The list or dictionary of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> num_threads
						</dt>
						<dd>The number of threads enqueuing `tensor_list`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list` is a single example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensor_list`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(Optional) If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the types as `tensors`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre># Creates batches of 32 images and 32 labels.
            image_batch, label_batch = tf.compat.v1.train.shuffle_batch(
                  [single_image, single_label],
                  batch_size=32,
                  num_threads=4,
                  capacity=50000,
                  min_after_dequeue=10000) </pre>
</div>
		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> batch_size, <span title="System.int">int</span> capacity, <span title="System.int">int</span> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> batch_size, <span title="System.int">int</span> capacity, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> batch_size, <span title="System.int">int</span> capacity, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join</strong>(<span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span> tensors_list, <span title="System.int">int</span> batch_size, <span title="System.int">int</span> capacity, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> min_after_dequeue, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> enqueue_many, <span title="System.object">object</span> shapes, <span title="System.bool">bool</span> allow_smaller_final_batch, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IDictionary<string, string>>">IEnumerable&lt;IDictionary&lt;string, string&gt;&gt;</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shuffle_batch_join_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shuffle_batch_join_dyn</strong>(<span title="System.object">object</span> tensors_list, <span title="System.object">object</span> batch_size, <span title="System.object">object</span> capacity, <span title="System.object">object</span> min_after_dequeue, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> enqueue_many, <span title="System.object">object</span> shapes, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_smaller_final_batch, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Create batches by randomly shuffling tensors. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`. <p></p> The `tensors_list` argument is a list of tuples of tensors, or a list of
dictionaries of tensors.  Each element in the list is treated similarly
to the `tensors` argument of `tf.compat.v1.train.shuffle_batch()`. <p></p> This version enqueues a different list of tensors in different threads.
It adds the following to the current `Graph`: <p></p> * A shuffling queue into which tensors from `tensors_list` are enqueued.
* A `dequeue_many` operation to create batches from the queue.
* A `QueueRunner` to `QUEUE_RUNNER` collection, to enqueue the tensors
from `tensors_list`. <p></p> `len(tensors_list)` threads will be started, with thread `i` enqueuing
the tensors from `tensors_list[i]`. `tensors_list[i1][j]` must match
`tensors_list[i2][j]` in type and shape, except in the first dimension if
`enqueue_many` is true. <p></p> If `enqueue_many` is `False`, each `tensors_list[i]` is assumed
to represent a single example.  An input tensor with shape `[x, y, z]`
will be output as a tensor with shape `[batch_size, x, y, z]`. <p></p> If `enqueue_many` is `True`, `tensors_list[i]` is assumed to
represent a batch of examples, where the first dimension is indexed
by example, and all members of `tensors_list[i]` should have the
same size in the first dimension.  If an input tensor has shape `[*, x,
y, z]`, the output will have shape `[batch_size, x, y, z]`. <p></p> The `capacity` argument controls the how long the prefetching is allowed to
grow the queues. <p></p> The returned operation is a dequeue operation and will throw
<a href="..\..\tf\errors\OutOfRangeError.md"><code>tf.errors.OutOfRangeError</code></a> if the input queue is exhausted. If this
operation is feeding another input queue, its queue runner will catch
this exception, however, if this operation is used in your main thread
you are responsible for catching this yourself. <p></p> If `allow_smaller_final_batch` is `True`, a smaller batch value than
`batch_size` is returned when the queue is closed and there are not enough
elements to fill the batch, otherwise the pending elements are discarded.
In addition, all output tensors' static shapes, as accessed via the
`shape` property will have a first `Dimension` value of `None`, and
operations that depend on fixed batch_size would fail. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors_list
						</dt>
						<dd>A list of tuples or dictionaries of tensors to enqueue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>An integer. The new batch size pulled from the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> capacity
						</dt>
						<dd>An integer. The maximum number of elements in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> min_after_dequeue
						</dt>
						<dd>Minimum number elements in the queue after a
dequeue, used to ensure a level of mixing of elements. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>Seed for the random shuffling within the queue. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> enqueue_many
						</dt>
						<dd>Whether each tensor in `tensor_list_list` is a single
example. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shapes
						</dt>
						<dd>(Optional) The shapes for each example.  Defaults to the
inferred shapes for `tensors_list[i]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_smaller_final_batch
						</dt>
						<dd>(Optional) Boolean. If `True`, allow the final
batch to be smaller if there are insufficient items left in the queue. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>(Optional) A name for the operations. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list or dictionary of tensors with the same number and types as
`tensors_list[i]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="slice_input_producer" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<Tensor>">IList&lt;Tensor&gt;</span> <strong>slice_input_producer</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> tensor_list, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Produces a slice of each `Tensor` in `tensor_list`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Implemented using a Queue -- a `QueueRunner` for the Queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> tensor_list
						</dt>
						<dd>A list of `Tensor` objects. Every `Tensor` in
`tensor_list` must have the same size in the first dimension. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>An integer (optional). If specified, `slice_input_producer`
produces each slice `num_epochs` times before generating
an `OutOfRange` error. If not specified, `slice_input_producer` can cycle
through the slices an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>Boolean. If true, the integers are randomly shuffled within each
epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>An integer (optional). Seed used if shuffle == True. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. Sets the queue capacity. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<Tensor>">IList&lt;Tensor&gt;</span></code>
					</dt>
					<dd>A list of tensors, one for each element of `tensor_list`.  If the tensor
in `tensor_list` has shape `[N, a, b,.., z]`, then the corresponding output
tensor will have shape `[a, b,..., z]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="slice_input_producer_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>slice_input_producer_dyn</strong>(<span title="System.object">object</span> tensor_list, <span title="System.object">object</span> num_epochs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> shuffle, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Produces a slice of each `Tensor` in `tensor_list`. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Implemented using a Queue -- a `QueueRunner` for the Queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensor_list
						</dt>
						<dd>A list of `Tensor` objects. Every `Tensor` in
`tensor_list` must have the same size in the first dimension. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_epochs
						</dt>
						<dd>An integer (optional). If specified, `slice_input_producer`
produces each slice `num_epochs` times before generating
an `OutOfRange` error. If not specified, `slice_input_producer` can cycle
through the slices an unlimited number of times. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> shuffle
						</dt>
						<dd>Boolean. If true, the integers are randomly shuffled within each
epoch. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>An integer (optional). Seed used if shuffle == True. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>An integer. Sets the queue capacity. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list of tensors, one for each element of `tensor_list`.  If the tensor
in `tensor_list` has shape `[N, a, b,.., z]`, then the corresponding output
tensor will have shape `[a, b,..., z]`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="start_queue_runners" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>start_queue_runners</strong>(<a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a> sess, <a href="../tensorflow.train/Coordinator.htm">Coordinator</a> coord, <span title="System.bool">bool</span> daemon, <span title="System.bool">bool</span> start, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Starts all queue runners collected in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> This is a companion method to `add_queue_runner()`.  It just starts
threads for all queue runners collected in the graph.  It returns
the list of all threads. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a></code> sess
						</dt>
						<dd>`Session` used to run the queue ops.  Defaults to the
default session. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Coordinator.htm">Coordinator</a></code> coord
						</dt>
						<dd>Optional `Coordinator` for coordinating the started threads. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> daemon
						</dt>
						<dd>Whether the threads should be marked as `daemons`, meaning
they don't block program exit. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> start
						</dt>
						<dd>Set to `False` to only create the threads, not start them. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to
get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span></code>
					</dt>
					<dd>A list of threads. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="start_queue_runners" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>start_queue_runners</strong>(<span title="System.string">string</span> sess, <a href="../tensorflow.train/Coordinator.htm">Coordinator</a> coord, <span title="System.bool">bool</span> daemon, <span title="System.bool">bool</span> start, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Starts all queue runners collected in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> This is a companion method to `add_queue_runner()`.  It just starts
threads for all queue runners collected in the graph.  It returns
the list of all threads. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> sess
						</dt>
						<dd>`Session` used to run the queue ops.  Defaults to the
default session. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Coordinator.htm">Coordinator</a></code> coord
						</dt>
						<dd>Optional `Coordinator` for coordinating the started threads. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> daemon
						</dt>
						<dd>Whether the threads should be marked as `daemons`, meaning
they don't block program exit. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> start
						</dt>
						<dd>Set to `False` to only create the threads, not start them. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to
get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span></code>
					</dt>
					<dd>A list of threads. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="start_queue_runners" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>start_queue_runners</strong>(<a href="../tensorflow/Session.htm">Session</a> sess, <a href="../tensorflow.train/Coordinator.htm">Coordinator</a> coord, <span title="System.bool">bool</span> daemon, <span title="System.bool">bool</span> start, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Starts all queue runners collected in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> This is a companion method to `add_queue_runner()`.  It just starts
threads for all queue runners collected in the graph.  It returns
the list of all threads. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/Session.htm">Session</a></code> sess
						</dt>
						<dd>`Session` used to run the queue ops.  Defaults to the
default session. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Coordinator.htm">Coordinator</a></code> coord
						</dt>
						<dd>Optional `Coordinator` for coordinating the started threads. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> daemon
						</dt>
						<dd>Whether the threads should be marked as `daemons`, meaning
they don't block program exit. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> start
						</dt>
						<dd>Set to `False` to only create the threads, not start them. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to
get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span></code>
					</dt>
					<dd>A list of threads. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="start_queue_runners" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>start_queue_runners</strong>(<a href="../tensorflow.python.training.monitored_session/_CoordinatedSession.htm">_CoordinatedSession</a> sess, <a href="../tensorflow.train/Coordinator.htm">Coordinator</a> coord, <span title="System.bool">bool</span> daemon, <span title="System.bool">bool</span> start, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Starts all queue runners collected in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> This is a companion method to `add_queue_runner()`.  It just starts
threads for all queue runners collected in the graph.  It returns
the list of all threads. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_CoordinatedSession.htm">_CoordinatedSession</a></code> sess
						</dt>
						<dd>`Session` used to run the queue ops.  Defaults to the
default session. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Coordinator.htm">Coordinator</a></code> coord
						</dt>
						<dd>Optional `Coordinator` for coordinating the started threads. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> daemon
						</dt>
						<dd>Whether the threads should be marked as `daemons`, meaning
they don't block program exit. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> start
						</dt>
						<dd>Set to `False` to only create the threads, not start them. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to
get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span></code>
					</dt>
					<dd>A list of threads. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="start_queue_runners" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>start_queue_runners</strong>(<a href="../tensorflow.python.debug/LocalCLIDebugWrapperSession.htm">LocalCLIDebugWrapperSession</a> sess, <a href="../tensorflow.train/Coordinator.htm">Coordinator</a> coord, <span title="System.bool">bool</span> daemon, <span title="System.bool">bool</span> start, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Starts all queue runners collected in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> This is a companion method to `add_queue_runner()`.  It just starts
threads for all queue runners collected in the graph.  It returns
the list of all threads. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.debug/LocalCLIDebugWrapperSession.htm">LocalCLIDebugWrapperSession</a></code> sess
						</dt>
						<dd>`Session` used to run the queue ops.  Defaults to the
default session. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Coordinator.htm">Coordinator</a></code> coord
						</dt>
						<dd>Optional `Coordinator` for coordinating the started threads. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> daemon
						</dt>
						<dd>Whether the threads should be marked as `daemons`, meaning
they don't block program exit. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> start
						</dt>
						<dd>Set to `False` to only create the threads, not start them. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to
get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span></code>
					</dt>
					<dd>A list of threads. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="start_queue_runners_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>start_queue_runners_dyn</strong>(<span title="System.object">object</span> sess, <span title="System.object">object</span> coord, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> daemon, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> start, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> collection)
		</h4>
		<div class="content">Starts all queue runners collected in the graph. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the <a href="..\..\..\tf\data.md"><code>tf.data</code></a> module. <p></p> This is a companion method to `add_queue_runner()`.  It just starts
threads for all queue runners collected in the graph.  It returns
the list of all threads. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> sess
						</dt>
						<dd>`Session` used to run the queue ops.  Defaults to the
default session. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> coord
						</dt>
						<dd>Optional `Coordinator` for coordinating the started threads. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> daemon
						</dt>
						<dd>Whether the threads should be marked as `daemons`, meaning
they don't block program exit. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> start
						</dt>
						<dd>Set to `False` to only create the threads, not start them. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> collection
						</dt>
						<dd>A `GraphKey` specifying the graph collection to
get the queue runners from.  Defaults to `GraphKeys.QUEUE_RUNNERS`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list of threads. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="string_input_producer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>string_input_producer</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> string_tensor, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output strings (e.g. filenames) to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> string_tensor
						</dt>
						<dd>A 1-D string tensor with the strings to produce. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>An integer (optional). If specified, `string_input_producer`
produces each string from `string_tensor` `num_epochs` times before
generating an `OutOfRange` error. If not specified,
`string_input_producer` can cycle through the strings in `string_tensor`
an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>Boolean. If true, the strings are randomly shuffled within each
epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>An integer (optional). Seed used if shuffle == True. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. Sets the queue capacity. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. All sessions open to the device which has
this queue will be able to access it via the shared_name. Using this in
a distributed setting means each name will only be seen by one of the
sessions which has access to this operation. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>Cancel op for the queue (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output strings.  A `QueueRunner` for the Queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="string_input_producer" class="method">
		<h4>
			<span title="System.object">object</span> <strong>string_input_producer</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> string_tensor, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> num_epochs, <span title="System.bool">bool</span> shuffle, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.int">int</span> capacity, <span title="System.string">string</span> shared_name, <span title="System.string">string</span> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output strings (e.g. filenames) to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> string_tensor
						</dt>
						<dd>A 1-D string tensor with the strings to produce. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> num_epochs
						</dt>
						<dd>An integer (optional). If specified, `string_input_producer`
produces each string from `string_tensor` `num_epochs` times before
generating an `OutOfRange` error. If not specified,
`string_input_producer` can cycle through the strings in `string_tensor`
an unlimited number of times. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> shuffle
						</dt>
						<dd>Boolean. If true, the strings are randomly shuffled within each
epoch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>An integer (optional). Seed used if shuffle == True. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> capacity
						</dt>
						<dd>An integer. Sets the queue capacity. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. All sessions open to the device which has
this queue will be able to access it via the shared_name. Using this in
a distributed setting means each name will only be seen by one of the
sessions which has access to this operation. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>Cancel op for the queue (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output strings.  A `QueueRunner` for the Queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="string_input_producer_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>string_input_producer_dyn</strong>(<span title="System.object">object</span> string_tensor, <span title="System.object">object</span> num_epochs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> shuffle, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> capacity, <span title="System.object">object</span> shared_name, <span title="System.object">object</span> name, <span title="System.object">object</span> cancel_op)
		</h4>
		<div class="content">Output strings (e.g. filenames) to a queue for an input pipeline. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by <a href="..\..\tf\data.md"><code>tf.data</code></a>. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`. <p></p> Note: if `num_epochs` is not `None`, this function creates local counter
`epochs`. Use `local_variables_initializer()` to initialize local variables. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> string_tensor
						</dt>
						<dd>A 1-D string tensor with the strings to produce. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> num_epochs
						</dt>
						<dd>An integer (optional). If specified, `string_input_producer`
produces each string from `string_tensor` `num_epochs` times before
generating an `OutOfRange` error. If not specified,
`string_input_producer` can cycle through the strings in `string_tensor`
an unlimited number of times. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> shuffle
						</dt>
						<dd>Boolean. If true, the strings are randomly shuffled within each
epoch. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>An integer (optional). Seed used if shuffle == True. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> capacity
						</dt>
						<dd>An integer. Sets the queue capacity. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> shared_name
						</dt>
						<dd>(optional). If set, this queue will be shared under the given
name across multiple sessions. All sessions open to the device which has
this queue will be able to access it via the shared_name. Using this in
a distributed setting means each name will only be seen by one of the
sessions which has access to this operation. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A name for the operations (optional). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> cancel_op
						</dt>
						<dd>Cancel op for the queue (optional). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A queue with the output strings.  A `QueueRunner` for the Queue
is added to the current `Graph`'s `QUEUE_RUNNER` collection. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="summary_iterator" class="method">
		<h4>
			<span title="System.Collections.Generic.IEnumerator<object>">IEnumerator&lt;object&gt;</span> <strong>summary_iterator</strong>(<span title="System.string">string</span> path)
		</h4>
		<div class="content">An iterator for reading `Event` protocol buffers from an event file. <p></p> You can use this function to read events written to an event file. It returns
a Python iterator that yields `Event` protocol buffers. <p></p> Example: Print the contents of an events file.
Example: Print selected summary values.
See the protocol buffer definitions of
[Event](https://www.tensorflow.org/code/tensorflow/core/util/event.proto)
and
[Summary](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)
for more information about their attributes. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> path
						</dt>
						<dd>The path to an event file created by a `SummaryWriter`. 
						</dd>
				</dl>
			</div>

<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>for e in tf.compat.v1.train.summary_iterator(path to events file):
                print(e) </pre>
</div>
		</div>
	</div>
	<div id="summary_iterator" class="method">
		<h4>
			<span title="System.Collections.Generic.IEnumerator<object>">IEnumerator&lt;object&gt;</span> <strong>summary_iterator</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> path)
		</h4>
		<div class="content">An iterator for reading `Event` protocol buffers from an event file. <p></p> You can use this function to read events written to an event file. It returns
a Python iterator that yields `Event` protocol buffers. <p></p> Example: Print the contents of an events file.
Example: Print selected summary values.
See the protocol buffer definitions of
[Event](https://www.tensorflow.org/code/tensorflow/core/util/event.proto)
and
[Summary](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)
for more information about their attributes. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> path
						</dt>
						<dd>The path to an event file created by a `SummaryWriter`. 
						</dd>
				</dl>
			</div>

<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>for e in tf.compat.v1.train.summary_iterator(path to events file):
                print(e) </pre>
</div>
		</div>
	</div>
	<div id="summary_iterator_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>summary_iterator_dyn</strong>(<span title="System.object">object</span> path)
		</h4>
		<div class="content">An iterator for reading `Event` protocol buffers from an event file. <p></p> You can use this function to read events written to an event file. It returns
a Python iterator that yields `Event` protocol buffers. <p></p> Example: Print the contents of an events file.
Example: Print selected summary values.
See the protocol buffer definitions of
[Event](https://www.tensorflow.org/code/tensorflow/core/util/event.proto)
and
[Summary](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)
for more information about their attributes. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> path
						</dt>
						<dd>The path to an event file created by a `SummaryWriter`. 
						</dd>
				</dl>
			</div>

<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>for e in tf.compat.v1.train.summary_iterator(path to events file):
                print(e) </pre>
</div>
		</div>
	</div>
	<div id="update_checkpoint_state" class="method">
		<h4>
			<span title="System.void">void</span> <strong>update_checkpoint_state</strong>(<span title="System.string">string</span> save_dir, <span title="System.object">object</span> model_checkpoint_path, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> all_model_checkpoint_paths, <span title="System.object">object</span> latest_filename, <span title="System.object">object</span> all_model_checkpoint_timestamps, <span title="System.object">object</span> last_preserved_timestamp)
		</h4>
		<div class="content">Updates the content of the 'checkpoint' file. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\train\CheckpointManager.md"><code>tf.train.CheckpointManager</code></a> to manage checkpoints rather than manually editing the Checkpoint proto. <p></p> This updates the checkpoint file containing a CheckpointState
proto. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> save_dir
						</dt>
						<dd>Directory where the model was saved. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> model_checkpoint_path
						</dt>
						<dd>The checkpoint file. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> all_model_checkpoint_paths
						</dt>
						<dd>List of strings.  Paths to all not-yet-deleted
checkpoints, sorted from oldest to newest.  If this is a non-empty list,
the last element must be equal to model_checkpoint_path.  These paths
are also saved in the CheckpointState proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> latest_filename
						</dt>
						<dd>Optional name of the checkpoint file.  Default to
'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_timestamps
						</dt>
						<dd>Optional list of timestamps (floats,
seconds since the Epoch) indicating when the checkpoints in
`all_model_checkpoint_paths` were created. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> last_preserved_timestamp
						</dt>
						<dd>A float, indicating the number of seconds since
the Epoch when the last preserved checkpoint was written, e.g. due to a
`keep_checkpoint_every_n_hours` parameter (see
<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for an implementation). 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="update_checkpoint_state_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>update_checkpoint_state_dyn</strong>(<span title="System.object">object</span> save_dir, <span title="System.object">object</span> model_checkpoint_path, <span title="System.object">object</span> all_model_checkpoint_paths, <span title="System.object">object</span> latest_filename, <span title="System.object">object</span> all_model_checkpoint_timestamps, <span title="System.object">object</span> last_preserved_timestamp)
		</h4>
		<div class="content">Updates the content of the 'checkpoint' file. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\train\CheckpointManager.md"><code>tf.train.CheckpointManager</code></a> to manage checkpoints rather than manually editing the Checkpoint proto. <p></p> This updates the checkpoint file containing a CheckpointState
proto. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> save_dir
						</dt>
						<dd>Directory where the model was saved. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> model_checkpoint_path
						</dt>
						<dd>The checkpoint file. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_paths
						</dt>
						<dd>List of strings.  Paths to all not-yet-deleted
checkpoints, sorted from oldest to newest.  If this is a non-empty list,
the last element must be equal to model_checkpoint_path.  These paths
are also saved in the CheckpointState proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> latest_filename
						</dt>
						<dd>Optional name of the checkpoint file.  Default to
'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> all_model_checkpoint_timestamps
						</dt>
						<dd>Optional list of timestamps (floats,
seconds since the Epoch) indicating when the checkpoints in
`all_model_checkpoint_paths` were created. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> last_preserved_timestamp
						</dt>
						<dd>A float, indicating the number of seconds since
the Epoch when the last preserved checkpoint was written, e.g. due to a
`keep_checkpoint_every_n_hours` parameter (see
<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for an implementation). 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="warm_start" class="method">
		<h4>
			<span title="System.void">void</span> <strong>warm_start</strong>(<span title="System.string">string</span> ckpt_to_initialize_from, <span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span> vars_to_warm_start, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> var_name_to_vocab_info, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> var_name_to_prev_var_name)
		</h4>
		<div class="content">Warm-starts a model using the given settings. <p></p> If you are using a tf.estimator.Estimator, this will automatically be called
during training. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> ckpt_to_initialize_from
						</dt>
						<dd>[Required] A string specifying the directory with
checkpoint file(s) or path to checkpoint from which to warm-start the
model parameters. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<string>">IEnumerable&lt;string&gt;</span></code> vars_to_warm_start
						</dt>
						<dd>[Optional] One of the following: <p></p> - A regular expression (string) that captures which variables to
warm-start (see tf.compat.v1.get_collection).  This expression will only
consider variables in the TRAINABLE_VARIABLES collection -- if you need
to warm-start non_TRAINABLE vars (such as optimizer accumulators or
batch norm statistics), please use the below option.
- A list of strings, each a regex scope provided to
tf.compat.v1.get_collection with GLOBAL_VARIABLES (please see
tf.compat.v1.get_collection).  For backwards compatibility reasons,
this is separate from the single-string argument type.
- A list of Variables to warm-start.  If you do not have access to the
`Variable` objects at the call site, please use the above option.
- `None`, in which case only TRAINABLE variables specified in
`var_name_to_vocab_info` will be warm-started. <p></p> Defaults to `'.*'`, which warm-starts all variables in the
TRAINABLE_VARIABLES collection.  Note that this excludes variables such
as accumulators and moving statistics from batch norm. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> var_name_to_vocab_info
						</dt>
						<dd>[Optional] Dict of variable names (strings) to
<a href="..\..\tf\train\VocabInfo.md"><code>tf.estimator.VocabInfo</code></a>. The variable names should be "full" variables,
not the names of the partitions.  If not explicitly provided, the variable
is assumed to have no (changes to) vocabulary. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> var_name_to_prev_var_name
						</dt>
						<dd>[Optional] Dict of variable names (strings) to
name of the previously-trained variable in `ckpt_to_initialize_from`. If
not explicitly provided, the name of the variable is assumed to be same
between previous checkpoint and current model.  Note that this has no
effect on the set of variables that is warm-started, and only controls
name mapping (use `vars_to_warm_start` for controlling what variables to
warm-start). 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="warm_start" class="method">
		<h4>
			<span title="System.void">void</span> <strong>warm_start</strong>(<span title="System.string">string</span> ckpt_to_initialize_from, <span title="System.string">string</span> vars_to_warm_start, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> var_name_to_vocab_info, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> var_name_to_prev_var_name)
		</h4>
		<div class="content">Warm-starts a model using the given settings. <p></p> If you are using a tf.estimator.Estimator, this will automatically be called
during training. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> ckpt_to_initialize_from
						</dt>
						<dd>[Required] A string specifying the directory with
checkpoint file(s) or path to checkpoint from which to warm-start the
model parameters. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> vars_to_warm_start
						</dt>
						<dd>[Optional] One of the following: <p></p> - A regular expression (string) that captures which variables to
warm-start (see tf.compat.v1.get_collection).  This expression will only
consider variables in the TRAINABLE_VARIABLES collection -- if you need
to warm-start non_TRAINABLE vars (such as optimizer accumulators or
batch norm statistics), please use the below option.
- A list of strings, each a regex scope provided to
tf.compat.v1.get_collection with GLOBAL_VARIABLES (please see
tf.compat.v1.get_collection).  For backwards compatibility reasons,
this is separate from the single-string argument type.
- A list of Variables to warm-start.  If you do not have access to the
`Variable` objects at the call site, please use the above option.
- `None`, in which case only TRAINABLE variables specified in
`var_name_to_vocab_info` will be warm-started. <p></p> Defaults to `'.*'`, which warm-starts all variables in the
TRAINABLE_VARIABLES collection.  Note that this excludes variables such
as accumulators and moving statistics from batch norm. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> var_name_to_vocab_info
						</dt>
						<dd>[Optional] Dict of variable names (strings) to
<a href="..\..\tf\train\VocabInfo.md"><code>tf.estimator.VocabInfo</code></a>. The variable names should be "full" variables,
not the names of the partitions.  If not explicitly provided, the variable
is assumed to have no (changes to) vocabulary. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> var_name_to_prev_var_name
						</dt>
						<dd>[Optional] Dict of variable names (strings) to
name of the previously-trained variable in `ckpt_to_initialize_from`. If
not explicitly provided, the name of the variable is assumed to be same
between previous checkpoint and current model.  Note that this has no
effect on the set of variables that is warm-started, and only controls
name mapping (use `vars_to_warm_start` for controlling what variables to
warm-start). 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="warm_start_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>warm_start_dyn</strong>(<span title="System.object">object</span> ckpt_to_initialize_from, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> vars_to_warm_start, <span title="System.object">object</span> var_name_to_vocab_info, <span title="System.object">object</span> var_name_to_prev_var_name)
		</h4>
		<div class="content">Warm-starts a model using the given settings. <p></p> If you are using a tf.estimator.Estimator, this will automatically be called
during training. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> ckpt_to_initialize_from
						</dt>
						<dd>[Required] A string specifying the directory with
checkpoint file(s) or path to checkpoint from which to warm-start the
model parameters. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> vars_to_warm_start
						</dt>
						<dd>[Optional] One of the following: <p></p> - A regular expression (string) that captures which variables to
warm-start (see tf.compat.v1.get_collection).  This expression will only
consider variables in the TRAINABLE_VARIABLES collection -- if you need
to warm-start non_TRAINABLE vars (such as optimizer accumulators or
batch norm statistics), please use the below option.
- A list of strings, each a regex scope provided to
tf.compat.v1.get_collection with GLOBAL_VARIABLES (please see
tf.compat.v1.get_collection).  For backwards compatibility reasons,
this is separate from the single-string argument type.
- A list of Variables to warm-start.  If you do not have access to the
`Variable` objects at the call site, please use the above option.
- `None`, in which case only TRAINABLE variables specified in
`var_name_to_vocab_info` will be warm-started. <p></p> Defaults to `'.*'`, which warm-starts all variables in the
TRAINABLE_VARIABLES collection.  Note that this excludes variables such
as accumulators and moving statistics from batch norm. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> var_name_to_vocab_info
						</dt>
						<dd>[Optional] Dict of variable names (strings) to
<a href="..\..\tf\train\VocabInfo.md"><code>tf.estimator.VocabInfo</code></a>. The variable names should be "full" variables,
not the names of the partitions.  If not explicitly provided, the variable
is assumed to have no (changes to) vocabulary. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> var_name_to_prev_var_name
						</dt>
						<dd>[Optional] Dict of variable names (strings) to
name of the previously-trained variable in `ckpt_to_initialize_from`. If
not explicitly provided, the name of the variable is assumed to be same
between previous checkpoint and current model.  Note that this has no
effect on the set of variables that is warm-started, and only controls
name mapping (use `vars_to_warm_start` for controlling what variables to
warm-start). 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="add_queue_runner_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>add_queue_runner_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="assert_global_step_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>assert_global_step_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="basic_train_loop_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>basic_train_loop_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="batch_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>batch_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="batch_join_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>batch_join_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="checkpoint_exists_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>checkpoint_exists_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="checkpoints_iterator_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>checkpoints_iterator_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="cosine_decay_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>cosine_decay_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="cosine_decay_restarts_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>cosine_decay_restarts_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="create_global_step_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>create_global_step_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="do_quantize_training_on_graphdef_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>do_quantize_training_on_graphdef_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="exponential_decay_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>exponential_decay_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="export_meta_graph_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>export_meta_graph_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="generate_checkpoint_state_proto_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>generate_checkpoint_state_proto_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_checkpoint_mtimes_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_checkpoint_mtimes_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_checkpoint_state_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_checkpoint_state_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_global_step__fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_global_step__fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_or_create_global_step_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_or_create_global_step_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="global_step_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>global_step_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="import_meta_graph_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>import_meta_graph_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="init_from_checkpoint_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>init_from_checkpoint_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="input_producer_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>input_producer_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="inverse_time_decay_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>inverse_time_decay_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="latest_checkpoint_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>latest_checkpoint_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="limit_epochs_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>limit_epochs_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="linear_cosine_decay_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>linear_cosine_decay_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="list_variables_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>list_variables_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="load_checkpoint_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>load_checkpoint_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="load_variable_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>load_variable_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="maybe_batch_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>maybe_batch_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="maybe_batch_join_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>maybe_batch_join_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="maybe_shuffle_batch_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>maybe_shuffle_batch_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="maybe_shuffle_batch_join_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>maybe_shuffle_batch_join_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="MonitoredTrainingSession_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>MonitoredTrainingSession_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="natural_exp_decay_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>natural_exp_decay_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="noisy_linear_cosine_decay_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>noisy_linear_cosine_decay_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="piecewise_constant_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>piecewise_constant_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="polynomial_decay_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>polynomial_decay_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="range_input_producer_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>range_input_producer_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="remove_checkpoint_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>remove_checkpoint_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="replica_device_setter_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>replica_device_setter_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="sdca_fprint_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>sdca_fprint_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="sdca_optimizer_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>sdca_optimizer_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="sdca_shrink_l1_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>sdca_shrink_l1_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="shuffle_batch_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>shuffle_batch_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="shuffle_batch_join_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>shuffle_batch_join_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="slice_input_producer_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>slice_input_producer_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="start_queue_runners_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>start_queue_runners_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="string_input_producer_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>string_input_producer_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="summary_iterator_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>summary_iterator_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="update_checkpoint_state_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>update_checkpoint_state_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="warm_start_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>warm_start_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>