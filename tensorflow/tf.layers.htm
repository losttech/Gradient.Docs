<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>tf.layers - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow/AggregationMethod.htm">AggregationMethod</a>
        </li>
				<li>
            <a href="../tensorflow/ConditionalAccumulator.htm">ConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/ConditionalAccumulatorBase.htm">ConditionalAccumulatorBase</a>
        </li>
				<li>
            <a href="../tensorflow/constant_initializer.htm">constant_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/CriticalSection.htm">CriticalSection</a>
        </li>
				<li>
            <a href="../tensorflow/DeviceSpec.htm">DeviceSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Dimension.htm">Dimension</a>
        </li>
				<li>
            <a href="../tensorflow/DType.htm">DType</a>
        </li>
				<li>
            <a href="../tensorflow/FIFOQueue.htm">FIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLenFeature.htm">FixedLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLengthRecordReader.htm">FixedLengthRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLenSequenceFeature.htm">FixedLenSequenceFeature</a>
        </li>
				<li>
            <a href="../tensorflow/glorot_normal_initializer.htm">glorot_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/glorot_uniform_initializer.htm">glorot_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/GradientTape.htm">GradientTape</a>
        </li>
				<li>
            <a href="../tensorflow/Graph.htm">Graph</a>
        </li>
				<li>
            <a href="../tensorflow/Graph._ControlDependenciesController.htm">Graph._ControlDependenciesController</a>
        </li>
				<li>
            <a href="../tensorflow/Graph.I_ControlDependenciesController.htm">Graph.I_ControlDependenciesController</a>
        </li>
				<li>
            <a href="../tensorflow/GraphKeys.htm">GraphKeys</a>
        </li>
				<li>
            <a href="../tensorflow/HeadingAxes.htm">HeadingAxes</a>
        </li>
				<li>
            <a href="../tensorflow/IAggregationMethod.htm">IAggregationMethod</a>
        </li>
				<li>
            <a href="../tensorflow/IConditionalAccumulator.htm">IConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/IConditionalAccumulatorBase.htm">IConditionalAccumulatorBase</a>
        </li>
				<li>
            <a href="../tensorflow/Iconstant_initializer.htm">Iconstant_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/ICriticalSection.htm">ICriticalSection</a>
        </li>
				<li>
            <a href="../tensorflow/IdentityReader.htm">IdentityReader</a>
        </li>
				<li>
            <a href="../tensorflow/IDeviceSpec.htm">IDeviceSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IDimension.htm">IDimension</a>
        </li>
				<li>
            <a href="../tensorflow/IDType.htm">IDType</a>
        </li>
				<li>
            <a href="../tensorflow/IFIFOQueue.htm">IFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLenFeature.htm">IFixedLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLengthRecordReader.htm">IFixedLengthRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLenSequenceFeature.htm">IFixedLenSequenceFeature</a>
        </li>
				<li>
            <a href="../tensorflow/Iglorot_normal_initializer.htm">Iglorot_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Iglorot_uniform_initializer.htm">Iglorot_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IGradientTape.htm">IGradientTape</a>
        </li>
				<li>
            <a href="../tensorflow/IGraph.htm">IGraph</a>
        </li>
				<li>
            <a href="../tensorflow/IGraphKeys.htm">IGraphKeys</a>
        </li>
				<li>
            <a href="../tensorflow/IIdentityReader.htm">IIdentityReader</a>
        </li>
				<li>
            <a href="../tensorflow/IIndexedSlices.htm">IIndexedSlices</a>
        </li>
				<li>
            <a href="../tensorflow/IIndexedSlicesSpec.htm">IIndexedSlicesSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IInteractiveSession.htm">IInteractiveSession</a>
        </li>
				<li>
            <a href="../tensorflow/ILazyLoader.htm">ILazyLoader</a>
        </li>
				<li>
            <a href="../tensorflow/ILMDBReader.htm">ILMDBReader</a>
        </li>
				<li>
            <a href="../tensorflow/IModule.htm">IModule</a>
        </li>
				<li>
            <a href="../tensorflow/Iname_scope.htm">Iname_scope</a>
        </li>
				<li>
            <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a>
        </li>
				<li>
            <a href="../tensorflow/IndexedSlicesSpec.htm">IndexedSlicesSpec</a>
        </li>
				<li>
            <a href="../tensorflow/InteractiveSession.htm">InteractiveSession</a>
        </li>
				<li>
            <a href="../tensorflow/Iones_initializer.htm">Iones_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IOperation.htm">IOperation</a>
        </li>
				<li>
            <a href="../tensorflow/IOpError.htm">IOpError</a>
        </li>
				<li>
            <a href="../tensorflow/IOptionalSpec.htm">IOptionalSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Iorthogonal_initializer.htm">Iorthogonal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IPaddingFIFOQueue.htm">IPaddingFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IPriorityQueue.htm">IPriorityQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IQueueBase.htm">IQueueBase</a>
        </li>
				<li>
            <a href="../tensorflow/IRaggedTensor.htm">IRaggedTensor</a>
        </li>
				<li>
            <a href="../tensorflow/IRaggedTensorSpec.htm">IRaggedTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Irandom_normal_initializer.htm">Irandom_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Irandom_uniform_initializer.htm">Irandom_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IRandomShuffleQueue.htm">IRandomShuffleQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IReaderBase.htm">IReaderBase</a>
        </li>
				<li>
            <a href="../tensorflow/IRegisterGradient.htm">IRegisterGradient</a>
        </li>
				<li>
            <a href="../tensorflow/ISession.htm">ISession</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseConditionalAccumulator.htm">ISparseConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseFeature.htm">ISparseFeature</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensor.htm">ISparseTensor</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensorSpec.htm">ISparseTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensorValue.htm">ISparseTensorValue</a>
        </li>
				<li>
            <a href="../tensorflow/ITensor.htm">ITensor</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorArray.htm">ITensorArray</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorArraySpec.htm">ITensorArraySpec</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorShape.htm">ITensorShape</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorSpec.htm">ITensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/ITextLineReader.htm">ITextLineReader</a>
        </li>
				<li>
            <a href="../tensorflow/ITFRecordReader.htm">ITFRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/Itruncated_normal_initializer.htm">Itruncated_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/ITypeSpec.htm">ITypeSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IUnconnectedGradients.htm">IUnconnectedGradients</a>
        </li>
				<li>
            <a href="../tensorflow/Iuniform_unit_scaling_initializer.htm">Iuniform_unit_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IVariable.htm">IVariable</a>
        </li>
				<li>
            <a href="../tensorflow/Ivariable_scope.htm">Ivariable_scope</a>
        </li>
				<li>
            <a href="../tensorflow/IVariableScope.htm">IVariableScope</a>
        </li>
				<li>
            <a href="../tensorflow/Ivariance_scaling_initializer.htm">Ivariance_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IVarLenFeature.htm">IVarLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/IWholeFileReader.htm">IWholeFileReader</a>
        </li>
				<li>
            <a href="../tensorflow/Izeros_initializer.htm">Izeros_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/LazyLoader.htm">LazyLoader</a>
        </li>
				<li>
            <a href="../tensorflow/LMDBReader.htm">LMDBReader</a>
        </li>
				<li>
            <a href="../tensorflow/Module.htm">Module</a>
        </li>
				<li>
            <a href="../tensorflow/name_scope.htm">name_scope</a>
        </li>
				<li>
            <a href="../tensorflow/ones_initializer.htm">ones_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Operation.htm">Operation</a>
        </li>
				<li>
            <a href="../tensorflow/Operation._InputList.htm">Operation._InputList</a>
        </li>
				<li>
            <a href="../tensorflow/Operation.I_InputList.htm">Operation.I_InputList</a>
        </li>
				<li>
            <a href="../tensorflow/OpError.htm">OpError</a>
        </li>
				<li>
            <a href="../tensorflow/OptionalSpec.htm">OptionalSpec</a>
        </li>
				<li>
            <a href="../tensorflow/orthogonal_initializer.htm">orthogonal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/PaddingFIFOQueue.htm">PaddingFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/PriorityQueue.htm">PriorityQueue</a>
        </li>
				<li>
            <a href="../tensorflow/QueueBase.htm">QueueBase</a>
        </li>
				<li>
            <a href="../tensorflow/RaggedTensor.htm">RaggedTensor</a>
        </li>
				<li>
            <a href="../tensorflow/RaggedTensorSpec.htm">RaggedTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/random_normal_initializer.htm">random_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/random_uniform_initializer.htm">random_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/RandomShuffleQueue.htm">RandomShuffleQueue</a>
        </li>
				<li>
            <a href="../tensorflow/ReaderBase.htm">ReaderBase</a>
        </li>
				<li>
            <a href="../tensorflow/RegisterGradient.htm">RegisterGradient</a>
        </li>
				<li>
            <a href="../tensorflow/Session.htm">Session</a>
        </li>
				<li>
            <a href="../tensorflow/SparseConditionalAccumulator.htm">SparseConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/SparseFeature.htm">SparseFeature</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensor.htm">SparseTensor</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensorSpec.htm">SparseTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensorValue.htm">SparseTensorValue</a>
        </li>
				<li>
            <a href="../tensorflow/Tensor.htm">Tensor</a>
        </li>
				<li>
            <a href="../tensorflow/Tensor`1.htm">Tensor&lt;T&gt;</a>
        </li>
				<li>
            <a href="../tensorflow/TensorArray.htm">TensorArray</a>
        </li>
				<li>
            <a href="../tensorflow/TensorArraySpec.htm">TensorArraySpec</a>
        </li>
				<li>
            <a href="../tensorflow/TensorDimension.htm">TensorDimension</a>
        </li>
				<li>
            <a href="../tensorflow/TensorDimensionSlice.htm">TensorDimensionSlice</a>
        </li>
				<li>
            <a href="../tensorflow/TensorShape.htm">TensorShape</a>
        </li>
				<li>
            <a href="../tensorflow/TensorSpec.htm">TensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/TextLineReader.htm">TextLineReader</a>
        </li>
				<li>
            <a href="../tensorflow/tf.htm">tf</a>
        </li>
				<li>
            <a href="../tensorflow/tf.audio.htm">tf.audio</a>
        </li>
				<li>
            <a href="../tensorflow/tf.autograph.htm">tf.autograph</a>
        </li>
				<li>
            <a href="../tensorflow/tf.autograph.experimental.htm">tf.autograph.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.bitwise.htm">tf.bitwise</a>
        </li>
				<li>
            <a href="../tensorflow/tf.compat.htm">tf.compat</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.htm">tf.config</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.experimental.htm">tf.config.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.optimizer.htm">tf.config.optimizer</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.threading.htm">tf.config.threading</a>
        </li>
				<li>
            <a href="../tensorflow/tf.data.htm">tf.data</a>
        </li>
				<li>
            <a href="../tensorflow/tf.data.experimental.htm">tf.data.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.debugging.htm">tf.debugging</a>
        </li>
				<li>
            <a href="../tensorflow/tf.distribute.htm">tf.distribute</a>
        </li>
				<li>
            <a href="../tensorflow/tf.distributions.htm">tf.distributions</a>
        </li>
				<li>
            <a href="../tensorflow/tf.errors.htm">tf.errors</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.htm">tf.estimator</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.experimental.htm">tf.estimator.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.export.htm">tf.estimator.export</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.inputs.htm">tf.estimator.inputs</a>
        </li>
				<li>
            <a href="../tensorflow/tf.experimental.htm">tf.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.feature_column.htm">tf.feature_column</a>
        </li>
				<li>
            <a href="../tensorflow/tf.gfile.htm">tf.gfile</a>
        </li>
				<li>
            <a href="../tensorflow/tf.graph_util.htm">tf.graph_util</a>
        </li>
				<li>
            <a href="../tensorflow/tf.image.htm">tf.image</a>
        </li>
				<li>
            <a href="../tensorflow/tf.initializers.htm">tf.initializers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.io.htm">tf.io</a>
        </li>
				<li>
            <a href="../tensorflow/tf.io.gfile.htm">tf.io.gfile</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.htm">tf.keras</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.activations.htm">tf.keras.activations</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.htm">tf.keras.applications</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.densenet.htm">tf.keras.applications.densenet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.imagenet_utils.htm">tf.keras.applications.imagenet_utils</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.inception_resnet_v2.htm">tf.keras.applications.inception_resnet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.inception_v3.htm">tf.keras.applications.inception_v3</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.mobilenet.htm">tf.keras.applications.mobilenet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.mobilenet_v2.htm">tf.keras.applications.mobilenet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.nasnet.htm">tf.keras.applications.nasnet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.resnet.htm">tf.keras.applications.resnet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.resnet_v2.htm">tf.keras.applications.resnet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.vgg16.htm">tf.keras.applications.vgg16</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.vgg19.htm">tf.keras.applications.vgg19</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.xception.htm">tf.keras.applications.xception</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.backend.htm">tf.keras.backend</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.constraints.htm">tf.keras.constraints</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.htm">tf.keras.datasets</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.boston_housing.htm">tf.keras.datasets.boston_housing</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.cifar10.htm">tf.keras.datasets.cifar10</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.cifar100.htm">tf.keras.datasets.cifar100</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.fashion_mnist.htm">tf.keras.datasets.fashion_mnist</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.imdb.htm">tf.keras.datasets.imdb</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.mnist.htm">tf.keras.datasets.mnist</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.reuters.htm">tf.keras.datasets.reuters</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.estimator.htm">tf.keras.estimator</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.experimental.htm">tf.keras.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.initializers.htm">tf.keras.initializers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.layers.htm">tf.keras.layers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.losses.htm">tf.keras.losses</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.metrics.htm">tf.keras.metrics</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.mixed_precision.htm">tf.keras.mixed_precision</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.mixed_precision.experimental.htm">tf.keras.mixed_precision.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.models.htm">tf.keras.models</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.optimizers.htm">tf.keras.optimizers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.optimizers.schedules.htm">tf.keras.optimizers.schedules</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.preprocessing.htm">tf.keras.preprocessing</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.preprocessing.image.htm">tf.keras.preprocessing.image</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.regularizers.htm">tf.keras.regularizers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.utils.htm">tf.keras.utils</a>
        </li>
				<li>
            <a href="../tensorflow/tf.layers.htm" class="current">tf.layers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.layers.experimental.htm">tf.layers.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.linalg.htm">tf.linalg</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.htm">tf.lite</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.htm">tf.lite.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.htm">tf.lite.experimental.microfrontend</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.python.htm">tf.lite.experimental.microfrontend.python</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.python.ops.htm">tf.lite.experimental.microfrontend.python.ops</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.nn.htm">tf.lite.experimental.nn</a>
        </li>
				<li>
            <a href="../tensorflow/tf.logging.htm">tf.logging</a>
        </li>
				<li>
            <a href="../tensorflow/tf.losses.htm">tf.losses</a>
        </li>
				<li>
            <a href="../tensorflow/tf.math.htm">tf.math</a>
        </li>
				<li>
            <a href="../tensorflow/tf.metrics.htm">tf.metrics</a>
        </li>
				<li>
            <a href="../tensorflow/tf.nest.htm">tf.nest</a>
        </li>
				<li>
            <a href="../tensorflow/tf.nn.htm">tf.nn</a>
        </li>
				<li>
            <a href="../tensorflow/tf.profiler.htm">tf.profiler</a>
        </li>
				<li>
            <a href="../tensorflow/tf.quantization.htm">tf.quantization</a>
        </li>
				<li>
            <a href="../tensorflow/tf.ragged.htm">tf.ragged</a>
        </li>
				<li>
            <a href="../tensorflow/tf.random.htm">tf.random</a>
        </li>
				<li>
            <a href="../tensorflow/tf.random.experimental.htm">tf.random.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.resource_loader.htm">tf.resource_loader</a>
        </li>
				<li>
            <a href="../tensorflow/tf.saved_model.htm">tf.saved_model</a>
        </li>
				<li>
            <a href="../tensorflow/tf.saved_model.main_op.htm">tf.saved_model.main_op</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sets.htm">tf.sets</a>
        </li>
				<li>
            <a href="../tensorflow/tf.signal.htm">tf.signal</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sparse.htm">tf.sparse</a>
        </li>
				<li>
            <a href="../tensorflow/tf.strings.htm">tf.strings</a>
        </li>
				<li>
            <a href="../tensorflow/tf.summary.htm">tf.summary</a>
        </li>
				<li>
            <a href="../tensorflow/tf.summary.experimental.htm">tf.summary.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sysconfig.htm">tf.sysconfig</a>
        </li>
				<li>
            <a href="../tensorflow/tf.test.htm">tf.test</a>
        </li>
				<li>
            <a href="../tensorflow/tf.tpu.htm">tf.tpu</a>
        </li>
				<li>
            <a href="../tensorflow/tf.tpu.experimental.htm">tf.tpu.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.train.htm">tf.train</a>
        </li>
				<li>
            <a href="../tensorflow/tf.train.experimental.htm">tf.train.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.user_ops.htm">tf.user_ops</a>
        </li>
				<li>
            <a href="../tensorflow/tf.xla.htm">tf.xla</a>
        </li>
				<li>
            <a href="../tensorflow/tf.xla.experimental.htm">tf.xla.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/TFRecordReader.htm">TFRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/truncated_normal_initializer.htm">truncated_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/TypeSpec.htm">TypeSpec</a>
        </li>
				<li>
            <a href="../tensorflow/UnconnectedGradients.htm">UnconnectedGradients</a>
        </li>
				<li>
            <a href="../tensorflow/uniform_unit_scaling_initializer.htm">uniform_unit_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Variable.htm">Variable</a>
        </li>
				<li>
            <a href="../tensorflow/variable_scope.htm">variable_scope</a>
        </li>
				<li>
            <a href="../tensorflow/VariableAggregation.htm">VariableAggregation</a>
        </li>
				<li>
            <a href="../tensorflow/VariableScope.htm">VariableScope</a>
        </li>
				<li>
            <a href="../tensorflow/VariableSynchronization.htm">VariableSynchronization</a>
        </li>
				<li>
            <a href="../tensorflow/variance_scaling_initializer.htm">variance_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/VarLenFeature.htm">VarLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/WholeFileReader.htm">WholeFileReader</a>
        </li>
				<li>
            <a href="../tensorflow/zeros_initializer.htm">zeros_initializer</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> tf.layers</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow</p>
		</header>
    <div class="sub-header">
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling1d">average_pooling1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling1d_dyn">average_pooling1d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling2d">average_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling2d_dyn">average_pooling2d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling3d">average_pooling3d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling3d_dyn">average_pooling3d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization">batch_normalization</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization_dyn">batch_normalization_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv1d">conv1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv1d">conv1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv1d_dyn">conv1d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d">conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_dyn">conv2d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_transpose">conv2d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_transpose">conv2d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_transpose">conv2d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_transpose">conv2d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_transpose_dyn">conv2d_transpose_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d">conv3d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d">conv3d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_dyn">conv3d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_transpose">conv3d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_transpose">conv3d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_transpose">conv3d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_transpose">conv3d_transpose</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_transpose_dyn">conv3d_transpose_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dense">dense</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dense_dyn">dense_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dropout">dropout</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dropout">dropout</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dropout">dropout</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dropout">dropout</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dropout_dyn">dropout_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#flatten_dyn">flatten_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling1d">max_pooling1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling1d_dyn">max_pooling1d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d">max_pooling2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d_dyn">max_pooling2d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling3d">max_pooling3d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling3d">max_pooling3d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling3d">max_pooling3d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling3d">max_pooling3d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling3d_dyn">max_pooling3d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv1d">separable_conv1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv1d">separable_conv1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv1d">separable_conv1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv1d">separable_conv1d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv1d_dyn">separable_conv1d_dyn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv2d">separable_conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv2d">separable_conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv2d">separable_conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv2d">separable_conv2d</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv2d_dyn">separable_conv2d_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling1d_fn">average_pooling1d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling2d_fn">average_pooling2d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#average_pooling3d_fn">average_pooling3d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#batch_normalization_fn">batch_normalization_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv1d_fn">conv1d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_fn">conv2d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv2d_transpose_fn">conv2d_transpose_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_fn">conv3d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#conv3d_transpose_fn">conv3d_transpose_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dense_fn">dense_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#dropout_fn">dropout_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#flatten_fn">flatten_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling1d_fn">max_pooling1d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling2d_fn">max_pooling2d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#max_pooling3d_fn">max_pooling3d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv1d_fn">separable_conv1d_fn</a></li>
				<li><a href="../tensorflow/tf.layers.htm#separable_conv2d_fn">separable_conv2d_fn</a></li>
			</ul>
		
	</div>
	
	
	<h3 class="section">Public static methods</h3>

	<div id="average_pooling1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>average_pooling1d</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Average Pooling layer for 1D inputs. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling1D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 3. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of a single integer,
representing the size of the pooling window. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying the
strides of the pooling operation. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The output tensor, of rank 3. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="average_pooling1d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>average_pooling1d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Average Pooling layer for 1D inputs. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling1D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 3. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of a single integer,
representing the size of the pooling window. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying the
strides of the pooling operation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The output tensor, of rank 3. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="average_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>average_pooling2d</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Average pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="average_pooling2d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>average_pooling2d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Average pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="average_pooling3d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>average_pooling3d</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Average pooling layer for 3D inputs (e.g. volumes). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling3D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 5. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 3 integers:
(pool_depth, pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="average_pooling3d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>average_pooling3d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Average pooling layer for 3D inputs (e.g. volumes). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.AveragePooling3D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 5. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 3 integers:
(pool_depth, pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <span title="System.bool">bool</span> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <span title="System.bool">bool</span> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <span title="System.bool">bool</span> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <span title="System.bool">bool</span> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> axis, <span title="System.double">double</span> momentum, <span title="System.double">double</span> epsilon, <span title="System.bool">bool</span> center, <span title="System.bool">bool</span> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> training, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse, <span title="System.bool">bool</span> renorm, <span title="System.object">object</span> renorm_clipping, <span title="System.double">double</span> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="batch_normalization_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_normalization_dyn</strong>(<span title="System.object">object</span> inputs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> axis, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> momentum, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> epsilon, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> center, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> scale, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> beta_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> gamma_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_mean_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> moving_variance_initializer, <span title="System.object">object</span> beta_regularizer, <span title="System.object">object</span> gamma_regularizer, <span title="System.object">object</span> beta_constraint, <span title="System.object">object</span> gamma_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> training, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> renorm, <span title="System.object">object</span> renorm_clipping, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> renorm_momentum, <span title="System.object">object</span> fused, <span title="System.object">object</span> virtual_batch_size, <span title="System.object">object</span> adjustment)
		</h4>
		<div class="content">Functional interface for the batch normalization layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation). <p></p> Reference: http://arxiv.org/abs/1502.03167 <p></p> "Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift" <p></p> Sergey Ioffe, Christian Szegedy <p></p> Note: when training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in <a href="..\..\tf\GraphKeys\UPDATE_OPS.md"><code>tf.GraphKeys.UPDATE_OPS</code></a>, so they
need to be executed alongside the `train_op`. Also, be sure to add any
batch_normalization ops before getting the update_ops collection. Otherwise,
update_ops will be empty, and training/inference will not work properly. For
example: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> axis
						</dt>
						<dd>An `int`, the axis that should be normalized (typically the features
axis). For instance, after a `Convolution2D` layer with
`data_format="channels_first"`, set `axis=1` in `BatchNormalization`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> momentum
						</dt>
						<dd>Momentum for the moving average. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> epsilon
						</dt>
						<dd>Small float added to variance to avoid dividing by zero. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> center
						</dt>
						<dd>If True, add offset of `beta` to normalized tensor. If False, `beta`
is ignored. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> scale
						</dt>
						<dd>If True, multiply by `gamma`. If False, `gamma` is
not used. When the next layer is linear (also e.g. `nn.relu`), this can be
disabled since the scaling can be done by the next layer. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> beta_initializer
						</dt>
						<dd>Initializer for the beta weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> gamma_initializer
						</dt>
						<dd>Initializer for the gamma weight. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_mean_initializer
						</dt>
						<dd>Initializer for the moving mean. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> moving_variance_initializer
						</dt>
						<dd>Initializer for the moving variance. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_regularizer
						</dt>
						<dd>Optional regularizer for the beta weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_regularizer
						</dt>
						<dd>Optional regularizer for the gamma weight. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> beta_constraint
						</dt>
						<dd>An optional projection function to be applied to the `beta`
weight after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> gamma_constraint
						</dt>
						<dd>An optional projection function to be applied to the
`gamma` weight after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(normalized with statistics of the current batch) or in inference mode
(normalized with moving statistics). **NOTE**: make sure to set this
parameter correctly, or else your training/inference will not work
properly. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> renorm
						</dt>
						<dd>Whether to use Batch Renormalization
(https://arxiv.org/abs/1702.03275). This adds extra variables during
training. The inference is the same for either value of this parameter. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> renorm_clipping
						</dt>
						<dd>A dictionary that may map keys 'rmax', 'rmin', 'dmax' to
scalar `Tensors` used to clip the renorm correction. The correction
`(r, d)` is used as `corrected_value = normalized_value * r + d`, with
`r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,
dmax are set to inf, 0, inf, respectively. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> renorm_momentum
						</dt>
						<dd>Momentum used to update the moving means and standard
deviations with renorm. Unlike `momentum`, this affects training
and should be neither too small (which would add noise) nor too large
(which would give stale estimates). Note that `momentum` is still applied
to get the means and variances for inference. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> fused
						</dt>
						<dd>if `None` or `True`, use a faster, fused implementation if possible.
If `False`, use the system recommended implementation. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> virtual_batch_size
						</dt>
						<dd>An `int`. By default, `virtual_batch_size` is `None`,
which means batch normalization is performed across the whole batch. When
`virtual_batch_size` is not `None`, instead perform "Ghost Batch
Normalization", which creates virtual sub-batches which are each
normalized separately (with shared gamma, beta, and moving statistics).
Must divide the actual batch size during execution. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> adjustment
						</dt>
						<dd>A function taking the `Tensor` containing the (dynamic) shape of
the input tensor and returning a pair (scale, bias) to apply to the
normalized values (before gamma and beta), only during training. For
example, if axis==-1,
`adjustment = lambda shape: (
tf.random.uniform(shape[-1:], 0.93, 1.07),
tf.random.uniform(shape[-1:], -0.1, 0.1))`
will scale the normalized value by up to 7% up or down, then shift the
result by up to 0.1 (with independent scaling and bias for each feature
but shared across all examples), and finally apply gamma and/or beta. If
`None`, no adjustment is applied. Cannot be specified if
virtual_batch_size is specified. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>x_norm = tf.compat.v1.layers.batch_normalization(x, training=training) <p></p> #... <p></p> update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS)
train_op = optimizer.minimize(loss)
train_op = tf.group([train_op, update_ops]) </pre>
</div>
		</div>
	</div>
	<div id="conv1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv1d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.int">int</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for 1D convolution layer (e.g. temporal convolution). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv1D.md"><code>tf.keras.layers.Conv1D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying the
length of the 1D convolution window. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any `strides` value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv1d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.int">int</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for 1D convolution layer (e.g. temporal convolution). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv1D.md"><code>tf.keras.layers.Conv1D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying the
length of the 1D convolution window. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any `strides` value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv1d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv1d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> filters, <span title="System.object">object</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dilation_rate, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for 1D convolution layer (e.g. temporal convolution). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv1D.md"><code>tf.keras.layers.Conv1D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying the
length of the 1D convolution window. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>An integer or tuple/list of a single integer,
specifying the stride length of the convolution.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any `strides` value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> filters, <span title="System.object">object</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dilation_rate, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for the 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2D.md"><code>tf.keras.layers.Conv2D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying the
height and width of the 2D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the convolution along the height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2DTranspose.md"><code>tf.keras.layers.Conv2DTranspose</code></a> instead. <p></p> The need for transposed convolutions generally arises
from the desire to use a transformation going in the opposite direction
of a normal convolution, i.e., from something that has the shape of the
output of some convolution to something that has the shape of its input
while maintaining a connectivity pattern that is compatible with
said convolution. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to `None` to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If `None`, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2DTranspose.md"><code>tf.keras.layers.Conv2DTranspose</code></a> instead. <p></p> The need for transposed convolutions generally arises
from the desire to use a transformation going in the opposite direction
of a normal convolution, i.e., from something that has the shape of the
output of some convolution to something that has the shape of its input
while maintaining a connectivity pattern that is compatible with
said convolution. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to `None` to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If `None`, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2DTranspose.md"><code>tf.keras.layers.Conv2DTranspose</code></a> instead. <p></p> The need for transposed convolutions generally arises
from the desire to use a transformation going in the opposite direction
of a normal convolution, i.e., from something that has the shape of the
output of some convolution to something that has the shape of its input
while maintaining a connectivity pattern that is compatible with
said convolution. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to `None` to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If `None`, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2DTranspose.md"><code>tf.keras.layers.Conv2DTranspose</code></a> instead. <p></p> The need for transposed convolutions generally arises
from the desire to use a transformation going in the opposite direction
of a normal convolution, i.e., from something that has the shape of the
output of some convolution to something that has the shape of its input
while maintaining a connectivity pattern that is compatible with
said convolution. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to `None` to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If `None`, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv2d_transpose_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv2d_transpose_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> filters, <span title="System.object">object</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv2DTranspose.md"><code>tf.keras.layers.Conv2DTranspose</code></a> instead. <p></p> The need for transposed convolutions generally arises
from the desire to use a transformation going in the opposite direction
of a normal convolution, i.e., from something that has the shape of the
output of some convolution to something that has the shape of its input
while maintaining a connectivity pattern that is compatible with
said convolution. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to `None` to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If `None`, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for the 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3D.md"><code>tf.keras.layers.Conv3D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the
depth, height and width of the 3D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers,
specifying the strides of the convolution along the depth,
height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span> dilation_rate, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for the 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3D.md"><code>tf.keras.layers.Conv3D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the
depth, height and width of the 3D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers,
specifying the strides of the convolution along the depth,
height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> filters, <span title="System.object">object</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dilation_rate, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for the 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3D.md"><code>tf.keras.layers.Conv3D</code></a> instead. <p></p> This layer creates a convolution kernel that is convolved
(actually cross-correlated) with the layer input to produce a tensor of
outputs. If `use_bias` is True (and a `bias_initializer` is provided),
a bias vector is created and added to the outputs. Finally, if
`activation` is not `None`, it is applied to the outputs as well. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_size
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the
depth, height and width of the 3D convolution window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers,
specifying the strides of the convolution along the depth,
height and width.
Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any stride value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3DTranspose.md"><code>tf.keras.layers.Conv3DTranspose</code></a> instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3DTranspose.md"><code>tf.keras.layers.Conv3DTranspose</code></a> instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3DTranspose.md"><code>tf.keras.layers.Conv3DTranspose</code></a> instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d_transpose" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d_transpose</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3DTranspose.md"><code>tf.keras.layers.Conv3DTranspose</code></a> instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="conv3d_transpose_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>conv3d_transpose_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> filters, <span title="System.object">object</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for transposed 3D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\Conv3DTranspose.md"><code>tf.keras.layers.Conv3DTranspose</code></a> instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>A tuple or list of 3 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>one of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, depth, height, width, channels)` while `channels_first`
corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>An initializer for the convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Optional regularizer for the convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>Optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="dense" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dense</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> units, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the densely-connected layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead. <p></p> This layer implements the operation:
`outputs = activation(inputs * kernel + bias)`
where `activation` is the activation function passed as the `activation`
argument (if not `None`), `kernel` is a weights matrix created by the layer,
and `bias` is a bias vector created by the layer
(only if `use_bias` is `True`). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> units
						</dt>
						<dd>Integer or Long, dimensionality of the output space. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function (callable). Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><a href="../tensorflow.keras.initializers/Initializer.htm">Initializer</a></code> kernel_initializer
						</dt>
						<dd>Initializer function for the weight matrix.
If `None` (default), weights are initialized using the default
initializer used by `tf.compat.v1.get_variable`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>Initializer function for the bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Regularizer function for the weight matrix. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Regularizer function for the bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>An optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>An optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor the same shape as `inputs` except the last dimension is of
size `units`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="dense_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dense_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> units, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> kernel_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> kernel_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> kernel_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for the densely-connected layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead. <p></p> This layer implements the operation:
`outputs = activation(inputs * kernel + bias)`
where `activation` is the activation function passed as the `activation`
argument (if not `None`), `kernel` is a weights matrix created by the layer,
and `bias` is a bias vector created by the layer
(only if `use_bias` is `True`). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> units
						</dt>
						<dd>Integer or Long, dimensionality of the output space. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function (callable). Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_initializer
						</dt>
						<dd>Initializer function for the weight matrix.
If `None` (default), weights are initialized using the default
initializer used by `tf.compat.v1.get_variable`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>Initializer function for the bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_regularizer
						</dt>
						<dd>Regularizer function for the weight matrix. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Regularizer function for the bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_constraint
						</dt>
						<dd>An optional projection function to be applied to the
kernel after being updated by an `Optimizer` (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>An optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor the same shape as `inputs` except the last dimension is of
size `units`. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="dropout" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dropout</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.double">double</span> rate, <span title="System.bool">bool</span> noise_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> training, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies Dropout to the input. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead. <p></p> Dropout consists in randomly setting a fraction `rate` of input units to 0
at each update during training time, which helps prevent overfitting.
The units that are kept are scaled by `1 / (1 - rate)`, so that their
sum is unchanged at training time and inference time. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> rate
						</dt>
						<dd>The dropout rate, between 0 and 1. E.g. "rate=0.1" would drop out
10% of input units. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> noise_shape
						</dt>
						<dd>1D tensor of type `int32` representing the shape of the
binary dropout mask that will be multiplied with the input.
For instance, if your inputs have shape
`(batch_size, timesteps, features)`, and you want the dropout mask
to be the same for all timesteps, you can use
`noise_shape=[batch_size, 1, features]`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>A Python integer. Used to create random seeds. See
`tf.compat.v1.set_random_seed`
for behavior. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(apply dropout) or in inference mode (return the input untouched). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>The name of the layer (string). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="dropout" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dropout</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.double">double</span> rate, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> noise_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> training, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies Dropout to the input. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead. <p></p> Dropout consists in randomly setting a fraction `rate` of input units to 0
at each update during training time, which helps prevent overfitting.
The units that are kept are scaled by `1 / (1 - rate)`, so that their
sum is unchanged at training time and inference time. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> rate
						</dt>
						<dd>The dropout rate, between 0 and 1. E.g. "rate=0.1" would drop out
10% of input units. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> noise_shape
						</dt>
						<dd>1D tensor of type `int32` representing the shape of the
binary dropout mask that will be multiplied with the input.
For instance, if your inputs have shape
`(batch_size, timesteps, features)`, and you want the dropout mask
to be the same for all timesteps, you can use
`noise_shape=[batch_size, 1, features]`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>A Python integer. Used to create random seeds. See
`tf.compat.v1.set_random_seed`
for behavior. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(apply dropout) or in inference mode (return the input untouched). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>The name of the layer (string). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="dropout" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dropout</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.double">double</span> rate, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> noise_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> training, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies Dropout to the input. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead. <p></p> Dropout consists in randomly setting a fraction `rate` of input units to 0
at each update during training time, which helps prevent overfitting.
The units that are kept are scaled by `1 / (1 - rate)`, so that their
sum is unchanged at training time and inference time. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> rate
						</dt>
						<dd>The dropout rate, between 0 and 1. E.g. "rate=0.1" would drop out
10% of input units. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> noise_shape
						</dt>
						<dd>1D tensor of type `int32` representing the shape of the
binary dropout mask that will be multiplied with the input.
For instance, if your inputs have shape
`(batch_size, timesteps, features)`, and you want the dropout mask
to be the same for all timesteps, you can use
`noise_shape=[batch_size, 1, features]`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>A Python integer. Used to create random seeds. See
`tf.compat.v1.set_random_seed`
for behavior. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(apply dropout) or in inference mode (return the input untouched). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>The name of the layer (string). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="dropout" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dropout</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.double">double</span> rate, <span title="System.bool">bool</span> noise_shape, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> seed, <span title="System.bool">bool</span> training, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Applies Dropout to the input. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead. <p></p> Dropout consists in randomly setting a fraction `rate` of input units to 0
at each update during training time, which helps prevent overfitting.
The units that are kept are scaled by `1 / (1 - rate)`, so that their
sum is unchanged at training time and inference time. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> rate
						</dt>
						<dd>The dropout rate, between 0 and 1. E.g. "rate=0.1" would drop out
10% of input units. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> noise_shape
						</dt>
						<dd>1D tensor of type `int32` representing the shape of the
binary dropout mask that will be multiplied with the input.
For instance, if your inputs have shape
`(batch_size, timesteps, features)`, and you want the dropout mask
to be the same for all timesteps, you can use
`noise_shape=[batch_size, 1, features]`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> seed
						</dt>
						<dd>A Python integer. Used to create random seeds. See
`tf.compat.v1.set_random_seed`
for behavior. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(apply dropout) or in inference mode (return the input untouched). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>The name of the layer (string). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="dropout_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>dropout_dyn</strong>(<span title="System.object">object</span> inputs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> rate, <span title="System.object">object</span> noise_shape, <span title="System.object">object</span> seed, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> training, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Applies Dropout to the input. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.dropout instead. <p></p> Dropout consists in randomly setting a fraction `rate` of input units to 0
at each update during training time, which helps prevent overfitting.
The units that are kept are scaled by `1 / (1 - rate)`, so that their
sum is unchanged at training time and inference time. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> rate
						</dt>
						<dd>The dropout rate, between 0 and 1. E.g. "rate=0.1" would drop out
10% of input units. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> noise_shape
						</dt>
						<dd>1D tensor of type `int32` representing the shape of the
binary dropout mask that will be multiplied with the input.
For instance, if your inputs have shape
`(batch_size, timesteps, features)`, and you want the dropout mask
to be the same for all timesteps, you can use
`noise_shape=[batch_size, 1, features]`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> seed
						</dt>
						<dd>A Python integer. Used to create random seeds. See
`tf.compat.v1.set_random_seed`
for behavior. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> training
						</dt>
						<dd>Either a Python boolean, or a TensorFlow boolean scalar tensor
(e.g. a placeholder). Whether to return the output in training mode
(apply dropout) or in inference mode (return the input untouched). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>The name of the layer (string). 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="flatten_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>flatten_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> name, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format)
		</h4>
		<div class="content">Flattens an input tensor while preserving the batch axis (axis 0). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Tensor input. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>The name of the layer (string). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Reshaped tensor. <p></p> Examples: <p></p> ```
x = tf.compat.v1.placeholder(shape=(None, 4, 4), dtype='float32')
y = flatten(x)
# now `y` has shape `(None, 16)` <p></p> x = tf.compat.v1.placeholder(shape=(None, 3, None), dtype='float32')
y = flatten(x)
# now `y` has shape `(None, None)`
``` 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling1d</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max Pooling layer for 1D inputs. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling1D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 3. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of a single integer,
representing the size of the pooling window. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying the
strides of the pooling operation. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The output tensor, of rank 3. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling1d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling1d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Max Pooling layer for 1D inputs. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling1D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 3. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of a single integer,
representing the size of the pooling window. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of a single integer, specifying the
strides of the pooling operation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The output tensor, of rank 3. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> pool_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> pool_size, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> pool_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> pool_size, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> pool_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> pool_size, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> pool_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> pool_size, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling2d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling2d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Max pooling layer for 2D inputs (e.g. images). (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 4. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 2 integers: (pool_height, pool_width)
specifying the size of the pooling window.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 2 integers,
specifying the strides of the pooling operation.
Can be a single integer to specify the same value for
all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling3d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling3d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> pool_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 3D inputs (e.g. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling3D instead. <p></p> volumes). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 5. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 3 integers: (pool_depth, pool_height,
pool_width) specifying the size of the pooling window. Can be a single
integer to specify the same value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the strides of
the pooling operation. Can be a single integer to specify the same value
for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape `(batch, depth, height,
width, channels)` while `channels_first` corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling3d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling3d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.ValueTuple<int, object, int>">ValueTuple&lt;int, object, int&gt;</span> pool_size, <span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 3D inputs (e.g. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling3D instead. <p></p> volumes). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 5. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, int>">ValueTuple&lt;int, object, int&gt;</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 3 integers: (pool_depth, pool_height,
pool_width) specifying the size of the pooling window. Can be a single
integer to specify the same value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the strides of
the pooling operation. Can be a single integer to specify the same value
for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape `(batch, depth, height,
width, channels)` while `channels_first` corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling3d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling3d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.ValueTuple<int, object, int>">ValueTuple&lt;int, object, int&gt;</span> pool_size, <span title="System.int">int</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 3D inputs (e.g. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling3D instead. <p></p> volumes). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 5. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, int>">ValueTuple&lt;int, object, int&gt;</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 3 integers: (pool_depth, pool_height,
pool_width) specifying the size of the pooling window. Can be a single
integer to specify the same value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the strides of
the pooling operation. Can be a single integer to specify the same value
for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape `(batch, depth, height,
width, channels)` while `channels_first` corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling3d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling3d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> pool_size, <span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Max pooling layer for 3D inputs (e.g. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling3D instead. <p></p> volumes). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 5. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 3 integers: (pool_depth, pool_height,
pool_width) specifying the size of the pooling window. Can be a single
integer to specify the same value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object, object>">ValueTuple&lt;int, object, object&gt;</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the strides of
the pooling operation. Can be a single integer to specify the same value
for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape `(batch, depth, height,
width, channels)` while `channels_first` corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="max_pooling3d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>max_pooling3d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> pool_size, <span title="System.object">object</span> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <span title="System.object">object</span> name)
		</h4>
		<div class="content">Max pooling layer for 3D inputs (e.g. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling3D instead. <p></p> volumes). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>The tensor over which to pool. Must have rank 5. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pool_size
						</dt>
						<dd>An integer or tuple/list of 3 integers: (pool_depth, pool_height,
pool_width) specifying the size of the pooling window. Can be a single
integer to specify the same value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> strides
						</dt>
						<dd>An integer or tuple/list of 3 integers, specifying the strides of
the pooling operation. Can be a single integer to specify the same value
for all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>A string. The padding method, either 'valid' or 'same'.
Case-insensitive. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string. The ordering of the dimensions in the inputs.
`channels_last` (default) and `channels_first` are supported.
`channels_last` corresponds to inputs with shape `(batch, depth, height,
width, channels)` while `channels_first` corresponds to inputs with shape
`(batch, channels, depth, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv1d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span> kernel_size, <span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.int">int</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 1D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv1D.md"><code>tf.keras.layers.SeparableConv1D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span></code> kernel_size
						</dt>
						<dd>A single integer specifying the spatial
dimensions of the filters. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span></code> strides
						</dt>
						<dd>A single integer specifying the strides
of the convolution.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> dilation_rate
						</dt>
						<dd>A single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv1d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span> kernel_size, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.int">int</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 1D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv1D.md"><code>tf.keras.layers.SeparableConv1D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span></code> kernel_size
						</dt>
						<dd>A single integer specifying the spatial
dimensions of the filters. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> strides
						</dt>
						<dd>A single integer specifying the strides
of the convolution.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> dilation_rate
						</dt>
						<dd>A single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv1d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> kernel_size, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.int">int</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 1D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv1D.md"><code>tf.keras.layers.SeparableConv1D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A single integer specifying the spatial
dimensions of the filters. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> strides
						</dt>
						<dd>A single integer specifying the strides
of the convolution.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> dilation_rate
						</dt>
						<dd>A single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv1d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv1d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> kernel_size, <span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.int">int</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 1D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv1D.md"><code>tf.keras.layers.SeparableConv1D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A single integer specifying the spatial
dimensions of the filters. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, int>">ValueTuple&lt;int, int&gt;</span></code> strides
						</dt>
						<dd>A single integer specifying the strides
of the convolution.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> dilation_rate
						</dt>
						<dd>A single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv1d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv1d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> filters, <span title="System.object">object</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dilation_rate, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> depth_multiplier, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 1D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv1D.md"><code>tf.keras.layers.SeparableConv1D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_size
						</dt>
						<dd>A single integer specifying the spatial
dimensions of the filters. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>A single integer specifying the strides
of the convolution.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, length, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, length)`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dilation_rate
						</dt>
						<dd>A single integer, specifying
the dilation rate to use for dilated convolution.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv2D.md"><code>tf.keras.layers.SeparableConv2D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv2d</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv2D.md"><code>tf.keras.layers.SeparableConv2D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.int">int</span> kernel_size, <span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv2D.md"><code>tf.keras.layers.SeparableConv2D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv2d" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv2d</strong>(<span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> inputs, <span title="System.int">int</span> filters, <span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> kernel_size, <span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span> strides, <span title="System.string">string</span> padding, <span title="System.string">string</span> data_format, <span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span> dilation_rate, <span title="System.int">int</span> depth_multiplier, <span title="System.object">object</span> activation, <span title="System.bool">bool</span> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <span title="System.bool">bool</span> trainable, <span title="System.string">string</span> name, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv2D.md"><code>tf.keras.layers.SeparableConv2D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<int, object>>">Nullable&lt;ValueTuple&lt;int, object&gt;&gt;</span></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<int, object>">ValueTuple&lt;int, object&gt;</span></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="separable_conv2d_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>separable_conv2d_dyn</strong>(<span title="System.object">object</span> inputs, <span title="System.object">object</span> filters, <span title="System.object">object</span> kernel_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strides, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> padding, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> data_format, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> dilation_rate, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> depth_multiplier, <span title="System.object">object</span> activation, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> use_bias, <span title="System.object">object</span> depthwise_initializer, <span title="System.object">object</span> pointwise_initializer, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> bias_initializer, <span title="System.object">object</span> depthwise_regularizer, <span title="System.object">object</span> pointwise_regularizer, <span title="System.object">object</span> bias_regularizer, <span title="System.object">object</span> activity_regularizer, <span title="System.object">object</span> depthwise_constraint, <span title="System.object">object</span> pointwise_constraint, <span title="System.object">object</span> bias_constraint, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> trainable, <span title="System.object">object</span> name, <span title="System.object">object</span> reuse)
		</h4>
		<div class="content">Functional interface for the depthwise separable 2D convolution layer. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use <a href="..\..\tf\keras\layers\SeparableConv2D.md"><code>tf.keras.layers.SeparableConv2D</code></a> instead. <p></p> This layer performs a depthwise convolution that acts separately on
channels, followed by a pointwise convolution that mixes channels.
If `use_bias` is True and a bias initializer is provided,
it adds a bias vector to the output.
It then optionally applies an activation function to produce the final output. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> inputs
						</dt>
						<dd>Input tensor. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filters
						</dt>
						<dd>Integer, the dimensionality of the output space (i.e. the number
of filters in the convolution). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> kernel_size
						</dt>
						<dd>A tuple or list of 2 integers specifying the spatial
dimensions of the filters. Can be a single integer to specify the same
value for all spatial dimensions. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strides
						</dt>
						<dd>A tuple or list of 2 positive integers specifying the strides
of the convolution. Can be a single integer to specify the same value for
all spatial dimensions.
Specifying any `stride` value != 1 is incompatible with specifying
any `dilation_rate` value != 1. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> padding
						</dt>
						<dd>One of `"valid"` or `"same"` (case-insensitive). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> data_format
						</dt>
						<dd>A string, one of `channels_last` (default) or `channels_first`.
The ordering of the dimensions in the inputs.
`channels_last` corresponds to inputs with shape
`(batch, height, width, channels)` while `channels_first` corresponds to
inputs with shape `(batch, channels, height, width)`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> dilation_rate
						</dt>
						<dd>An integer or tuple/list of 2 integers, specifying
the dilation rate to use for dilated convolution.
Can be a single integer to specify the same value for
all spatial dimensions.
Currently, specifying any `dilation_rate` value != 1 is
incompatible with specifying any stride value != 1. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> depth_multiplier
						</dt>
						<dd>The number of depthwise convolution output channels for
each input channel. The total number of depthwise convolution output
channels will be equal to `num_filters_in * depth_multiplier`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activation
						</dt>
						<dd>Activation function. Set it to None to maintain a
linear activation. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> use_bias
						</dt>
						<dd>Boolean, whether the layer uses a bias. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_initializer
						</dt>
						<dd>An initializer for the depthwise convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_initializer
						</dt>
						<dd>An initializer for the pointwise convolution kernel. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> bias_initializer
						</dt>
						<dd>An initializer for the bias vector. If None, the default
initializer will be used. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_regularizer
						</dt>
						<dd>Optional regularizer for the depthwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_regularizer
						</dt>
						<dd>Optional regularizer for the pointwise
convolution kernel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_regularizer
						</dt>
						<dd>Optional regularizer for the bias vector. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> activity_regularizer
						</dt>
						<dd>Optional regularizer function for the output. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> depthwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
depthwise kernel after being updated by an `Optimizer` (e.g. used for
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> pointwise_constraint
						</dt>
						<dd>Optional projection function to be applied to the
pointwise kernel after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> bias_constraint
						</dt>
						<dd>Optional projection function to be applied to the
bias after being updated by an `Optimizer`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> trainable
						</dt>
						<dd>Boolean, if `True` also add variables to the graph collection
`GraphKeys.TRAINABLE_VARIABLES` (see <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>A string, the name of the layer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> reuse
						</dt>
						<dd>Boolean, whether to reuse the weights of a previous layer
by the same name. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Output tensor. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="average_pooling1d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>average_pooling1d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="average_pooling2d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>average_pooling2d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="average_pooling3d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>average_pooling3d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="batch_normalization_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>batch_normalization_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="conv1d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>conv1d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="conv2d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>conv2d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="conv2d_transpose_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>conv2d_transpose_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="conv3d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>conv3d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="conv3d_transpose_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>conv3d_transpose_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="dense_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>dense_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="dropout_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>dropout_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="flatten_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>flatten_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="max_pooling1d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>max_pooling1d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="max_pooling2d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>max_pooling2d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="max_pooling3d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>max_pooling3d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="separable_conv1d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>separable_conv1d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="separable_conv2d_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>separable_conv2d_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>