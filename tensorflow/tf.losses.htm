<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>tf.losses - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow/AggregationMethod.htm">AggregationMethod</a>
        </li>
				<li>
            <a href="../tensorflow/ConditionalAccumulator.htm">ConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/ConditionalAccumulatorBase.htm">ConditionalAccumulatorBase</a>
        </li>
				<li>
            <a href="../tensorflow/constant_initializer.htm">constant_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/CriticalSection.htm">CriticalSection</a>
        </li>
				<li>
            <a href="../tensorflow/DeviceSpec.htm">DeviceSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Dimension.htm">Dimension</a>
        </li>
				<li>
            <a href="../tensorflow/DType.htm">DType</a>
        </li>
				<li>
            <a href="../tensorflow/FIFOQueue.htm">FIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLenFeature.htm">FixedLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLengthRecordReader.htm">FixedLengthRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/FixedLenSequenceFeature.htm">FixedLenSequenceFeature</a>
        </li>
				<li>
            <a href="../tensorflow/glorot_normal_initializer.htm">glorot_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/glorot_uniform_initializer.htm">glorot_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/GradientTape.htm">GradientTape</a>
        </li>
				<li>
            <a href="../tensorflow/Graph.htm">Graph</a>
        </li>
				<li>
            <a href="../tensorflow/Graph._ControlDependenciesController.htm">Graph._ControlDependenciesController</a>
        </li>
				<li>
            <a href="../tensorflow/Graph.I_ControlDependenciesController.htm">Graph.I_ControlDependenciesController</a>
        </li>
				<li>
            <a href="../tensorflow/GraphKeys.htm">GraphKeys</a>
        </li>
				<li>
            <a href="../tensorflow/HeadingAxes.htm">HeadingAxes</a>
        </li>
				<li>
            <a href="../tensorflow/IAggregationMethod.htm">IAggregationMethod</a>
        </li>
				<li>
            <a href="../tensorflow/IConditionalAccumulator.htm">IConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/IConditionalAccumulatorBase.htm">IConditionalAccumulatorBase</a>
        </li>
				<li>
            <a href="../tensorflow/Iconstant_initializer.htm">Iconstant_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/ICriticalSection.htm">ICriticalSection</a>
        </li>
				<li>
            <a href="../tensorflow/IdentityReader.htm">IdentityReader</a>
        </li>
				<li>
            <a href="../tensorflow/IDeviceSpec.htm">IDeviceSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IDimension.htm">IDimension</a>
        </li>
				<li>
            <a href="../tensorflow/IDType.htm">IDType</a>
        </li>
				<li>
            <a href="../tensorflow/IFIFOQueue.htm">IFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLenFeature.htm">IFixedLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLengthRecordReader.htm">IFixedLengthRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/IFixedLenSequenceFeature.htm">IFixedLenSequenceFeature</a>
        </li>
				<li>
            <a href="../tensorflow/Iglorot_normal_initializer.htm">Iglorot_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Iglorot_uniform_initializer.htm">Iglorot_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IGradientTape.htm">IGradientTape</a>
        </li>
				<li>
            <a href="../tensorflow/IGraph.htm">IGraph</a>
        </li>
				<li>
            <a href="../tensorflow/IGraphKeys.htm">IGraphKeys</a>
        </li>
				<li>
            <a href="../tensorflow/IIdentityReader.htm">IIdentityReader</a>
        </li>
				<li>
            <a href="../tensorflow/IIndexedSlices.htm">IIndexedSlices</a>
        </li>
				<li>
            <a href="../tensorflow/IIndexedSlicesSpec.htm">IIndexedSlicesSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IInteractiveSession.htm">IInteractiveSession</a>
        </li>
				<li>
            <a href="../tensorflow/ILazyLoader.htm">ILazyLoader</a>
        </li>
				<li>
            <a href="../tensorflow/ILMDBReader.htm">ILMDBReader</a>
        </li>
				<li>
            <a href="../tensorflow/IModule.htm">IModule</a>
        </li>
				<li>
            <a href="../tensorflow/Iname_scope.htm">Iname_scope</a>
        </li>
				<li>
            <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a>
        </li>
				<li>
            <a href="../tensorflow/IndexedSlicesSpec.htm">IndexedSlicesSpec</a>
        </li>
				<li>
            <a href="../tensorflow/InteractiveSession.htm">InteractiveSession</a>
        </li>
				<li>
            <a href="../tensorflow/Iones_initializer.htm">Iones_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IOperation.htm">IOperation</a>
        </li>
				<li>
            <a href="../tensorflow/IOpError.htm">IOpError</a>
        </li>
				<li>
            <a href="../tensorflow/IOptionalSpec.htm">IOptionalSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Iorthogonal_initializer.htm">Iorthogonal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IPaddingFIFOQueue.htm">IPaddingFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IPriorityQueue.htm">IPriorityQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IQueueBase.htm">IQueueBase</a>
        </li>
				<li>
            <a href="../tensorflow/IRaggedTensor.htm">IRaggedTensor</a>
        </li>
				<li>
            <a href="../tensorflow/IRaggedTensorSpec.htm">IRaggedTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/Irandom_normal_initializer.htm">Irandom_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Irandom_uniform_initializer.htm">Irandom_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IRandomShuffleQueue.htm">IRandomShuffleQueue</a>
        </li>
				<li>
            <a href="../tensorflow/IReaderBase.htm">IReaderBase</a>
        </li>
				<li>
            <a href="../tensorflow/IRegisterGradient.htm">IRegisterGradient</a>
        </li>
				<li>
            <a href="../tensorflow/ISession.htm">ISession</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseConditionalAccumulator.htm">ISparseConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseFeature.htm">ISparseFeature</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensor.htm">ISparseTensor</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensorSpec.htm">ISparseTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/ISparseTensorValue.htm">ISparseTensorValue</a>
        </li>
				<li>
            <a href="../tensorflow/ITensor.htm">ITensor</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorArray.htm">ITensorArray</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorArraySpec.htm">ITensorArraySpec</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorShape.htm">ITensorShape</a>
        </li>
				<li>
            <a href="../tensorflow/ITensorSpec.htm">ITensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/ITextLineReader.htm">ITextLineReader</a>
        </li>
				<li>
            <a href="../tensorflow/ITFRecordReader.htm">ITFRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/Itruncated_normal_initializer.htm">Itruncated_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/ITypeSpec.htm">ITypeSpec</a>
        </li>
				<li>
            <a href="../tensorflow/IUnconnectedGradients.htm">IUnconnectedGradients</a>
        </li>
				<li>
            <a href="../tensorflow/Iuniform_unit_scaling_initializer.htm">Iuniform_unit_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IVariable.htm">IVariable</a>
        </li>
				<li>
            <a href="../tensorflow/Ivariable_scope.htm">Ivariable_scope</a>
        </li>
				<li>
            <a href="../tensorflow/IVariableScope.htm">IVariableScope</a>
        </li>
				<li>
            <a href="../tensorflow/Ivariance_scaling_initializer.htm">Ivariance_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/IVarLenFeature.htm">IVarLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/IWholeFileReader.htm">IWholeFileReader</a>
        </li>
				<li>
            <a href="../tensorflow/Izeros_initializer.htm">Izeros_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/LazyLoader.htm">LazyLoader</a>
        </li>
				<li>
            <a href="../tensorflow/LMDBReader.htm">LMDBReader</a>
        </li>
				<li>
            <a href="../tensorflow/Module.htm">Module</a>
        </li>
				<li>
            <a href="../tensorflow/name_scope.htm">name_scope</a>
        </li>
				<li>
            <a href="../tensorflow/ones_initializer.htm">ones_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Operation.htm">Operation</a>
        </li>
				<li>
            <a href="../tensorflow/Operation._InputList.htm">Operation._InputList</a>
        </li>
				<li>
            <a href="../tensorflow/Operation.I_InputList.htm">Operation.I_InputList</a>
        </li>
				<li>
            <a href="../tensorflow/OpError.htm">OpError</a>
        </li>
				<li>
            <a href="../tensorflow/OptionalSpec.htm">OptionalSpec</a>
        </li>
				<li>
            <a href="../tensorflow/orthogonal_initializer.htm">orthogonal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/PaddingFIFOQueue.htm">PaddingFIFOQueue</a>
        </li>
				<li>
            <a href="../tensorflow/PriorityQueue.htm">PriorityQueue</a>
        </li>
				<li>
            <a href="../tensorflow/QueueBase.htm">QueueBase</a>
        </li>
				<li>
            <a href="../tensorflow/RaggedTensor.htm">RaggedTensor</a>
        </li>
				<li>
            <a href="../tensorflow/RaggedTensorSpec.htm">RaggedTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/random_normal_initializer.htm">random_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/random_uniform_initializer.htm">random_uniform_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/RandomShuffleQueue.htm">RandomShuffleQueue</a>
        </li>
				<li>
            <a href="../tensorflow/ReaderBase.htm">ReaderBase</a>
        </li>
				<li>
            <a href="../tensorflow/RegisterGradient.htm">RegisterGradient</a>
        </li>
				<li>
            <a href="../tensorflow/Session.htm">Session</a>
        </li>
				<li>
            <a href="../tensorflow/SparseConditionalAccumulator.htm">SparseConditionalAccumulator</a>
        </li>
				<li>
            <a href="../tensorflow/SparseFeature.htm">SparseFeature</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensor.htm">SparseTensor</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensorSpec.htm">SparseTensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/SparseTensorValue.htm">SparseTensorValue</a>
        </li>
				<li>
            <a href="../tensorflow/Tensor.htm">Tensor</a>
        </li>
				<li>
            <a href="../tensorflow/Tensor`1.htm">Tensor&lt;T&gt;</a>
        </li>
				<li>
            <a href="../tensorflow/TensorArray.htm">TensorArray</a>
        </li>
				<li>
            <a href="../tensorflow/TensorArraySpec.htm">TensorArraySpec</a>
        </li>
				<li>
            <a href="../tensorflow/TensorDimension.htm">TensorDimension</a>
        </li>
				<li>
            <a href="../tensorflow/TensorDimensionSlice.htm">TensorDimensionSlice</a>
        </li>
				<li>
            <a href="../tensorflow/TensorShape.htm">TensorShape</a>
        </li>
				<li>
            <a href="../tensorflow/TensorSpec.htm">TensorSpec</a>
        </li>
				<li>
            <a href="../tensorflow/TextLineReader.htm">TextLineReader</a>
        </li>
				<li>
            <a href="../tensorflow/tf.htm">tf</a>
        </li>
				<li>
            <a href="../tensorflow/tf.audio.htm">tf.audio</a>
        </li>
				<li>
            <a href="../tensorflow/tf.autograph.htm">tf.autograph</a>
        </li>
				<li>
            <a href="../tensorflow/tf.autograph.experimental.htm">tf.autograph.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.bitwise.htm">tf.bitwise</a>
        </li>
				<li>
            <a href="../tensorflow/tf.compat.htm">tf.compat</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.htm">tf.config</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.experimental.htm">tf.config.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.optimizer.htm">tf.config.optimizer</a>
        </li>
				<li>
            <a href="../tensorflow/tf.config.threading.htm">tf.config.threading</a>
        </li>
				<li>
            <a href="../tensorflow/tf.data.htm">tf.data</a>
        </li>
				<li>
            <a href="../tensorflow/tf.data.experimental.htm">tf.data.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.debugging.htm">tf.debugging</a>
        </li>
				<li>
            <a href="../tensorflow/tf.distribute.htm">tf.distribute</a>
        </li>
				<li>
            <a href="../tensorflow/tf.distributions.htm">tf.distributions</a>
        </li>
				<li>
            <a href="../tensorflow/tf.errors.htm">tf.errors</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.htm">tf.estimator</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.experimental.htm">tf.estimator.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.export.htm">tf.estimator.export</a>
        </li>
				<li>
            <a href="../tensorflow/tf.estimator.inputs.htm">tf.estimator.inputs</a>
        </li>
				<li>
            <a href="../tensorflow/tf.experimental.htm">tf.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.feature_column.htm">tf.feature_column</a>
        </li>
				<li>
            <a href="../tensorflow/tf.gfile.htm">tf.gfile</a>
        </li>
				<li>
            <a href="../tensorflow/tf.graph_util.htm">tf.graph_util</a>
        </li>
				<li>
            <a href="../tensorflow/tf.image.htm">tf.image</a>
        </li>
				<li>
            <a href="../tensorflow/tf.initializers.htm">tf.initializers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.io.htm">tf.io</a>
        </li>
				<li>
            <a href="../tensorflow/tf.io.gfile.htm">tf.io.gfile</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.htm">tf.keras</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.activations.htm">tf.keras.activations</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.htm">tf.keras.applications</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.densenet.htm">tf.keras.applications.densenet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.imagenet_utils.htm">tf.keras.applications.imagenet_utils</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.inception_resnet_v2.htm">tf.keras.applications.inception_resnet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.inception_v3.htm">tf.keras.applications.inception_v3</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.mobilenet.htm">tf.keras.applications.mobilenet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.mobilenet_v2.htm">tf.keras.applications.mobilenet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.nasnet.htm">tf.keras.applications.nasnet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.resnet.htm">tf.keras.applications.resnet</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.resnet_v2.htm">tf.keras.applications.resnet_v2</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.vgg16.htm">tf.keras.applications.vgg16</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.vgg19.htm">tf.keras.applications.vgg19</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.applications.xception.htm">tf.keras.applications.xception</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.backend.htm">tf.keras.backend</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.constraints.htm">tf.keras.constraints</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.htm">tf.keras.datasets</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.boston_housing.htm">tf.keras.datasets.boston_housing</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.cifar10.htm">tf.keras.datasets.cifar10</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.cifar100.htm">tf.keras.datasets.cifar100</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.fashion_mnist.htm">tf.keras.datasets.fashion_mnist</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.imdb.htm">tf.keras.datasets.imdb</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.mnist.htm">tf.keras.datasets.mnist</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.datasets.reuters.htm">tf.keras.datasets.reuters</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.estimator.htm">tf.keras.estimator</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.experimental.htm">tf.keras.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.initializers.htm">tf.keras.initializers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.layers.htm">tf.keras.layers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.losses.htm">tf.keras.losses</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.metrics.htm">tf.keras.metrics</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.mixed_precision.htm">tf.keras.mixed_precision</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.mixed_precision.experimental.htm">tf.keras.mixed_precision.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.models.htm">tf.keras.models</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.optimizers.htm">tf.keras.optimizers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.optimizers.schedules.htm">tf.keras.optimizers.schedules</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.preprocessing.htm">tf.keras.preprocessing</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.preprocessing.image.htm">tf.keras.preprocessing.image</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.regularizers.htm">tf.keras.regularizers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.keras.utils.htm">tf.keras.utils</a>
        </li>
				<li>
            <a href="../tensorflow/tf.layers.htm">tf.layers</a>
        </li>
				<li>
            <a href="../tensorflow/tf.layers.experimental.htm">tf.layers.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.linalg.htm">tf.linalg</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.htm">tf.lite</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.htm">tf.lite.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.htm">tf.lite.experimental.microfrontend</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.python.htm">tf.lite.experimental.microfrontend.python</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.microfrontend.python.ops.htm">tf.lite.experimental.microfrontend.python.ops</a>
        </li>
				<li>
            <a href="../tensorflow/tf.lite.experimental.nn.htm">tf.lite.experimental.nn</a>
        </li>
				<li>
            <a href="../tensorflow/tf.logging.htm">tf.logging</a>
        </li>
				<li>
            <a href="../tensorflow/tf.losses.htm" class="current">tf.losses</a>
        </li>
				<li>
            <a href="../tensorflow/tf.math.htm">tf.math</a>
        </li>
				<li>
            <a href="../tensorflow/tf.metrics.htm">tf.metrics</a>
        </li>
				<li>
            <a href="../tensorflow/tf.nest.htm">tf.nest</a>
        </li>
				<li>
            <a href="../tensorflow/tf.nn.htm">tf.nn</a>
        </li>
				<li>
            <a href="../tensorflow/tf.profiler.htm">tf.profiler</a>
        </li>
				<li>
            <a href="../tensorflow/tf.quantization.htm">tf.quantization</a>
        </li>
				<li>
            <a href="../tensorflow/tf.ragged.htm">tf.ragged</a>
        </li>
				<li>
            <a href="../tensorflow/tf.random.htm">tf.random</a>
        </li>
				<li>
            <a href="../tensorflow/tf.random.experimental.htm">tf.random.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.resource_loader.htm">tf.resource_loader</a>
        </li>
				<li>
            <a href="../tensorflow/tf.saved_model.htm">tf.saved_model</a>
        </li>
				<li>
            <a href="../tensorflow/tf.saved_model.main_op.htm">tf.saved_model.main_op</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sets.htm">tf.sets</a>
        </li>
				<li>
            <a href="../tensorflow/tf.signal.htm">tf.signal</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sparse.htm">tf.sparse</a>
        </li>
				<li>
            <a href="../tensorflow/tf.strings.htm">tf.strings</a>
        </li>
				<li>
            <a href="../tensorflow/tf.summary.htm">tf.summary</a>
        </li>
				<li>
            <a href="../tensorflow/tf.summary.experimental.htm">tf.summary.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.sysconfig.htm">tf.sysconfig</a>
        </li>
				<li>
            <a href="../tensorflow/tf.test.htm">tf.test</a>
        </li>
				<li>
            <a href="../tensorflow/tf.tpu.htm">tf.tpu</a>
        </li>
				<li>
            <a href="../tensorflow/tf.tpu.experimental.htm">tf.tpu.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.train.htm">tf.train</a>
        </li>
				<li>
            <a href="../tensorflow/tf.train.experimental.htm">tf.train.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/tf.user_ops.htm">tf.user_ops</a>
        </li>
				<li>
            <a href="../tensorflow/tf.xla.htm">tf.xla</a>
        </li>
				<li>
            <a href="../tensorflow/tf.xla.experimental.htm">tf.xla.experimental</a>
        </li>
				<li>
            <a href="../tensorflow/TFRecordReader.htm">TFRecordReader</a>
        </li>
				<li>
            <a href="../tensorflow/truncated_normal_initializer.htm">truncated_normal_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/TypeSpec.htm">TypeSpec</a>
        </li>
				<li>
            <a href="../tensorflow/UnconnectedGradients.htm">UnconnectedGradients</a>
        </li>
				<li>
            <a href="../tensorflow/uniform_unit_scaling_initializer.htm">uniform_unit_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/Variable.htm">Variable</a>
        </li>
				<li>
            <a href="../tensorflow/variable_scope.htm">variable_scope</a>
        </li>
				<li>
            <a href="../tensorflow/VariableAggregation.htm">VariableAggregation</a>
        </li>
				<li>
            <a href="../tensorflow/VariableScope.htm">VariableScope</a>
        </li>
				<li>
            <a href="../tensorflow/VariableSynchronization.htm">VariableSynchronization</a>
        </li>
				<li>
            <a href="../tensorflow/variance_scaling_initializer.htm">variance_scaling_initializer</a>
        </li>
				<li>
            <a href="../tensorflow/VarLenFeature.htm">VarLenFeature</a>
        </li>
				<li>
            <a href="../tensorflow/WholeFileReader.htm">WholeFileReader</a>
        </li>
				<li>
            <a href="../tensorflow/zeros_initializer.htm">zeros_initializer</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> tf.losses</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow</p>
		</header>
    <div class="sub-header">
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow/tf.losses.htm#absolute_difference">absolute_difference</a></li>
				<li><a href="../tensorflow/tf.losses.htm#absolute_difference">absolute_difference</a></li>
				<li><a href="../tensorflow/tf.losses.htm#absolute_difference_dyn">absolute_difference_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#add_loss">add_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#add_loss">add_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss">compute_weighted_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss_dyn">compute_weighted_loss_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#cosine_distance">cosine_distance</a></li>
				<li><a href="../tensorflow/tf.losses.htm#cosine_distance">cosine_distance</a></li>
				<li><a href="../tensorflow/tf.losses.htm#cosine_distance_dyn">cosine_distance_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_regularization_loss">get_regularization_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_regularization_loss_dyn">get_regularization_loss_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_regularization_losses">get_regularization_losses</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_total_loss">get_total_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_total_loss_dyn">get_total_loss_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#hinge_loss">hinge_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#hinge_loss">hinge_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#hinge_loss">hinge_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#hinge_loss">hinge_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#hinge_loss_dyn">hinge_loss_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#huber_loss">huber_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#huber_loss_dyn">huber_loss_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#log_loss">log_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#log_loss">log_loss</a></li>
				<li><a href="../tensorflow/tf.losses.htm#log_loss_dyn">log_loss_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error">mean_pairwise_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error_dyn">mean_pairwise_squared_error_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error">mean_squared_error</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error_dyn">mean_squared_error_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy">sigmoid_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy_dyn">sigmoid_cross_entropy_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy">softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy_dyn">softmax_cross_entropy_dyn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy">sparse_softmax_cross_entropy</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy_dyn">sparse_softmax_cross_entropy_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow/tf.losses.htm#absolute_difference_fn">absolute_difference_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#add_loss_fn">add_loss_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#compute_weighted_loss_fn">compute_weighted_loss_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#cosine_distance_fn">cosine_distance_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_losses_fn">get_losses_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_regularization_loss_fn">get_regularization_loss_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_regularization_losses_fn">get_regularization_losses_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#get_total_loss_fn">get_total_loss_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#hinge_loss_fn">hinge_loss_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#huber_loss_fn">huber_loss_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#log_loss_fn">log_loss_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_pairwise_squared_error_fn">mean_pairwise_squared_error_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#mean_squared_error_fn">mean_squared_error_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sigmoid_cross_entropy_fn">sigmoid_cross_entropy_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#softmax_cross_entropy_fn">softmax_cross_entropy_fn</a></li>
				<li><a href="../tensorflow/tf.losses.htm#sparse_softmax_cross_entropy_fn">sparse_softmax_cross_entropy_fn</a></li>
			</ul>
		
	</div>
	
	
	<h3 class="section">Public static methods</h3>

	<div id="absolute_difference" class="method">
		<h4>
			<span title="System.object">object</span> <strong>absolute_difference</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds an Absolute Difference loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a `Tensor` of
shape `[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which this loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="absolute_difference" class="method">
		<h4>
			<span title="System.object">object</span> <strong>absolute_difference</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds an Absolute Difference loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a `Tensor` of
shape `[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which this loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="absolute_difference_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>absolute_difference_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds an Absolute Difference loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a `Tensor` of
shape `[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which this loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="add_loss" class="method">
		<h4>
			<span title="System.void">void</span> <strong>add_loss</strong>(<span title="System.string">string</span> loss, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a externally defined loss to the collection of losses. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> loss
						</dt>
						<dd>A loss `Tensor`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>Optional collection to add the loss to. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="add_loss" class="method">
		<h4>
			<span title="System.void">void</span> <strong>add_loss</strong>(<span title="System.object">object</span> loss, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a externally defined loss to the collection of losses. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> loss
						</dt>
						<dd>A loss `Tensor`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>Optional collection to add the loss to. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> losses, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../numpy/ndarray.htm">ndarray</a> losses, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../numpy/ndarray.htm">ndarray</a> losses, <span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../numpy/ndarray.htm">ndarray</a> losses, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> losses, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> losses, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> losses, <span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> losses, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> losses, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> losses, <span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> losses, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../numpy/ndarray.htm">ndarray</a> losses, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> losses, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.string">string</span> losses, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.string">string</span> losses, <span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.string">string</span> losses, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> losses, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.object">object</span> losses, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.string">string</span> losses, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.object">object</span> losses, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.object">object</span> losses, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> losses, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> losses, <span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss</strong>(<span title="System.object">object</span> losses, <span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<double, object, object>">ValueTuple&lt;double, object, object&gt;</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="compute_weighted_loss_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>compute_weighted_loss_dyn</strong>(<span title="System.object">object</span> losses, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Computes the weighted loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> losses
						</dt>
						<dd>`Tensor` of shape `[batch_size, d1,... dN]`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`losses`, and must be broadcastable to `losses` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>the loss will be added to these collections. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `losses`. If `reduction` is
`NONE`, this has the same shape as `losses`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="cosine_distance" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_distance</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.object">object</span> axis, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> dim)
		</h4>
		<div class="content">Adds a cosine-distance loss to the training procedure. (deprecated arguments) <p></p> Warning: SOME ARGUMENTS ARE DEPRECATED: `(dim)`. They will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead <p></p> Note that the function assumes that `predictions` and `labels` are already
unit-normalized. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` whose shape matches 'predictions' 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>An arbitrary matrix. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> axis
						</dt>
						<dd>The dimension along which the cosine distance is computed. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which this loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> dim
						</dt>
						<dd>The old (deprecated) name for `axis`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="cosine_distance" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_distance</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.object">object</span> axis, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction, <span title="System.Nullable<int>">Nullable&lt;int&gt;</span> dim)
		</h4>
		<div class="content">Adds a cosine-distance loss to the training procedure. (deprecated arguments) <p></p> Warning: SOME ARGUMENTS ARE DEPRECATED: `(dim)`. They will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead <p></p> Note that the function assumes that `predictions` and `labels` are already
unit-normalized. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` whose shape matches 'predictions' 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>An arbitrary matrix. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> axis
						</dt>
						<dd>The dimension along which the cosine distance is computed. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which this loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
						<dt>
							<code><span title="System.Nullable<int>">Nullable&lt;int&gt;</span></code> dim
						</dt>
						<dd>The old (deprecated) name for `axis`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="cosine_distance_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>cosine_distance_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <span title="System.object">object</span> axis, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction, <span title="System.object">object</span> dim)
		</h4>
		<div class="content">Adds a cosine-distance loss to the training procedure. (deprecated arguments) <p></p> Warning: SOME ARGUMENTS ARE DEPRECATED: `(dim)`. They will be removed in a future version.
Instructions for updating:
dim is deprecated, use axis instead <p></p> Note that the function assumes that `predictions` and `labels` are already
unit-normalized. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>`Tensor` whose shape matches 'predictions' 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>An arbitrary matrix. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> axis
						</dt>
						<dd>The dimension along which the cosine distance is computed. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which this loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> dim
						</dt>
						<dd>The old (deprecated) name for `axis`. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_regularization_loss" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>get_regularization_loss</strong>(<span title="System.string">string</span> scope, <span title="System.string">string</span> name)
		</h4>
		<div class="content">Gets the total regularization loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> scope
						</dt>
						<dd>An optional scope name for filtering the losses to return. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>The name of the returned tensor. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar regularization loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_regularization_loss_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_regularization_loss_dyn</strong>(<span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> name)
		</h4>
		<div class="content">Gets the total regularization loss. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>An optional scope name for filtering the losses to return. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> name
						</dt>
						<dd>The name of the returned tensor. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar regularization loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_regularization_losses" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_regularization_losses</strong>(<span title="System.string">string</span> scope)
		</h4>
		<div class="content">Gets the list of regularization losses. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> scope
						</dt>
						<dd>An optional scope name for filtering the losses to return. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A list of regularization losses as Tensors. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_total_loss" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>get_total_loss</strong>(<span title="System.bool">bool</span> add_regularization_losses, <span title="System.string">string</span> name, <span title="System.object">object</span> scope)
		</h4>
		<div class="content">Returns a tensor whose value represents the total loss. <p></p> In particular, this adds any losses you have added with `tf.add_loss()` to
any regularization losses that have been added by regularization parameters
on layers constructors e.g. <a href="..\..\tf\layers.md"><code>tf.layers</code></a>. Be very sure to use this if you
are constructing a loss_op manually. Otherwise regularization arguments
on <a href="..\..\tf\layers.md"><code>tf.layers</code></a> methods will not function. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.bool">bool</span></code> add_regularization_losses
						</dt>
						<dd>A boolean indicating whether or not to use the
regularization losses in the sum. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> name
						</dt>
						<dd>The name of the returned tensor. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>An optional scope name for filtering the losses to return. Note that
this filters the losses added with `tf.add_loss()` as well as the
regularization losses to that scope. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A `Tensor` whose value represents the total loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="get_total_loss_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>get_total_loss_dyn</strong>(<a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> add_regularization_losses, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> name, <span title="System.object">object</span> scope)
		</h4>
		<div class="content">Returns a tensor whose value represents the total loss. <p></p> In particular, this adds any losses you have added with `tf.add_loss()` to
any regularization losses that have been added by regularization parameters
on layers constructors e.g. <a href="..\..\tf\layers.md"><code>tf.layers</code></a>. Be very sure to use this if you
are constructing a loss_op manually. Otherwise regularization arguments
on <a href="..\..\tf\layers.md"><code>tf.layers</code></a> methods will not function. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> add_regularization_losses
						</dt>
						<dd>A boolean indicating whether or not to use the
regularization losses in the sum. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> name
						</dt>
						<dd>The name of the returned tensor. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>An optional scope name for filtering the losses to return. Note that
this filters the losses added with `tf.add_loss()` as well as the
regularization losses to that scope. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `Tensor` whose value represents the total loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="hinge_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>hinge_loss</strong>(<span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a hinge loss to the training procedure. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor. Its shape should match the shape of
logits. The values of the tensor are expected to be 0.0 or 1.0. Internally
the {0,1} labels are converted to {-1,1} when calculating the hinge loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>The logits, a float tensor. Note that logits are assumed to be
unbounded and 0-centered. A value > 0 (resp. < 0) is considered a positive
(resp. negative) binary prediction. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="hinge_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>hinge_loss</strong>(<span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.string">string</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a hinge loss to the training procedure. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor. Its shape should match the shape of
logits. The values of the tensor are expected to be 0.0 or 1.0. Internally
the {0,1} labels are converted to {-1,1} when calculating the hinge loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>The logits, a float tensor. Note that logits are assumed to be
unbounded and 0-centered. A value > 0 (resp. < 0) is considered a positive
(resp. negative) binary prediction. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="hinge_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>hinge_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a hinge loss to the training procedure. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor. Its shape should match the shape of
logits. The values of the tensor are expected to be 0.0 or 1.0. Internally
the {0,1} labels are converted to {-1,1} when calculating the hinge loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>The logits, a float tensor. Note that logits are assumed to be
unbounded and 0-centered. A value > 0 (resp. < 0) is considered a positive
(resp. negative) binary prediction. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="hinge_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>hinge_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.string">string</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a hinge loss to the training procedure. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor. Its shape should match the shape of
logits. The values of the tensor are expected to be 0.0 or 1.0. Internally
the {0,1} labels are converted to {-1,1} when calculating the hinge loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>The logits, a float tensor. Note that logits are assumed to be
unbounded and 0-centered. A value > 0 (resp. < 0) is considered a positive
(resp. negative) binary prediction. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="hinge_loss_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>hinge_loss_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> logits, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a hinge loss to the training procedure. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor. Its shape should match the shape of
logits. The values of the tensor are expected to be 0.0 or 1.0. Internally
the {0,1} labels are converted to {-1,1} when calculating the hinge loss. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>The logits, a float tensor. Note that logits are assumed to be
unbounded and 0-centered. A value > 0 (resp. < 0) is considered a positive
(resp. negative) binary prediction. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="huber_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>huber_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.double">double</span> weights, <span title="System.double">double</span> delta, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Huber Loss term to the training procedure. <p></p> For each value x in `error=labels-predictions`, the following is calculated: <p></p> ```
0.5 * x^2                  if |x| <= d
0.5 * d^2 + d * (|x| - d)  if |x| > d
``` <p></p> where d is `delta`. <p></p> See: https://en.wikipedia.org/wiki/Huber_loss <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> delta
						</dt>
						<dd>`float`, the point where the huber loss function
changes from a quadratic to linear. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="huber_loss_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>huber_loss_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> delta, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Huber Loss term to the training procedure. <p></p> For each value x in `error=labels-predictions`, the following is calculated: <p></p> ```
0.5 * x^2                  if |x| <= d
0.5 * d^2 + d * (|x| - d)  if |x| > d
``` <p></p> where d is `delta`. <p></p> See: https://en.wikipedia.org/wiki/Huber_loss <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> delta
						</dt>
						<dd>`float`, the point where the huber loss function
changes from a quadratic to linear. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="log_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>log_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.double">double</span> weights, <span title="System.double">double</span> epsilon, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Log Loss term to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>A small increment to add to avoid taking a log of zero. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="log_loss" class="method">
		<h4>
			<span title="System.object">object</span> <strong>log_loss</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.double">double</span> epsilon, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Log Loss term to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> epsilon
						</dt>
						<dd>A small increment to add to avoid taking a log of zero. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="log_loss_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>log_loss_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> epsilon, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Log Loss term to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> epsilon
						</dt>
						<dd>A small increment to add to avoid taking a log of zero. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <span title="System.int">int</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <span title="System.int">int</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.int">int</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <a href="../numpy/ndarray.htm">ndarray</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> predictions, <span title="System.int">int</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error" class="method">
		<h4>
			<a href="../tensorflow/Tensor.htm">Tensor</a> <strong>mean_pairwise_squared_error</strong>(<a href="../numpy/ndarray.htm">ndarray</a> labels, <a href="../numpy/ndarray.htm">ndarray</a> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow/Tensor.htm">Tensor</a></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_pairwise_squared_error_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_pairwise_squared_error_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection)
		</h4>
		<div class="content">Adds a pairwise-errors-squared loss to the training procedure. <p></p> Unlike `mean_squared_error`, which is a measure of the differences between
corresponding elements of `predictions` and `labels`,
`mean_pairwise_squared_error` is a measure of the differences between pairs of
corresponding elements of `predictions` and `labels`. <p></p> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are
three pairs of differences are summed to compute the loss:
loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3 <p></p> Note that since the inputs are of shape `[batch_size, d0,... dN]`, the
corresponding pairs are computed within each batch sample but not across
samples within a batch. For example, if `predictions` represents a batch of
16 grayscale images of dimension [batch_size, 100, 200], then the set of pairs
is drawn from each image, but not across images. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, whose shape must match the shape of
`predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs, a tensor of size
`[batch_size, d0,.. dN]` where N+1 is the total number of dimensions in
`predictions`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Coefficients for the loss a scalar, a tensor of shape
`[batch_size]` or a tensor whose shape matches `predictions`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A scalar `Tensor` that returns the weighted loss. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../numpy/float32.htm">float32</a> labels, <span title="System.object">object</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/float32.htm">float32</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../numpy/float32.htm">float32</a> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/float32.htm">float32</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> labels, <span title="System.object">object</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> labels, <span title="System.object">object</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../numpy/float32.htm">float32</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/float32.htm">float32</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.object">object</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.object">object</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.object">object</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error</strong>(<a href="../numpy/float32.htm">float32</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> predictions, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/float32.htm">float32</a></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="mean_squared_error_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>mean_squared_error_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> predictions, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Adds a Sum-of-Squares loss to the training procedure. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided, then
the loss is simply scaled by the given value. If `weights` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is rescaled
by the corresponding element in the `weights` vector. If the shape of
`weights` matches the shape of `predictions`, then the loss of each
measurable element of `predictions` is scaled by the corresponding value of
`weights`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>The ground truth output tensor, same dimensions as 'predictions'. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> predictions
						</dt>
						<dd>The predicted outputs. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same
shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <span title="System.double">double</span> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.double">double</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <span title="System.double">double</span> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> multi_class_labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.int">int</span> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sigmoid_cross_entropy_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sigmoid_cross_entropy_dyn</strong>(<span title="System.object">object</span> multi_class_labels, <span title="System.object">object</span> logits, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/2: <p></p> new_multiclass_labels = multiclass_labels * (1 - label_smoothing)
+ 0.5 * label_smoothing 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> multi_class_labels
						</dt>
						<dd>`[batch_size, num_classes]` target integer labels in
`{0, 1}`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>Float `[batch_size, num_classes]` logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` whose rank is either 0, or the same rank as
`labels`, and must be broadcastable to `labels` (i.e., all dimensions must
be either `1`, or the same as the corresponding `losses` dimension). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than `0` then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>The scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `logits`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <span title="System.object">object</span> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <span title="System.string">string</span> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <span title="System.string">string</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../tensorflow.contrib.seq2seq/AttentionWrapperState.htm">AttentionWrapperState</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <span title="System.object">object</span> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <span title="System.object">object</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <span title="System.string">string</span> logits, <span title="System.double">double</span> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> onehot_labels, <span title="System.string">string</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy</strong>(<span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> onehot_labels, <span title="System.object">object</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="softmax_cross_entropy_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>softmax_cross_entropy_dyn</strong>(<span title="System.object">object</span> onehot_labels, <span title="System.object">object</span> logits, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> label_smoothing, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. <p></p> If `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:
new_onehot_labels = onehot_labels * (1 - label_smoothing)
+ label_smoothing / num_classes <p></p> Note that `onehot_labels` and `logits` must have the same shape,
e.g. `[batch_size, num_classes]`. The shape of `weights` must be
broadcastable to loss, whose shape is decided by the shape of `logits`.
In case the shape of `logits` is `[batch_size, num_classes]`, loss is
a `Tensor` of shape `[batch_size]`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> onehot_labels
						</dt>
						<dd>One-hot-encoded labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>Logits outputs of the network. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Optional `Tensor` that is broadcastable to loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> label_smoothing
						</dt>
						<dd>If greater than 0 then smooth the labels. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has shape `[batch_size]`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../tensorflow/IndexedSlices.htm">IndexedSlices</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> labels, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> logits, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy</strong>(<span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span> labels, <span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span> logits, <span title="System.double">double</span> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.ValueTuple<PythonClassContainer, PythonClassContainer>">ValueTuple&lt;PythonClassContainer, PythonClassContainer&gt;</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<object, object>">IDictionary&lt;object, object&gt;</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>sparse_softmax_cross_entropy_dyn</strong>(<span title="System.object">object</span> labels, <span title="System.object">object</span> logits, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> weights, <span title="System.object">object</span> scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> loss_collection, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reduction)
		</h4>
		<div class="content">Cross-entropy loss using <a href="..\..\tf\nn\sparse_softmax_cross_entropy_with_logits.md"><code>tf.nn.sparse_softmax_cross_entropy_with_logits</code></a>. <p></p> `weights` acts as a coefficient for the loss. If a scalar is provided,
then the loss is simply scaled by the given value. If `weights` is a
tensor of shape `[batch_size]`, then the loss weights apply to each
corresponding sample. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> labels
						</dt>
						<dd>`Tensor` of shape `[d_0, d_1,..., d_{r-1}]` (where `r` is rank of
`labels` and result) and dtype `int32` or `int64`. Each entry in `labels`
must be an index in `[0, num_classes)`. Other values will raise an
exception when this op is run on CPU, and return `NaN` for corresponding
loss and gradient rows on GPU. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> logits
						</dt>
						<dd>Unscaled log probabilities of shape
`[d_0, d_1,..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or
`float64`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> weights
						</dt>
						<dd>Coefficients for the loss. This must be scalar or broadcastable to
`labels` (i.e. same rank and each dimension is either 1 or the same). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> scope
						</dt>
						<dd>the scope for the operations performed in computing the loss. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> loss_collection
						</dt>
						<dd>collection to which the loss will be added. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reduction
						</dt>
						<dd>Type of reduction to apply to loss. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss `Tensor` of the same type as `logits`. If `reduction` is
`NONE`, this has the same shape as `labels`; otherwise, it is scalar. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="absolute_difference_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>absolute_difference_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="add_loss_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>add_loss_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="compute_weighted_loss_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>compute_weighted_loss_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="cosine_distance_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>cosine_distance_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_losses_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_losses_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_regularization_loss_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_regularization_loss_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_regularization_losses_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_regularization_losses_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="get_total_loss_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>get_total_loss_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="hinge_loss_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>hinge_loss_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="huber_loss_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>huber_loss_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="log_loss_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>log_loss_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="mean_pairwise_squared_error_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>mean_pairwise_squared_error_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="mean_squared_error_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>mean_squared_error_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="sigmoid_cross_entropy_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>sigmoid_cross_entropy_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="softmax_cross_entropy_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>softmax_cross_entropy_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="sparse_softmax_cross_entropy_fn" class="method">
		<h4>
			<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> <strong>sparse_softmax_cross_entropy_fn</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>