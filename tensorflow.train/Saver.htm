<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>Saver - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.train</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.train/AdadeltaOptimizer.htm">AdadeltaOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/AdagradDAOptimizer.htm">AdagradDAOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/AdagradOptimizer.htm">AdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/AdamOptimizer.htm">AdamOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/Checkpoint.htm">Checkpoint</a>
        </li>
				<li>
            <a href="../tensorflow.train/CheckpointManager.htm">CheckpointManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/CheckpointSaverHook.htm">CheckpointSaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/CheckpointSaverListener.htm">CheckpointSaverListener</a>
        </li>
				<li>
            <a href="../tensorflow.train/ChiefSessionCreator.htm">ChiefSessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a>
        </li>
				<li>
            <a href="../tensorflow.train/Coordinator.htm">Coordinator</a>
        </li>
				<li>
            <a href="../tensorflow.train/ExponentialMovingAverage.htm">ExponentialMovingAverage</a>
        </li>
				<li>
            <a href="../tensorflow.train/FeedFnHook.htm">FeedFnHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/FinalOpsHook.htm">FinalOpsHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/FtrlOptimizer.htm">FtrlOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/GlobalStepWaiterHook.htm">GlobalStepWaiterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/GradientDescentOptimizer.htm">GradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdadeltaOptimizer.htm">IAdadeltaOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdagradDAOptimizer.htm">IAdagradDAOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdagradOptimizer.htm">IAdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdamOptimizer.htm">IAdamOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpoint.htm">ICheckpoint</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpointManager.htm">ICheckpointManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpointSaverHook.htm">ICheckpointSaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpointSaverListener.htm">ICheckpointSaverListener</a>
        </li>
				<li>
            <a href="../tensorflow.train/IChiefSessionCreator.htm">IChiefSessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/IClusterSpec.htm">IClusterSpec</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICoordinator.htm">ICoordinator</a>
        </li>
				<li>
            <a href="../tensorflow.train/IExponentialMovingAverage.htm">IExponentialMovingAverage</a>
        </li>
				<li>
            <a href="../tensorflow.train/IFeedFnHook.htm">IFeedFnHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IFinalOpsHook.htm">IFinalOpsHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IFtrlOptimizer.htm">IFtrlOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IGlobalStepWaiterHook.htm">IGlobalStepWaiterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IGradientDescentOptimizer.htm">IGradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ILoggingTensorHook.htm">ILoggingTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ILooperThread.htm">ILooperThread</a>
        </li>
				<li>
            <a href="../tensorflow.train/IMomentumOptimizer.htm">IMomentumOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IMonitoredSession.htm">IMonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/INanLossDuringTrainingError.htm">INanLossDuringTrainingError</a>
        </li>
				<li>
            <a href="../tensorflow.train/INanTensorHook.htm">INanTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IOptimizer.htm">IOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IProfilerHook.htm">IProfilerHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IProximalAdagradOptimizer.htm">IProximalAdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IProximalGradientDescentOptimizer.htm">IProximalGradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IQueueRunner.htm">IQueueRunner</a>
        </li>
				<li>
            <a href="../tensorflow.train/IRMSPropOptimizer.htm">IRMSPropOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISaver.htm">ISaver</a>
        </li>
				<li>
            <a href="../tensorflow.train/IScaffold.htm">IScaffold</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISecondOrStepTimer.htm">ISecondOrStepTimer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IServer.htm">IServer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionCreator.htm">ISessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionManager.htm">ISessionManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunArgs.htm">ISessionRunArgs</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunContext.htm">ISessionRunContext</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunHook.htm">ISessionRunHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunValues.htm">ISessionRunValues</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISingularMonitoredSession.htm">ISingularMonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/IStepCounterHook.htm">IStepCounterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IStopAtStepHook.htm">IStopAtStepHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISummarySaverHook.htm">ISummarySaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISupervisor.htm">ISupervisor</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISyncReplicasOptimizer.htm">ISyncReplicasOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IVocabInfo.htm">IVocabInfo</a>
        </li>
				<li>
            <a href="../tensorflow.train/IWorkerSessionCreator.htm">IWorkerSessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/LoggingTensorHook.htm">LoggingTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/LooperThread.htm">LooperThread</a>
        </li>
				<li>
            <a href="../tensorflow.train/MomentumOptimizer.htm">MomentumOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/NanLossDuringTrainingError.htm">NanLossDuringTrainingError</a>
        </li>
				<li>
            <a href="../tensorflow.train/NanTensorHook.htm">NanTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/Optimizer.htm">Optimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ProfilerHook.htm">ProfilerHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ProximalAdagradOptimizer.htm">ProximalAdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ProximalGradientDescentOptimizer.htm">ProximalGradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/QueueRunner.htm">QueueRunner</a>
        </li>
				<li>
            <a href="../tensorflow.train/RMSPropOptimizer.htm">RMSPropOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/Saver.htm" class="current">Saver</a>
        </li>
				<li>
            <a href="../tensorflow.train/Scaffold.htm">Scaffold</a>
        </li>
				<li>
            <a href="../tensorflow.train/SecondOrStepTimer.htm">SecondOrStepTimer</a>
        </li>
				<li>
            <a href="../tensorflow.train/Server.htm">Server</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionCreator.htm">SessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionManager.htm">SessionManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunArgs.htm">SessionRunArgs</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunContext.htm">SessionRunContext</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunHook.htm">SessionRunHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunValues.htm">SessionRunValues</a>
        </li>
				<li>
            <a href="../tensorflow.train/SingularMonitoredSession.htm">SingularMonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/StepCounterHook.htm">StepCounterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/StopAtStepHook.htm">StopAtStepHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/SummarySaverHook.htm">SummarySaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/Supervisor.htm">Supervisor</a>
        </li>
				<li>
            <a href="../tensorflow.train/SyncReplicasOptimizer.htm">SyncReplicasOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/train.htm">train</a>
        </li>
				<li>
            <a href="../tensorflow.train/VocabInfo.htm">VocabInfo</a>
        </li>
				<li>
            <a href="../tensorflow.train/WorkerSessionCreator.htm">WorkerSessionCreator</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> Saver</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.train</p>
		<p><strong>Parent</strong> <a href="../LostTech.Gradient/PythonObjectContainer.htm">PythonObjectContainer</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.train/ISaver.htm">ISaver</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">Saves and restores variables. <p></p> See [Variables](https://tensorflow.org/guide/variables)
for an overview of variables, saving and restoring. <p></p> The `Saver` class adds ops to save and restore variables to and from
*checkpoints*.  It also provides convenience methods to run these ops. <p></p> Checkpoints are binary files in a proprietary format which map variable names
to tensor values.  The best way to examine the contents of a checkpoint is to
load it using a `Saver`. <p></p> Savers can automatically number checkpoint filenames with a provided counter.
This lets you keep multiple checkpoints at different steps while training a
model.  For example you can number the checkpoint filenames with the training
step number.  To avoid filling up disks, savers manage checkpoint files
automatically. For example, they can keep only the N most recent files, or
one checkpoint for every N hours of training. <p></p> You number checkpoint filenames by passing a value to the optional
`global_step` argument to `save()`:
Additionally, optional arguments to the `Saver()` constructor let you control
the proliferation of checkpoint files on disk: <p></p> * `max_to_keep` indicates the maximum number of recent checkpoint files to
keep.  As new files are created, older files are deleted.   If None or 0,
no checkpoints are deleted from the filesystem but only the last one is
kept in the `checkpoint` file.  Defaults to 5 (that is, the 5 most recent
checkpoint files are kept.) <p></p> * `keep_checkpoint_every_n_hours`: In addition to keeping the most recent
`max_to_keep` checkpoint files, you might want to keep one checkpoint file
for every N hours of training.  This can be useful if you want to later
analyze how a model progressed during a long training session.  For
example, passing `keep_checkpoint_every_n_hours=2` ensures that you keep
one checkpoint file for every 2 hours of training.  The default value of
10,000 hours effectively disables the feature. <p></p> Note that you still have to call the `save()` method to save the model.
Passing these arguments to the constructor will not save variables
automatically for you. <p></p> A training program that saves regularly looks like:
In addition to checkpoint files, savers keep a protocol buffer on disk with
the list of recent checkpoints. This is used to manage numbered checkpoint
files and by `latest_checkpoint()`, which makes it easy to discover the path
to the most recent checkpoint. That protocol buffer is stored in a file named
'checkpoint' next to the checkpoint files. <p></p> If you create several savers, you can specify a different filename for the
protocol buffer file in the call to `save()`. <div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>saver.save(sess, 'my-model', global_step=0) ==> filename: 'my-model-0'
           ...
            saver.save(sess, 'my-model', global_step=1000) ==> filename: 'my-model-1000' </pre>
</div>
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.train/Saver.htm#as_saver_def_dyn">as_saver_def_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#build_dyn">build_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#export_meta_graph">export_meta_graph</a></li>
				<li><a href="../tensorflow.train/Saver.htm#export_meta_graph">export_meta_graph</a></li>
				<li><a href="../tensorflow.train/Saver.htm#export_meta_graph_dyn">export_meta_graph_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#from_proto">from_proto</a></li>
				<li><a href="../tensorflow.train/Saver.htm#from_proto_dyn">from_proto_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#NewDyn">NewDyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#recover_last_checkpoints">recover_last_checkpoints</a></li>
				<li><a href="../tensorflow.train/Saver.htm#recover_last_checkpoints_dyn">recover_last_checkpoints_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Saver.htm#restore_dyn">restore_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Saver.htm#save_dyn">save_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#set_last_checkpoints_">set_last_checkpoints_</a></li>
				<li><a href="../tensorflow.train/Saver.htm#set_last_checkpoints__dyn">set_last_checkpoints__dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#set_last_checkpoints_with_time">set_last_checkpoints_with_time</a></li>
				<li><a href="../tensorflow.train/Saver.htm#set_last_checkpoints_with_time_dyn">set_last_checkpoints_with_time_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.train/Saver.htm#last_checkpoints">last_checkpoints</a></li>
				<li><a href="../tensorflow.train/Saver.htm#last_checkpoints_dyn">last_checkpoints_dyn</a></li>
				<li><a href="../tensorflow.train/Saver.htm#PythonObject">PythonObject</a></li>
				<li><a href="../tensorflow.train/Saver.htm#saver_def">saver_def</a></li>
			</ul>
		
	</div>
	
	<h3 class="section">Public instance methods</h3>

	<div id="as_saver_def_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>as_saver_def_dyn</strong>()
		</h4>
		<div class="content">Generates a `SaverDef` representation of this saver. 



			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `SaverDef` proto. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="build_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>build_dyn</strong>()
		</h4>
		<div class="content">Build a profiling option. 



			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A dict of profiling options. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="export_meta_graph" class="method">
		<h4>
			<span title="System.object">object</span> <strong>export_meta_graph</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> filename, <span title="System.object">object</span> collection_list, <span title="System.bool">bool</span> as_text, <span title="System.object">object</span> export_scope, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> clear_devices, <span title="System.bool">bool</span> clear_extraneous_savers, <a href="../tensorflow.train/Saver.htm">Saver</a> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Writes `MetaGraphDef` to save_path/filename. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> filename
						</dt>
						<dd>Optional meta_graph filename including the path. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> collection_list
						</dt>
						<dd>List of string keys to collect. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> as_text
						</dt>
						<dd>If `True`, writes the meta_graph as an ASCII proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> export_scope
						</dt>
						<dd>Optional `string`. Name scope to remove. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during export. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> clear_extraneous_savers
						</dt>
						<dd>Remove any Saver-related information from the
graph (both Save/Restore ops and SaverDefs) that are not associated with
this Saver. 
						</dd>
						<dt>
							<code><a href="../tensorflow.train/Saver.htm">Saver</a></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of filename and with `_debug` added before
the file extension. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `MetaGraphDef` proto. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="export_meta_graph" class="method">
		<h4>
			<span title="System.object">object</span> <strong>export_meta_graph</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> filename, <span title="System.object">object</span> collection_list, <span title="System.bool">bool</span> as_text, <span title="System.object">object</span> export_scope, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> clear_devices, <span title="System.bool">bool</span> clear_extraneous_savers, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Writes `MetaGraphDef` to save_path/filename. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> filename
						</dt>
						<dd>Optional meta_graph filename including the path. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> collection_list
						</dt>
						<dd>List of string keys to collect. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> as_text
						</dt>
						<dd>If `True`, writes the meta_graph as an ASCII proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> export_scope
						</dt>
						<dd>Optional `string`. Name scope to remove. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during export. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> clear_extraneous_savers
						</dt>
						<dd>Remove any Saver-related information from the
graph (both Save/Restore ops and SaverDefs) that are not associated with
this Saver. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of filename and with `_debug` added before
the file extension. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `MetaGraphDef` proto. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="export_meta_graph_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>export_meta_graph_dyn</strong>(<span title="System.object">object</span> filename, <span title="System.object">object</span> collection_list, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> as_text, <span title="System.object">object</span> export_scope, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> clear_devices, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> clear_extraneous_savers, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strip_default_attrs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_debug_info)
		</h4>
		<div class="content">Writes `MetaGraphDef` to save_path/filename. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> filename
						</dt>
						<dd>Optional meta_graph filename including the path. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> collection_list
						</dt>
						<dd>List of string keys to collect. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> as_text
						</dt>
						<dd>If `True`, writes the meta_graph as an ASCII proto. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> export_scope
						</dt>
						<dd>Optional `string`. Name scope to remove. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> clear_devices
						</dt>
						<dd>Whether or not to clear the device field for an `Operation`
or `Tensor` during export. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> clear_extraneous_savers
						</dt>
						<dd>Remove any Saver-related information from the
graph (both Save/Restore ops and SaverDefs) that are not associated with
this Saver. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of filename and with `_debug` added before
the file extension. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `MetaGraphDef` proto. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="recover_last_checkpoints" class="method">
		<h4>
			<span title="System.void">void</span> <strong>recover_last_checkpoints</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> checkpoint_paths)
		</h4>
		<div class="content">Recovers the internal saver state after a crash. <p></p> This method is useful for recovering the "self._last_checkpoints" state. <p></p> Globs for the checkpoints pointed to by `checkpoint_paths`.  If the files
exist, use their mtime as the checkpoint timestamp. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> checkpoint_paths
						</dt>
						<dd>a list of checkpoint paths. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="recover_last_checkpoints_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>recover_last_checkpoints_dyn</strong>(<span title="System.object">object</span> checkpoint_paths)
		</h4>
		<div class="content">Recovers the internal saver state after a crash. <p></p> This method is useful for recovering the "self._last_checkpoints" state. <p></p> Globs for the checkpoints pointed to by `checkpoint_paths`.  If the files
exist, use their mtime as the checkpoint timestamp. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> checkpoint_paths
						</dt>
						<dd>a list of checkpoint paths. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a> sess, <span title="System.string">string</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a> sess, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a> sess, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a> sess, <span title="System.Byte[]">Byte[]</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_MonitoredSession.htm">_MonitoredSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a> sess, <span title="System.string">string</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a> sess, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a> sess, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a> sess, <span title="System.Byte[]">Byte[]</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.eager.wrap_function/WrappedFunction.htm">WrappedFunction</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.string">string</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.void">void</span> <strong>restore</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.Byte[]">Byte[]</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="restore_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>restore_dyn</strong>(<span title="System.object">object</span> sess, <span title="System.object">object</span> save_path)
		</h4>
		<div class="content">Restores previously saved variables. <p></p> This method runs the ops added by the constructor for restoring variables.
It requires a session in which the graph was launched.  The variables to
restore do not have to have been initialized, as restoring is itself a way
to initialize variables. <p></p> The `save_path` argument is typically a value previously returned from a
`save()` call, or a call to `latest_checkpoint()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> sess
						</dt>
						<dd>A `Session` to use to restore the parameters. None in eager mode. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> save_path
						</dt>
						<dd>Path where parameters were previously saved. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a> sess, <span title="System.string">string</span> save_path, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a> sess, <span title="System.string">string</span> save_path, <span title="System.int">int</span> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a> sess, <span title="System.Byte[]">Byte[]</span> save_path, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a> sess, <span title="System.Byte[]">Byte[]</span> save_path, <span title="System.int">int</span> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.training.monitored_session/_WrappedSession.htm">_WrappedSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.string">string</span> save_path, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.string">string</span> save_path, <span title="System.int">int</span> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.Byte[]">Byte[]</span> save_path, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> sess, <span title="System.Byte[]">Byte[]</span> save_path, <span title="System.int">int</span> global_step, <span title="System.string">string</span> latest_filename, <span title="System.string">string</span> meta_graph_suffix, <span title="System.bool">bool</span> write_meta_graph, <span title="System.bool">bool</span> write_state, <span title="System.bool">bool</span> strip_default_attrs, <span title="System.bool">bool</span> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><span title="System.string">string</span></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save_dyn</strong>(<span title="System.object">object</span> sess, <span title="System.object">object</span> save_path, <span title="System.object">object</span> global_step, <span title="System.object">object</span> latest_filename, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> meta_graph_suffix, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> write_meta_graph, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> write_state, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> strip_default_attrs, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_debug_info)
		</h4>
		<div class="content">Saves variables. <p></p> This method runs the ops added by the constructor for saving variables.
It requires a session in which the graph was launched.  The variables to
save must also have been initialized. <p></p> The method returns the path prefix of the newly created checkpoint files.
This string can be passed directly to a call to `restore()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> sess
						</dt>
						<dd>A Session to use to save the variables. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> save_path
						</dt>
						<dd>String.  Prefix of filenames created for the checkpoint. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> global_step
						</dt>
						<dd>If provided the global step number is appended to `save_path`
to create the checkpoint filenames. The optional argument can be a
`Tensor`, a `Tensor` name or an integer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> latest_filename
						</dt>
						<dd>Optional name for the protocol buffer file that will
contains the list of most recent checkpoints.  That file, kept in the
same directory as the checkpoint files, is automatically managed by the
saver to keep track of recent checkpoints.  Defaults to 'checkpoint'. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> meta_graph_suffix
						</dt>
						<dd>Suffix for `MetaGraphDef` file. Defaults to 'meta'. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> write_meta_graph
						</dt>
						<dd>`Boolean` indicating whether or not to write the meta
graph file. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> write_state
						</dt>
						<dd>`Boolean` indicating whether or not to write the
`CheckpointStateProto`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> strip_default_attrs
						</dt>
						<dd>Boolean. If `True`, default-valued attributes will be
removed from the NodeDefs. For a detailed guide, see
[Stripping Default-Valued
Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_debug_info
						</dt>
						<dd>If `True`, save the GraphDebugInfo to a separate file,
which in the same directory of save_path and with `_debug` added before
the file extension. This is only enabled when `write_meta_graph` is
`True` 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A string: path prefix used for the checkpoint files.  If the saver is
sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn'
is the number of shards created.
If the saver is empty, returns None. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="set_last_checkpoints_" class="method">
		<h4>
			<span title="System.void">void</span> <strong>set_last_checkpoints_</strong>(<span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> last_checkpoints)
		</h4>
		<div class="content">




		</div>
	</div>
	<div id="set_last_checkpoints__dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>set_last_checkpoints__dyn</strong>(<span title="System.object">object</span> last_checkpoints)
		</h4>
		<div class="content">




		</div>
	</div>
	<div id="set_last_checkpoints_with_time" class="method">
		<h4>
			<span title="System.void">void</span> <strong>set_last_checkpoints_with_time</strong>(<span title="System.Collections.Generic.IEnumerable<ValueTuple<object, object>>">IEnumerable&lt;ValueTuple&lt;object, object&gt;&gt;</span> last_checkpoints_with_time)
		</h4>
		<div class="content">Sets the list of old checkpoint filenames and timestamps. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<ValueTuple<object, object>>">IEnumerable&lt;ValueTuple&lt;object, object&gt;&gt;</span></code> last_checkpoints_with_time
						</dt>
						<dd>A list of tuples of checkpoint filenames and
timestamps. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	<div id="set_last_checkpoints_with_time_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>set_last_checkpoints_with_time_dyn</strong>(<span title="System.object">object</span> last_checkpoints_with_time)
		</h4>
		<div class="content">Sets the list of old checkpoint filenames and timestamps. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> last_checkpoints_with_time
						</dt>
						<dd>A list of tuples of checkpoint filenames and
timestamps. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	
	<h3 class="section">Public static methods</h3>

	<div id="from_proto" class="method">
		<h4>
			<a href="../tensorflow.train/Saver.htm">Saver</a> <strong>from_proto</strong>(<span title="System.object">object</span> saver_def, <span title="System.object">object</span> import_scope)
		</h4>
		<div class="content">Returns a `Saver` object created from `saver_def`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> saver_def
						</dt>
						<dd>a `SaverDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> import_scope
						</dt>
						<dd>Optional `string`. Name scope to use. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.train/Saver.htm">Saver</a></code>
					</dt>
					<dd>A `Saver` built from saver_def. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="from_proto_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>from_proto_dyn</strong>(<span title="System.object">object</span> saver_def, <span title="System.object">object</span> import_scope)
		</h4>
		<div class="content">Returns a `Saver` object created from `saver_def`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> saver_def
						</dt>
						<dd>a `SaverDef` protocol buffer. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> import_scope
						</dt>
						<dd>Optional `string`. Name scope to use. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A `Saver` built from saver_def. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="NewDyn" class="method">
		<h4>
			<a href="../tensorflow.train/Saver.htm">Saver</a> <strong>NewDyn</strong>(<span title="System.object">object</span> var_list, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> reshape, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> sharded, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> max_to_keep, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> keep_checkpoint_every_n_hours, <span title="System.object">object</span> name, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> restore_sequentially, <span title="System.object">object</span> saver_def, <span title="System.object">object</span> builder, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> defer_build, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> allow_empty, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> write_version, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> pad_step_number, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> save_relative_paths, <span title="System.object">object</span> filename)
		</h4>
		<div class="content">Creates a `Saver`. <p></p> The constructor adds ops to save and restore variables. <p></p> `var_list` specifies the variables that will be saved and restored. It can
be passed as a `dict` or a list: <p></p> * A `dict` of names to variables: The keys are the names that will be
used to save or restore the variables in the checkpoint files.
* A list of variables: The variables will be keyed with their op name in
the checkpoint files.
Note: the newer `AutoTrackable` API is not supported by `Saver`. In this
case, the <a href="..\..\tf\train\Checkpoint.md"><code>tf.train.Checkpoint</code></a> class should be used. <p></p> The optional `reshape` argument, if `True`, allows restoring a variable from
a save file where the variable had a different shape, but the same number
of elements and type.  This is useful if you have reshaped a variable and
want to reload it from an older checkpoint. <p></p> The optional `sharded` argument, if `True`, instructs the saver to shard
checkpoints per device. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> var_list
						</dt>
						<dd>A list of `Variable`/`SaveableObject`, or a dictionary mapping
names to `SaveableObject`s. If `None`, defaults to the list of all
saveable objects. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> reshape
						</dt>
						<dd>If `True`, allows restoring parameters from a checkpoint where
the variables have a different shape. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> sharded
						</dt>
						<dd>If `True`, shard the checkpoints, one per device. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> max_to_keep
						</dt>
						<dd>Maximum number of recent checkpoints to keep. Defaults to 5. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> keep_checkpoint_every_n_hours
						</dt>
						<dd>How often to keep checkpoints. Defaults to
10,000 hours. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> name
						</dt>
						<dd>String.  Optional name to use as a prefix when adding operations. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> restore_sequentially
						</dt>
						<dd>A `Bool`, which if true, causes restore of different
variables to happen sequentially within each device.  This can lower
memory usage when restoring very large models. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> saver_def
						</dt>
						<dd>Optional `SaverDef` proto to use instead of running the
builder. This is only useful for specialty code that wants to recreate a
`Saver` object for a previously built `Graph` that had a `Saver`. The
`saver_def` proto should be the one returned by the `as_saver_def()`
call of the `Saver` that was created for that `Graph`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> builder
						</dt>
						<dd>Optional `SaverBuilder` to use if a `saver_def` was not provided.
Defaults to `BulkSaverBuilder()`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> defer_build
						</dt>
						<dd>If `True`, defer adding the save and restore ops to the
`build()` call. In that case `build()` should be called before
finalizing the graph or using the saver. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> allow_empty
						</dt>
						<dd>If `False` (default) raise an error if there are no variables
in the graph. Otherwise, construct the saver anyway and make it a no-op. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> write_version
						</dt>
						<dd>controls what format to use when saving checkpoints.  It
also affects certain filepath matching logic.  The V2 format is the
recommended choice: it is much more optimized than V1 in terms of memory
required and latency incurred during restore.  Regardless of this
flag, the Saver is able to restore from both V2 and V1 checkpoints. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> pad_step_number
						</dt>
						<dd>if True, pads the global step number in the checkpoint
filepaths to some fixed width (8 by default).  This is turned off by
default. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> save_relative_paths
						</dt>
						<dd>If `True`, will write relative paths to the
checkpoint state file. This is needed if the user wants to copy the
checkpoint directory and reload from the copied directory. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> filename
						</dt>
						<dd>If known at graph construction time, filename used for variable
loading/saving. 
						</dd>
				</dl>
			</div>

<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>v1 = tf.Variable(..., name='v1')
            v2 = tf.Variable(..., name='v2') <p></p> # Pass the variables as a dict:
saver = tf.compat.v1.train.Saver({'v1': v1, 'v2': v2}) <p></p> # Or pass them as a list.
saver = tf.compat.v1.train.Saver([v1, v2])
# Passing a list is equivalent to passing a dict with the variable op names
# as keys:
saver = tf.compat.v1.train.Saver({v.op.name: v for v in [v1, v2]}) </pre>
</div>
		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="last_checkpoints" class="method">
		<h4>
			<span title="System.Collections.Generic.IList<object>">IList&lt;object&gt;</span> <strong>last_checkpoints</strong> get; 
		</h4>
		<div class="content">List of not-yet-deleted checkpoint filenames. <p></p> You can pass any of the returned values to `restore()`. 

		</div>
	</div>
	<div id="last_checkpoints_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>last_checkpoints_dyn</strong> get; 
		</h4>
		<div class="content">List of not-yet-deleted checkpoint filenames. <p></p> You can pass any of the returned values to `restore()`. 

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="saver_def" class="method">
		<h4>
			<span title="System.Nullable<int>">Nullable&lt;int&gt;</span> <strong>saver_def</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>