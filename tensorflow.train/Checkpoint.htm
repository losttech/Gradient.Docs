<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>Checkpoint - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.train</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.train/AdadeltaOptimizer.htm">AdadeltaOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/AdagradDAOptimizer.htm">AdagradDAOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/AdagradOptimizer.htm">AdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/AdamOptimizer.htm">AdamOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/Checkpoint.htm" class="current">Checkpoint</a>
        </li>
				<li>
            <a href="../tensorflow.train/CheckpointManager.htm">CheckpointManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/CheckpointSaverHook.htm">CheckpointSaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/CheckpointSaverListener.htm">CheckpointSaverListener</a>
        </li>
				<li>
            <a href="../tensorflow.train/ChiefSessionCreator.htm">ChiefSessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/ClusterSpec.htm">ClusterSpec</a>
        </li>
				<li>
            <a href="../tensorflow.train/Coordinator.htm">Coordinator</a>
        </li>
				<li>
            <a href="../tensorflow.train/ExponentialMovingAverage.htm">ExponentialMovingAverage</a>
        </li>
				<li>
            <a href="../tensorflow.train/FeedFnHook.htm">FeedFnHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/FinalOpsHook.htm">FinalOpsHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/FtrlOptimizer.htm">FtrlOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/GlobalStepWaiterHook.htm">GlobalStepWaiterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/GradientDescentOptimizer.htm">GradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdadeltaOptimizer.htm">IAdadeltaOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdagradDAOptimizer.htm">IAdagradDAOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdagradOptimizer.htm">IAdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IAdamOptimizer.htm">IAdamOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpoint.htm">ICheckpoint</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpointManager.htm">ICheckpointManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpointSaverHook.htm">ICheckpointSaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICheckpointSaverListener.htm">ICheckpointSaverListener</a>
        </li>
				<li>
            <a href="../tensorflow.train/IChiefSessionCreator.htm">IChiefSessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/IClusterSpec.htm">IClusterSpec</a>
        </li>
				<li>
            <a href="../tensorflow.train/ICoordinator.htm">ICoordinator</a>
        </li>
				<li>
            <a href="../tensorflow.train/IExponentialMovingAverage.htm">IExponentialMovingAverage</a>
        </li>
				<li>
            <a href="../tensorflow.train/IFeedFnHook.htm">IFeedFnHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IFinalOpsHook.htm">IFinalOpsHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IFtrlOptimizer.htm">IFtrlOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IGlobalStepWaiterHook.htm">IGlobalStepWaiterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IGradientDescentOptimizer.htm">IGradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ILoggingTensorHook.htm">ILoggingTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ILooperThread.htm">ILooperThread</a>
        </li>
				<li>
            <a href="../tensorflow.train/IMomentumOptimizer.htm">IMomentumOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IMonitoredSession.htm">IMonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/INanLossDuringTrainingError.htm">INanLossDuringTrainingError</a>
        </li>
				<li>
            <a href="../tensorflow.train/INanTensorHook.htm">INanTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IOptimizer.htm">IOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IProfilerHook.htm">IProfilerHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IProximalAdagradOptimizer.htm">IProximalAdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IProximalGradientDescentOptimizer.htm">IProximalGradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IQueueRunner.htm">IQueueRunner</a>
        </li>
				<li>
            <a href="../tensorflow.train/IRMSPropOptimizer.htm">IRMSPropOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISaver.htm">ISaver</a>
        </li>
				<li>
            <a href="../tensorflow.train/IScaffold.htm">IScaffold</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISecondOrStepTimer.htm">ISecondOrStepTimer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IServer.htm">IServer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionCreator.htm">ISessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionManager.htm">ISessionManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunArgs.htm">ISessionRunArgs</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunContext.htm">ISessionRunContext</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunHook.htm">ISessionRunHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISessionRunValues.htm">ISessionRunValues</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISingularMonitoredSession.htm">ISingularMonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/IStepCounterHook.htm">IStepCounterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/IStopAtStepHook.htm">IStopAtStepHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISummarySaverHook.htm">ISummarySaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISupervisor.htm">ISupervisor</a>
        </li>
				<li>
            <a href="../tensorflow.train/ISyncReplicasOptimizer.htm">ISyncReplicasOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/IVocabInfo.htm">IVocabInfo</a>
        </li>
				<li>
            <a href="../tensorflow.train/IWorkerSessionCreator.htm">IWorkerSessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/LoggingTensorHook.htm">LoggingTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/LooperThread.htm">LooperThread</a>
        </li>
				<li>
            <a href="../tensorflow.train/MomentumOptimizer.htm">MomentumOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/MonitoredSession.htm">MonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/NanLossDuringTrainingError.htm">NanLossDuringTrainingError</a>
        </li>
				<li>
            <a href="../tensorflow.train/NanTensorHook.htm">NanTensorHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/Optimizer.htm">Optimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ProfilerHook.htm">ProfilerHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/ProximalAdagradOptimizer.htm">ProximalAdagradOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/ProximalGradientDescentOptimizer.htm">ProximalGradientDescentOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/QueueRunner.htm">QueueRunner</a>
        </li>
				<li>
            <a href="../tensorflow.train/RMSPropOptimizer.htm">RMSPropOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/Saver.htm">Saver</a>
        </li>
				<li>
            <a href="../tensorflow.train/Scaffold.htm">Scaffold</a>
        </li>
				<li>
            <a href="../tensorflow.train/SecondOrStepTimer.htm">SecondOrStepTimer</a>
        </li>
				<li>
            <a href="../tensorflow.train/Server.htm">Server</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionCreator.htm">SessionCreator</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionManager.htm">SessionManager</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunArgs.htm">SessionRunArgs</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunContext.htm">SessionRunContext</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunHook.htm">SessionRunHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/SessionRunValues.htm">SessionRunValues</a>
        </li>
				<li>
            <a href="../tensorflow.train/SingularMonitoredSession.htm">SingularMonitoredSession</a>
        </li>
				<li>
            <a href="../tensorflow.train/StepCounterHook.htm">StepCounterHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/StopAtStepHook.htm">StopAtStepHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/SummarySaverHook.htm">SummarySaverHook</a>
        </li>
				<li>
            <a href="../tensorflow.train/Supervisor.htm">Supervisor</a>
        </li>
				<li>
            <a href="../tensorflow.train/SyncReplicasOptimizer.htm">SyncReplicasOptimizer</a>
        </li>
				<li>
            <a href="../tensorflow.train/train.htm">train</a>
        </li>
				<li>
            <a href="../tensorflow.train/VocabInfo.htm">VocabInfo</a>
        </li>
				<li>
            <a href="../tensorflow.train/WorkerSessionCreator.htm">WorkerSessionCreator</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> Checkpoint</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.train</p>
		<p><strong>Parent</strong> <a href="../tensorflow.python.training.tracking.tracking/AutoTrackable.htm">AutoTrackable</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.train/ICheckpoint.htm">ICheckpoint</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">Groups trackable objects, saving and restoring them. <p></p> `Checkpoint`'s constructor accepts keyword arguments whose values are types
that contain trackable state, such as `tf.compat.v1.train.Optimizer`
implementations, <a href="..\..\tf\Variable.md"><code>tf.Variable</code></a>, `tf.keras.Layer` implementations, or
<a href="..\..\tf\keras\Model.md"><code>tf.keras.Model</code></a> implementations. It saves these values with a checkpoint, and
maintains a `save_counter` for numbering checkpoints. <p></p> Example usage when graph building:
Example usage with eager execution enabled:
`Checkpoint.save` and `Checkpoint.restore` write and read object-based
checkpoints, in contrast to `tf.compat.v1.train.Saver` which writes and reads
`variable.name` based checkpoints. Object-based checkpointing saves a graph of
dependencies between Python objects (`Layer`s, `Optimizer`s, `Variable`s,
etc.) with named edges, and this graph is used to match variables when
restoring a checkpoint. It can be more robust to changes in the Python
program, and helps to support restore-on-create for variables when executing
eagerly. Prefer <a href="..\..\tf\train\Checkpoint.md"><code>tf.train.Checkpoint</code></a> over `tf.compat.v1.train.Saver` for new
code. <p></p> `Checkpoint` objects have dependencies on the objects passed as keyword
arguments to their constructors, and each dependency is given a name that is
identical to the name of the keyword argument for which it was created.
TensorFlow classes like `Layer`s and `Optimizer`s will automatically add
dependencies on their variables (e.g. "kernel" and "bias" for
<a href="..\..\tf\keras\layers\Dense.md"><code>tf.keras.layers.Dense</code></a>). Inheriting from <a href="..\..\tf\keras\Model.md"><code>tf.keras.Model</code></a> makes managing
dependencies easy in user-defined classes, since `Model` hooks into attribute
assignment.
This `Model` has a dependency named "input_transform" on its `Dense` layer,
which in turn depends on its variables. As a result, saving an instance of
`Regress` using <a href="..\..\tf\train\Checkpoint.md"><code>tf.train.Checkpoint</code></a> will also save all the variables created
by the `Dense` layer. <p></p> When variables are assigned to multiple workers, each worker writes its own
section of the checkpoint. These sections are then merged/re-indexed to behave
as a single checkpoint. This avoids copying all variables to one worker, but
does require that all workers see a common filesystem. <p></p> While <a href="..\..\tf\keras\Model\save_weights.md"><code>tf.keras.Model.save_weights</code></a> and <a href="..\..\tf\train\Checkpoint\save.md"><code>tf.train.Checkpoint.save</code></a> save in the
same format, note that the root of the resulting checkpoint is the object the
save method is attached to. This means saving a <a href="..\..\tf\keras\Model.md"><code>tf.keras.Model</code></a> using
`save_weights` and loading into a <a href="..\..\tf\train\Checkpoint.md"><code>tf.train.Checkpoint</code></a> with a `Model`
attached (or vice versa) will not match the `Model`'s variables. See the
[guide to training
checkpoints](https://www.tensorflow.org/alpha/guide/checkpoints) for
details. Prefer <a href="..\..\tf\train\Checkpoint.md"><code>tf.train.Checkpoint</code></a> over <a href="..\..\tf\keras\Model\save_weights.md"><code>tf.keras.Model.save_weights</code></a> for
training checkpoints. <div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import tensorflow as tf
            import os <p></p> checkpoint_directory = "/tmp/training_checkpoints"
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt") <p></p> checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))
train_op = optimizer.minimize(... )
status.assert_consumed()  # Optional sanity checks.
with tf.compat.v1.Session() as session:
  # Use the Session to restore variables, or initialize them if
  # tf.train.latest_checkpoint returned None.
  status.initialize_or_restore(session)
  for _ in range(num_training_steps):
    session.run(train_op)
  checkpoint.save(file_prefix=checkpoint_prefix) </pre>
</div>
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.train/Checkpoint.htm#NewDyn">NewDyn</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#restore">restore</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#restore_dyn">restore_dyn</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#save">save</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#save_dyn">save_dyn</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#write">write</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#write">write</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#write_dyn">write_dyn</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.train/Checkpoint.htm#PythonObject">PythonObject</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#save_counter">save_counter</a></li>
				<li><a href="../tensorflow.train/Checkpoint.htm#save_counter_dyn">save_counter_dyn</a></li>
			</ul>
		
	</div>
	
	<h3 class="section">Public instance methods</h3>

	<div id="restore" class="method">
		<h4>
			<span title="System.object">object</span> <strong>restore</strong>(<span title="System.Byte[]">Byte[]</span> save_path)
		</h4>
		<div class="content">Restore a training checkpoint. <p></p> Restores this `Checkpoint` and any objects it depends on. <p></p> When executing eagerly, either assigns values immediately if variables to
restore have been created already, or defers restoration until the variables
are created. Dependencies added after this call will be matched if they have
a corresponding object in the checkpoint (the restore request will queue in
any trackable object waiting for the expected dependency to be added). <p></p> When graph building, restoration ops are added to the graph but not run
immediately. <p></p> To ensure that loading is complete and no more assignments will take place,
use the `assert_consumed()` method of the status object returned by
`restore`:
An exception will be raised if any Python objects in the dependency graph
were not found in the checkpoint, or if any checkpointed values do not have
a matching Python object. <p></p> When graph building, `assert_consumed()` indicates that all of the restore
ops that will be created for this checkpoint have been created. They can be
run via the `run_restore_ops()` method of the status object:
If the checkpoint has not been consumed completely, then the list of restore
ops will grow as more objects are added to the dependency graph. <p></p> Name-based `tf.compat.v1.train.Saver` checkpoints can be loaded using this
method. Names are used to match variables. No restore ops are created/run
until `run_restore_ops()` or `initialize_or_restore()` are called on the
returned status object when graph building, but there is restore-on-creation
when executing eagerly. Re-encode name-based checkpoints using
<a href="..\..\tf\train\Checkpoint\save.md"><code>tf.train.Checkpoint.save</code></a> as soon as possible. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> save_path
						</dt>
						<dd>The path to the checkpoint, as returned by `save` or
<a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. If None (as when there is no latest
checkpoint for <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a> to return), returns an
object which may run initializers for objects in the dependency graph.
If the checkpoint was written by the name-based
`tf.compat.v1.train.Saver`, names are used to match variables. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A load status object, which can be used to make assertions about the
status of a checkpoint restoration and run initialization/restore ops. <p></p> The returned status object has the following methods: <p></p> * `assert_consumed()`:
Raises an exception if any variables/objects are unmatched: either
checkpointed values which don't have a matching Python object or
Python objects in the dependency graph with no values in the
checkpoint. This method returns the status object, and so may be
chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_existing_objects_matched()`:
Raises an exception if any existing Python objects in the dependency
graph are unmatched. Unlike `assert_consumed`, this assertion will
pass if values in the checkpoint have no corresponding Python
objects. For example a `tf.keras.Layer` object which has not yet been
built, and so has not created any variables, will pass this assertion
but fail `assert_consumed`. Useful when loading part of a larger
checkpoint into a new Python program, e.g. a training checkpoint with
a `tf.compat.v1.train.Optimizer` was saved but only the state required
for
inference is being loaded. This method returns the status object, and
so may be chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_nontrivial_match()`: Asserts that something aside from the root
object was matched. This is a very weak assertion, but is useful for
sanity checking in library code where objects may exist in the
checkpoint which haven't been created in Python and some Python
objects may not have a checkpointed value. <p></p> * `expect_partial()`: Silence warnings about incomplete checkpoint
restores. Warnings are otherwise printed for unused parts of the
checkpoint file or object when the `Checkpoint` object is deleted
(often at program shutdown). <p></p> * `initialize_or_restore(session=None)`:
When graph building, runs variable initializers if `save_path` is
`None`, but otherwise runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (variables are initialized or restored eagerly). <p></p> * `run_restore_ops(session=None)`:
When graph building, runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (restore operations are run eagerly). May only be
called when `save_path` is not `None`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>checkpoint = tf.train.Checkpoint(... )
            checkpoint.restore(path).assert_consumed() </pre>
</div>
		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.object">object</span> <strong>restore</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> save_path)
		</h4>
		<div class="content">Restore a training checkpoint. <p></p> Restores this `Checkpoint` and any objects it depends on. <p></p> When executing eagerly, either assigns values immediately if variables to
restore have been created already, or defers restoration until the variables
are created. Dependencies added after this call will be matched if they have
a corresponding object in the checkpoint (the restore request will queue in
any trackable object waiting for the expected dependency to be added). <p></p> When graph building, restoration ops are added to the graph but not run
immediately. <p></p> To ensure that loading is complete and no more assignments will take place,
use the `assert_consumed()` method of the status object returned by
`restore`:
An exception will be raised if any Python objects in the dependency graph
were not found in the checkpoint, or if any checkpointed values do not have
a matching Python object. <p></p> When graph building, `assert_consumed()` indicates that all of the restore
ops that will be created for this checkpoint have been created. They can be
run via the `run_restore_ops()` method of the status object:
If the checkpoint has not been consumed completely, then the list of restore
ops will grow as more objects are added to the dependency graph. <p></p> Name-based `tf.compat.v1.train.Saver` checkpoints can be loaded using this
method. Names are used to match variables. No restore ops are created/run
until `run_restore_ops()` or `initialize_or_restore()` are called on the
returned status object when graph building, but there is restore-on-creation
when executing eagerly. Re-encode name-based checkpoints using
<a href="..\..\tf\train\Checkpoint\save.md"><code>tf.train.Checkpoint.save</code></a> as soon as possible. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> save_path
						</dt>
						<dd>The path to the checkpoint, as returned by `save` or
<a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. If None (as when there is no latest
checkpoint for <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a> to return), returns an
object which may run initializers for objects in the dependency graph.
If the checkpoint was written by the name-based
`tf.compat.v1.train.Saver`, names are used to match variables. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A load status object, which can be used to make assertions about the
status of a checkpoint restoration and run initialization/restore ops. <p></p> The returned status object has the following methods: <p></p> * `assert_consumed()`:
Raises an exception if any variables/objects are unmatched: either
checkpointed values which don't have a matching Python object or
Python objects in the dependency graph with no values in the
checkpoint. This method returns the status object, and so may be
chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_existing_objects_matched()`:
Raises an exception if any existing Python objects in the dependency
graph are unmatched. Unlike `assert_consumed`, this assertion will
pass if values in the checkpoint have no corresponding Python
objects. For example a `tf.keras.Layer` object which has not yet been
built, and so has not created any variables, will pass this assertion
but fail `assert_consumed`. Useful when loading part of a larger
checkpoint into a new Python program, e.g. a training checkpoint with
a `tf.compat.v1.train.Optimizer` was saved but only the state required
for
inference is being loaded. This method returns the status object, and
so may be chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_nontrivial_match()`: Asserts that something aside from the root
object was matched. This is a very weak assertion, but is useful for
sanity checking in library code where objects may exist in the
checkpoint which haven't been created in Python and some Python
objects may not have a checkpointed value. <p></p> * `expect_partial()`: Silence warnings about incomplete checkpoint
restores. Warnings are otherwise printed for unused parts of the
checkpoint file or object when the `Checkpoint` object is deleted
(often at program shutdown). <p></p> * `initialize_or_restore(session=None)`:
When graph building, runs variable initializers if `save_path` is
`None`, but otherwise runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (variables are initialized or restored eagerly). <p></p> * `run_restore_ops(session=None)`:
When graph building, runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (restore operations are run eagerly). May only be
called when `save_path` is not `None`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>checkpoint = tf.train.Checkpoint(... )
            checkpoint.restore(path).assert_consumed() </pre>
</div>
		</div>
	</div>
	<div id="restore" class="method">
		<h4>
			<span title="System.object">object</span> <strong>restore</strong>(<span title="System.string">string</span> save_path)
		</h4>
		<div class="content">Restore a training checkpoint. <p></p> Restores this `Checkpoint` and any objects it depends on. <p></p> When executing eagerly, either assigns values immediately if variables to
restore have been created already, or defers restoration until the variables
are created. Dependencies added after this call will be matched if they have
a corresponding object in the checkpoint (the restore request will queue in
any trackable object waiting for the expected dependency to be added). <p></p> When graph building, restoration ops are added to the graph but not run
immediately. <p></p> To ensure that loading is complete and no more assignments will take place,
use the `assert_consumed()` method of the status object returned by
`restore`:
An exception will be raised if any Python objects in the dependency graph
were not found in the checkpoint, or if any checkpointed values do not have
a matching Python object. <p></p> When graph building, `assert_consumed()` indicates that all of the restore
ops that will be created for this checkpoint have been created. They can be
run via the `run_restore_ops()` method of the status object:
If the checkpoint has not been consumed completely, then the list of restore
ops will grow as more objects are added to the dependency graph. <p></p> Name-based `tf.compat.v1.train.Saver` checkpoints can be loaded using this
method. Names are used to match variables. No restore ops are created/run
until `run_restore_ops()` or `initialize_or_restore()` are called on the
returned status object when graph building, but there is restore-on-creation
when executing eagerly. Re-encode name-based checkpoints using
<a href="..\..\tf\train\Checkpoint\save.md"><code>tf.train.Checkpoint.save</code></a> as soon as possible. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> save_path
						</dt>
						<dd>The path to the checkpoint, as returned by `save` or
<a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. If None (as when there is no latest
checkpoint for <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a> to return), returns an
object which may run initializers for objects in the dependency graph.
If the checkpoint was written by the name-based
`tf.compat.v1.train.Saver`, names are used to match variables. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A load status object, which can be used to make assertions about the
status of a checkpoint restoration and run initialization/restore ops. <p></p> The returned status object has the following methods: <p></p> * `assert_consumed()`:
Raises an exception if any variables/objects are unmatched: either
checkpointed values which don't have a matching Python object or
Python objects in the dependency graph with no values in the
checkpoint. This method returns the status object, and so may be
chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_existing_objects_matched()`:
Raises an exception if any existing Python objects in the dependency
graph are unmatched. Unlike `assert_consumed`, this assertion will
pass if values in the checkpoint have no corresponding Python
objects. For example a `tf.keras.Layer` object which has not yet been
built, and so has not created any variables, will pass this assertion
but fail `assert_consumed`. Useful when loading part of a larger
checkpoint into a new Python program, e.g. a training checkpoint with
a `tf.compat.v1.train.Optimizer` was saved but only the state required
for
inference is being loaded. This method returns the status object, and
so may be chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_nontrivial_match()`: Asserts that something aside from the root
object was matched. This is a very weak assertion, but is useful for
sanity checking in library code where objects may exist in the
checkpoint which haven't been created in Python and some Python
objects may not have a checkpointed value. <p></p> * `expect_partial()`: Silence warnings about incomplete checkpoint
restores. Warnings are otherwise printed for unused parts of the
checkpoint file or object when the `Checkpoint` object is deleted
(often at program shutdown). <p></p> * `initialize_or_restore(session=None)`:
When graph building, runs variable initializers if `save_path` is
`None`, but otherwise runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (variables are initialized or restored eagerly). <p></p> * `run_restore_ops(session=None)`:
When graph building, runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (restore operations are run eagerly). May only be
called when `save_path` is not `None`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>checkpoint = tf.train.Checkpoint(... )
            checkpoint.restore(path).assert_consumed() </pre>
</div>
		</div>
	</div>
	<div id="restore_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>restore_dyn</strong>(<span title="System.object">object</span> save_path)
		</h4>
		<div class="content">Restore a training checkpoint. <p></p> Restores this `Checkpoint` and any objects it depends on. <p></p> When executing eagerly, either assigns values immediately if variables to
restore have been created already, or defers restoration until the variables
are created. Dependencies added after this call will be matched if they have
a corresponding object in the checkpoint (the restore request will queue in
any trackable object waiting for the expected dependency to be added). <p></p> When graph building, restoration ops are added to the graph but not run
immediately. <p></p> To ensure that loading is complete and no more assignments will take place,
use the `assert_consumed()` method of the status object returned by
`restore`:
An exception will be raised if any Python objects in the dependency graph
were not found in the checkpoint, or if any checkpointed values do not have
a matching Python object. <p></p> When graph building, `assert_consumed()` indicates that all of the restore
ops that will be created for this checkpoint have been created. They can be
run via the `run_restore_ops()` method of the status object:
If the checkpoint has not been consumed completely, then the list of restore
ops will grow as more objects are added to the dependency graph. <p></p> Name-based `tf.compat.v1.train.Saver` checkpoints can be loaded using this
method. Names are used to match variables. No restore ops are created/run
until `run_restore_ops()` or `initialize_or_restore()` are called on the
returned status object when graph building, but there is restore-on-creation
when executing eagerly. Re-encode name-based checkpoints using
<a href="..\..\tf\train\Checkpoint\save.md"><code>tf.train.Checkpoint.save</code></a> as soon as possible. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> save_path
						</dt>
						<dd>The path to the checkpoint, as returned by `save` or
<a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. If None (as when there is no latest
checkpoint for <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a> to return), returns an
object which may run initializers for objects in the dependency graph.
If the checkpoint was written by the name-based
`tf.compat.v1.train.Saver`, names are used to match variables. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>A load status object, which can be used to make assertions about the
status of a checkpoint restoration and run initialization/restore ops. <p></p> The returned status object has the following methods: <p></p> * `assert_consumed()`:
Raises an exception if any variables/objects are unmatched: either
checkpointed values which don't have a matching Python object or
Python objects in the dependency graph with no values in the
checkpoint. This method returns the status object, and so may be
chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_existing_objects_matched()`:
Raises an exception if any existing Python objects in the dependency
graph are unmatched. Unlike `assert_consumed`, this assertion will
pass if values in the checkpoint have no corresponding Python
objects. For example a `tf.keras.Layer` object which has not yet been
built, and so has not created any variables, will pass this assertion
but fail `assert_consumed`. Useful when loading part of a larger
checkpoint into a new Python program, e.g. a training checkpoint with
a `tf.compat.v1.train.Optimizer` was saved but only the state required
for
inference is being loaded. This method returns the status object, and
so may be chained with `initialize_or_restore` or `run_restore_ops`. <p></p> * `assert_nontrivial_match()`: Asserts that something aside from the root
object was matched. This is a very weak assertion, but is useful for
sanity checking in library code where objects may exist in the
checkpoint which haven't been created in Python and some Python
objects may not have a checkpointed value. <p></p> * `expect_partial()`: Silence warnings about incomplete checkpoint
restores. Warnings are otherwise printed for unused parts of the
checkpoint file or object when the `Checkpoint` object is deleted
(often at program shutdown). <p></p> * `initialize_or_restore(session=None)`:
When graph building, runs variable initializers if `save_path` is
`None`, but otherwise runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (variables are initialized or restored eagerly). <p></p> * `run_restore_ops(session=None)`:
When graph building, runs restore operations. If no `session` is
explicitly specified, the default session is used. No effect when
executing eagerly (restore operations are run eagerly). May only be
called when `save_path` is not `None`. 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>checkpoint = tf.train.Checkpoint(... )
            checkpoint.restore(path).assert_consumed() </pre>
</div>
		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<span title="System.Byte[]">Byte[]</span> file_prefix, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> session)
		</h4>
		<div class="content">Saves a training checkpoint and provides basic checkpoint management. <p></p> The saved checkpoint includes variables created by this object and any
trackable objects it depends on at the time `Checkpoint.save()` is
called. <p></p> `save` is a basic convenience wrapper around the `write` method,
sequentially numbering checkpoints using `save_counter` and updating the
metadata used by <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. More advanced checkpoint
management, for example garbage collection and custom numbering, may be
provided by other utilities which also wrap `write`
(<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for example). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Byte[]">Byte[]</span></code> file_prefix
						</dt>
						<dd>A prefix to use for the checkpoint filenames
(/path/to/directory/and_a_prefix). Names are generated based on this
prefix and `Checkpoint.save_counter`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> session
						</dt>
						<dd>The session to evaluate variables in. Ignored when executing
eagerly. If not provided when graph building, the default session is
used. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the checkpoint. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> file_prefix, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> session)
		</h4>
		<div class="content">Saves a training checkpoint and provides basic checkpoint management. <p></p> The saved checkpoint includes variables created by this object and any
trackable objects it depends on at the time `Checkpoint.save()` is
called. <p></p> `save` is a basic convenience wrapper around the `write` method,
sequentially numbering checkpoints using `save_counter` and updating the
metadata used by <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. More advanced checkpoint
management, for example garbage collection and custom numbering, may be
provided by other utilities which also wrap `write`
(<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for example). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> file_prefix
						</dt>
						<dd>A prefix to use for the checkpoint filenames
(/path/to/directory/and_a_prefix). Names are generated based on this
prefix and `Checkpoint.save_counter`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> session
						</dt>
						<dd>The session to evaluate variables in. Ignored when executing
eagerly. If not provided when graph building, the default session is
used. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the checkpoint. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save</strong>(<span title="System.string">string</span> file_prefix, <span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span> session)
		</h4>
		<div class="content">Saves a training checkpoint and provides basic checkpoint management. <p></p> The saved checkpoint includes variables created by this object and any
trackable objects it depends on at the time `Checkpoint.save()` is
called. <p></p> `save` is a basic convenience wrapper around the `write` method,
sequentially numbering checkpoints using `save_counter` and updating the
metadata used by <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. More advanced checkpoint
management, for example garbage collection and custom numbering, may be
provided by other utilities which also wrap `write`
(<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for example). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> file_prefix
						</dt>
						<dd>A prefix to use for the checkpoint filenames
(/path/to/directory/and_a_prefix). Names are generated based on this
prefix and `Checkpoint.save_counter`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<bool>">Nullable&lt;bool&gt;</span></code> session
						</dt>
						<dd>The session to evaluate variables in. Ignored when executing
eagerly. If not provided when graph building, the default session is
used. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the checkpoint. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="save_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save_dyn</strong>(<span title="System.object">object</span> file_prefix, <span title="System.object">object</span> session)
		</h4>
		<div class="content">Saves a training checkpoint and provides basic checkpoint management. <p></p> The saved checkpoint includes variables created by this object and any
trackable objects it depends on at the time `Checkpoint.save()` is
called. <p></p> `save` is a basic convenience wrapper around the `write` method,
sequentially numbering checkpoints using `save_counter` and updating the
metadata used by <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. More advanced checkpoint
management, for example garbage collection and custom numbering, may be
provided by other utilities which also wrap `write`
(<a href="..\..\tf\train\CheckpointManager.md"><code>tf.contrib.checkpoint.CheckpointManager</code></a> for example). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> file_prefix
						</dt>
						<dd>A prefix to use for the checkpoint filenames
(/path/to/directory/and_a_prefix). Names are generated based on this
prefix and `Checkpoint.save_counter`. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> session
						</dt>
						<dd>The session to evaluate variables in. Ignored when executing
eagerly. If not provided when graph building, the default session is
used. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the checkpoint. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="write" class="method">
		<h4>
			<span title="System.object">object</span> <strong>write</strong>(<span title="System.string">string</span> file_prefix, <span title="System.bool">bool</span> session)
		</h4>
		<div class="content">Writes a training checkpoint. <p></p> The checkpoint includes variables created by this object and any
trackable objects it depends on at the time `Checkpoint.write()` is
called. <p></p> `write` does not number checkpoints, increment `save_counter`, or update the
metadata used by <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. It is primarily intended for
use by higher level checkpoint management utilities. `save` provides a very
basic implementation of these features. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> file_prefix
						</dt>
						<dd>A prefix to use for the checkpoint filenames
(/path/to/directory/and_a_prefix). 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> session
						</dt>
						<dd>The session to evaluate variables in. Ignored when executing
eagerly. If not provided when graph building, the default session is
used. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the checkpoint (i.e. `file_prefix`). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="write" class="method">
		<h4>
			<span title="System.object">object</span> <strong>write</strong>(<span title="System.string">string</span> file_prefix, <a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a> session)
		</h4>
		<div class="content">Writes a training checkpoint. <p></p> The checkpoint includes variables created by this object and any
trackable objects it depends on at the time `Checkpoint.write()` is
called. <p></p> `write` does not number checkpoints, increment `save_counter`, or update the
metadata used by <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. It is primarily intended for
use by higher level checkpoint management utilities. `save` provides a very
basic implementation of these features. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.string">string</span></code> file_prefix
						</dt>
						<dd>A prefix to use for the checkpoint filenames
(/path/to/directory/and_a_prefix). 
						</dd>
						<dt>
							<code><a href="../tensorflow.python.client.session/BaseSession.htm">BaseSession</a></code> session
						</dt>
						<dd>The session to evaluate variables in. Ignored when executing
eagerly. If not provided when graph building, the default session is
used. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the checkpoint (i.e. `file_prefix`). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="write_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>write_dyn</strong>(<span title="System.object">object</span> file_prefix, <span title="System.object">object</span> session)
		</h4>
		<div class="content">Writes a training checkpoint. <p></p> The checkpoint includes variables created by this object and any
trackable objects it depends on at the time `Checkpoint.write()` is
called. <p></p> `write` does not number checkpoints, increment `save_counter`, or update the
metadata used by <a href="..\..\tf\train\latest_checkpoint.md"><code>tf.train.latest_checkpoint</code></a>. It is primarily intended for
use by higher level checkpoint management utilities. `save` provides a very
basic implementation of these features. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> file_prefix
						</dt>
						<dd>A prefix to use for the checkpoint filenames
(/path/to/directory/and_a_prefix). 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> session
						</dt>
						<dd>The session to evaluate variables in. Ignored when executing
eagerly. If not provided when graph building, the default session is
used. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>The full path to the checkpoint (i.e. `file_prefix`). 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	
	<h3 class="section">Public static methods</h3>

	<div id="NewDyn" class="method">
		<h4>
			<a href="../tensorflow.train/Checkpoint.htm">Checkpoint</a> <strong>NewDyn</strong>(<span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> kwargs)
		</h4>
		<div class="content">Group objects into a training checkpoint. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> kwargs
						</dt>
						<dd>Keyword arguments are set as attributes of this object, and are
saved with the checkpoint. Values must be trackable objects. 
						</dd>
				</dl>
			</div>


		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="save_counter" class="method">
		<h4>
			<a href="../tensorflow.python.training.tracking.data_structures/NoDependency.htm">NoDependency</a> <strong>save_counter</strong> get; 
		</h4>
		<div class="content">An integer variable which starts at zero and is incremented on save. <p></p> Used to number checkpoints. 

		</div>
	</div>
	<div id="save_counter_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>save_counter_dyn</strong> get; 
		</h4>
		<div class="content">An integer variable which starts at zero and is incremented on save. <p></p> Used to number checkpoints. 

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>