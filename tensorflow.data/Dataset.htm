<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>Dataset - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.data</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.data/Dataset.htm" class="current">Dataset</a>
        </li>
				<li>
            <a href="../tensorflow.data/DatasetSpec.htm">DatasetSpec</a>
        </li>
				<li>
            <a href="../tensorflow.data/FixedLengthRecordDataset.htm">FixedLengthRecordDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data/IDataset.htm">IDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data/IDatasetSpec.htm">IDatasetSpec</a>
        </li>
				<li>
            <a href="../tensorflow.data/IFixedLengthRecordDataset.htm">IFixedLengthRecordDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data/IIterator.htm">IIterator</a>
        </li>
				<li>
            <a href="../tensorflow.data/IOptions.htm">IOptions</a>
        </li>
				<li>
            <a href="../tensorflow.data/Iterator.htm">Iterator</a>
        </li>
				<li>
            <a href="../tensorflow.data/ITextLineDataset.htm">ITextLineDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data/ITFRecordDataset.htm">ITFRecordDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data/Options.htm">Options</a>
        </li>
				<li>
            <a href="../tensorflow.data/TextLineDataset.htm">TextLineDataset</a>
        </li>
				<li>
            <a href="../tensorflow.data/TFRecordDataset.htm">TFRecordDataset</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> Dataset</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.data</p>
		<p><strong>Parent</strong> <a href="../tensorflow.compat.v2.data/Dataset.htm">Dataset</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.data/IDataset.htm">IDataset</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">Represents a potentially large set of elements. <p></p> A `Dataset` can be used to represent an input pipeline as a
collection of elements and a "logical plan" of transformations that act on
those elements. 
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.data/Dataset.htm#batch">batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#batch">batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#batch">batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#batch">batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#batch">batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#batch_dyn">batch_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_generator">from_generator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_sparse_tensor_slices">from_sparse_tensor_slices</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_sparse_tensor_slices_dyn">from_sparse_tensor_slices_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_tensor_slices">from_tensor_slices</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_tensor_slices_dyn">from_tensor_slices_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_tensors">from_tensors</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#from_tensors_dyn">from_tensors_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#make_one_shot_iterator">make_one_shot_iterator</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#make_one_shot_iterator_dyn">make_one_shot_iterator_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#padded_batch">padded_batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#padded_batch">padded_batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#padded_batch">padded_batch</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#shard">shard</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#shard_dyn">shard_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#skip">skip</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#take">take</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#take_dyn">take_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#zip">zip</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#zip">zip</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#zip">zip</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.data/Dataset.htm#element_spec">element_spec</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#element_spec_dyn">element_spec_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#output_classes">output_classes</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#output_classes_dyn">output_classes_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#output_shapes">output_shapes</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#output_shapes_dyn">output_shapes_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#output_types">output_types</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#output_types_dyn">output_types_dyn</a></li>
				<li><a href="../tensorflow.data/Dataset.htm#PythonObject">PythonObject</a></li>
			</ul>
		
	</div>
	
	<h3 class="section">Public instance methods</h3>

	<div id="batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch</strong>(<a href="../tensorflow/TensorShape.htm">TensorShape</a> batch_size, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into batches. <p></p> The components of the resulting element will have an additional outer
dimension, which will be `batch_size` (or `N % batch_size` for the last
element if `batch_size` does not divide the number of input elements `N`
evenly and `drop_remainder` is `False`). If your program depends on the
batches having the same outer dimension, you should set the `drop_remainder`
argument to `True` to prevent the smaller batch from being produced. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/TensorShape.htm">TensorShape</a></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> batch_size, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into batches. <p></p> The components of the resulting element will have an additional outer
dimension, which will be `batch_size` (or `N % batch_size` for the last
element if `batch_size` does not divide the number of input elements `N`
evenly and `drop_remainder` is `False`). If your program depends on the
batches having the same outer dimension, you should set the `drop_remainder`
argument to `True` to prevent the smaller batch from being produced. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch</strong>(<span title="System.int">int</span> batch_size, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into batches. <p></p> The components of the resulting element will have an additional outer
dimension, which will be `batch_size` (or `N % batch_size` for the last
element if `batch_size` does not divide the number of input elements `N`
evenly and `drop_remainder` is `False`). If your program depends on the
batches having the same outer dimension, you should set the `drop_remainder`
argument to `True` to prevent the smaller batch from being produced. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch</strong>(<a href="../numpy/ndarray.htm">ndarray</a> batch_size, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into batches. <p></p> The components of the resulting element will have an additional outer
dimension, which will be `batch_size` (or `N % batch_size` for the last
element if `batch_size` does not divide the number of input elements `N`
evenly and `drop_remainder` is `False`). If your program depends on the
batches having the same outer dimension, you should set the `drop_remainder`
argument to `True` to prevent the smaller batch from being produced. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch</strong>(<a href="../tensorflow/Dimension.htm">Dimension</a> batch_size, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into batches. <p></p> The components of the resulting element will have an additional outer
dimension, which will be `batch_size` (or `N % batch_size` for the last
element if `batch_size` does not divide the number of input elements `N`
evenly and `drop_remainder` is `False`). If your program depends on the
batches having the same outer dimension, you should set the `drop_remainder`
argument to `True` to prevent the smaller batch from being produced. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/Dimension.htm">Dimension</a></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="batch_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>batch_dyn</strong>(<span title="System.object">object</span> batch_size, <a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into batches. <p></p> The components of the resulting element will have an additional outer
dimension, which will be `batch_size` (or `N % batch_size` for the last
element if `batch_size` does not divide the number of input elements `N`
evenly and `drop_remainder` is `False`). If your program depends on the
batches having the same outer dimension, you should set the `drop_remainder`
argument to `True` to prevent the smaller batch from being produced. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/ImplicitContainer`1.htm">ImplicitContainer&lt;T&gt;</a></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="make_one_shot_iterator" class="method">
		<h4>
			<span title="System.object">object</span> <strong>make_one_shot_iterator</strong>()
		</h4>
		<div class="content">Creates an `Iterator` for enumerating the elements of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `for... in dataset:` to iterate over a dataset. If using <a href="..\..\tf\estimator.md"><code>tf.estimator</code></a>, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`. <p></p> Note: The returned iterator will be initialized automatically.
A "one-shot" iterator does not currently support re-initialization. 



			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>An `Iterator` over the elements of this dataset. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="make_one_shot_iterator_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>make_one_shot_iterator_dyn</strong>()
		</h4>
		<div class="content">Creates an `Iterator` for enumerating the elements of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `for... in dataset:` to iterate over a dataset. If using <a href="..\..\tf\estimator.md"><code>tf.estimator</code></a>, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`. <p></p> Note: The returned iterator will be initialized automatically.
A "one-shot" iterator does not currently support re-initialization. 



			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>An `Iterator` over the elements of this dataset. 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="padded_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>padded_batch</strong>(<span title="System.int">int</span> batch_size, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> padded_shapes, <span title="System.Nullable<ValueTuple<int, string>>">Nullable&lt;ValueTuple&lt;int, string&gt;&gt;</span> padding_values, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into padded batches. <p></p> This transformation combines multiple consecutive elements of the input
dataset into a single element. <p></p> Like <a href="..\..\tf\data\Dataset\batch.md"><code>tf.data.Dataset.batch</code></a>, the components of the resulting element will
have an additional outer dimension, which will be `batch_size` (or
`N % batch_size` for the last element if `batch_size` does not divide the
number of input elements `N` evenly and `drop_remainder` is `False`). If
your program depends on the batches having the same outer dimension, you
should set the `drop_remainder` argument to `True` to prevent the smaller
batch from being produced. <p></p> Unlike <a href="..\..\tf\data\Dataset\batch.md"><code>tf.data.Dataset.batch</code></a>, the input elements to be batched may have
different shapes, and this transformation will pad each component to the
respective shape in `padding_shapes`. The `padding_shapes` argument
determines the resulting shape for each dimension of each component in an
output element: <p></p> * If the dimension is a constant (e.g. `tf.compat.v1.Dimension(37)`), the
component
will be padded out to that length in that dimension.
* If the dimension is unknown (e.g. `tf.compat.v1.Dimension(None)`), the
component
will be padded out to the maximum length of all elements in that
dimension. <p></p> See also <a href="..\..\tf\data\experimental\dense_to_sparse_batch.md"><code>tf.data.experimental.dense_to_sparse_batch</code></a>, which combines
elements that may have different shapes into a <a href="..\..\tf\sparse\SparseTensor.md"><code>tf.SparseTensor</code></a>. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> padded_shapes
						</dt>
						<dd>A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> or <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> vector
tensor-like objects representing the shape to which the respective
component of each input element should be padded prior to batching. Any
unknown dimensions (e.g. `tf.compat.v1.Dimension(None)` in a
<a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> or `-1` in a tensor-like object) will be padded to the
maximum size of that dimension in each batch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<int, string>>">Nullable&lt;ValueTuple&lt;int, string&gt;&gt;</span></code> padding_values
						</dt>
						<dd>(Optional.) A nested structure of scalar-shaped
<a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the padding values to use for the respective
components.  Defaults are `0` for numeric types and the empty string for
string types. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="padded_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>padded_batch</strong>(<span title="System.int">int</span> batch_size, <span title="System.ValueTuple<IEnumerable<int>, object>">ValueTuple&lt;IEnumerable&lt;int&gt;, object&gt;</span> padded_shapes, <span title="System.Nullable<ValueTuple<int, string>>">Nullable&lt;ValueTuple&lt;int, string&gt;&gt;</span> padding_values, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into padded batches. <p></p> This transformation combines multiple consecutive elements of the input
dataset into a single element. <p></p> Like <a href="..\..\tf\data\Dataset\batch.md"><code>tf.data.Dataset.batch</code></a>, the components of the resulting element will
have an additional outer dimension, which will be `batch_size` (or
`N % batch_size` for the last element if `batch_size` does not divide the
number of input elements `N` evenly and `drop_remainder` is `False`). If
your program depends on the batches having the same outer dimension, you
should set the `drop_remainder` argument to `True` to prevent the smaller
batch from being produced. <p></p> Unlike <a href="..\..\tf\data\Dataset\batch.md"><code>tf.data.Dataset.batch</code></a>, the input elements to be batched may have
different shapes, and this transformation will pad each component to the
respective shape in `padding_shapes`. The `padding_shapes` argument
determines the resulting shape for each dimension of each component in an
output element: <p></p> * If the dimension is a constant (e.g. `tf.compat.v1.Dimension(37)`), the
component
will be padded out to that length in that dimension.
* If the dimension is unknown (e.g. `tf.compat.v1.Dimension(None)`), the
component
will be padded out to the maximum length of all elements in that
dimension. <p></p> See also <a href="..\..\tf\data\experimental\dense_to_sparse_batch.md"><code>tf.data.experimental.dense_to_sparse_batch</code></a>, which combines
elements that may have different shapes into a <a href="..\..\tf\sparse\SparseTensor.md"><code>tf.SparseTensor</code></a>. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><span title="System.ValueTuple<IEnumerable<int>, object>">ValueTuple&lt;IEnumerable&lt;int&gt;, object&gt;</span></code> padded_shapes
						</dt>
						<dd>A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> or <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> vector
tensor-like objects representing the shape to which the respective
component of each input element should be padded prior to batching. Any
unknown dimensions (e.g. `tf.compat.v1.Dimension(None)` in a
<a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> or `-1` in a tensor-like object) will be padded to the
maximum size of that dimension in each batch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<int, string>>">Nullable&lt;ValueTuple&lt;int, string&gt;&gt;</span></code> padding_values
						</dt>
						<dd>(Optional.) A nested structure of scalar-shaped
<a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the padding values to use for the respective
components.  Defaults are `0` for numeric types and the empty string for
string types. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="padded_batch" class="method">
		<h4>
			<span title="System.object">object</span> <strong>padded_batch</strong>(<span title="System.int">int</span> batch_size, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> padded_shapes, <span title="System.Nullable<ValueTuple<int, string>>">Nullable&lt;ValueTuple&lt;int, string&gt;&gt;</span> padding_values, <span title="System.bool">bool</span> drop_remainder)
		</h4>
		<div class="content">Combines consecutive elements of this dataset into padded batches. <p></p> This transformation combines multiple consecutive elements of the input
dataset into a single element. <p></p> Like <a href="..\..\tf\data\Dataset\batch.md"><code>tf.data.Dataset.batch</code></a>, the components of the resulting element will
have an additional outer dimension, which will be `batch_size` (or
`N % batch_size` for the last element if `batch_size` does not divide the
number of input elements `N` evenly and `drop_remainder` is `False`). If
your program depends on the batches having the same outer dimension, you
should set the `drop_remainder` argument to `True` to prevent the smaller
batch from being produced. <p></p> Unlike <a href="..\..\tf\data\Dataset\batch.md"><code>tf.data.Dataset.batch</code></a>, the input elements to be batched may have
different shapes, and this transformation will pad each component to the
respective shape in `padding_shapes`. The `padding_shapes` argument
determines the resulting shape for each dimension of each component in an
output element: <p></p> * If the dimension is a constant (e.g. `tf.compat.v1.Dimension(37)`), the
component
will be padded out to that length in that dimension.
* If the dimension is unknown (e.g. `tf.compat.v1.Dimension(None)`), the
component
will be padded out to the maximum length of all elements in that
dimension. <p></p> See also <a href="..\..\tf\data\experimental\dense_to_sparse_batch.md"><code>tf.data.experimental.dense_to_sparse_batch</code></a>, which combines
elements that may have different shapes into a <a href="..\..\tf\sparse\SparseTensor.md"><code>tf.SparseTensor</code></a>. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> batch_size
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
consecutive elements of this dataset to combine in a single batch. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> padded_shapes
						</dt>
						<dd>A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> or <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> vector
tensor-like objects representing the shape to which the respective
component of each input element should be padded prior to batching. Any
unknown dimensions (e.g. `tf.compat.v1.Dimension(None)` in a
<a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> or `-1` in a tensor-like object) will be padded to the
maximum size of that dimension in each batch. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<int, string>>">Nullable&lt;ValueTuple&lt;int, string&gt;&gt;</span></code> padding_values
						</dt>
						<dd>(Optional.) A nested structure of scalar-shaped
<a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the padding values to use for the respective
components.  Defaults are `0` for numeric types and the empty string for
string types. 
						</dd>
						<dt>
							<code><span title="System.bool">bool</span></code> drop_remainder
						</dt>
						<dd>(Optional.) A <a href="..\..\tf\dtypes\bool.md"><code>tf.bool</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing
whether the last batch should be dropped in the case it has fewer than
`batch_size` elements; the default behavior is not to drop the smaller
batch. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="shard" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shard</strong>(<span title="System.int">int</span> num_shards, <span title="System.int">int</span> index)
		</h4>
		<div class="content">Creates a `Dataset` that includes only 1/`num_shards` of this dataset. <p></p> This dataset operator is very useful when running distributed training, as
it allows each worker to read a unique subset. <p></p> When reading a single input file, you can skip elements as follows:
Important caveats: <p></p> - Be sure to shard before you use any randomizing operator (such as
shuffle).
- Generally it is best if the shard operator is used early in the dataset
pipeline. For example, when reading from a set of TFRecord files, shard
before converting the dataset to input samples. This avoids reading every
file on every worker. The following is an example of an efficient
sharding strategy within a complete pipeline: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> num_shards
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
shards operating in parallel. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> index
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the worker index. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>d = tf.data.TFRecordDataset(input_file)
            d = d.shard(num_workers, worker_index)
            d = d.repeat(num_epochs)
            d = d.shuffle(shuffle_buffer_size)
            d = d.map(parser_fn, num_parallel_calls=num_map_threads) </pre>
</div>
		</div>
	</div>
	<div id="shard_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>shard_dyn</strong>(<span title="System.object">object</span> num_shards, <span title="System.object">object</span> index)
		</h4>
		<div class="content">Creates a `Dataset` that includes only 1/`num_shards` of this dataset. <p></p> This dataset operator is very useful when running distributed training, as
it allows each worker to read a unique subset. <p></p> When reading a single input file, you can skip elements as follows:
Important caveats: <p></p> - Be sure to shard before you use any randomizing operator (such as
shuffle).
- Generally it is best if the shard operator is used early in the dataset
pipeline. For example, when reading from a set of TFRecord files, shard
before converting the dataset to input samples. This avoids reading every
file on every worker. The following is an example of an efficient
sharding strategy within a complete pipeline: 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> num_shards
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
shards operating in parallel. 
						</dd>
						<dt>
							<code><span title="System.object">object</span></code> index
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the worker index. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>d = tf.data.TFRecordDataset(input_file)
            d = d.shard(num_workers, worker_index)
            d = d.repeat(num_epochs)
            d = d.shuffle(shuffle_buffer_size)
            d = d.map(parser_fn, num_parallel_calls=num_map_threads) </pre>
</div>
		</div>
	</div>
	<div id="skip" class="method">
		<h4>
			<span title="System.object">object</span> <strong>skip</strong>(<span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span> count)
		</h4>
		<div class="content">Creates a `Dataset` that skips `count` elements from this dataset. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<int>">IEnumerable&lt;int&gt;</span></code> count
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
elements of this dataset that should be skipped to form the new dataset.
If `count` is greater than the size of this dataset, the new dataset
will contain no elements.  If `count` is -1, skips the entire dataset. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="take" class="method">
		<h4>
			<span title="System.object">object</span> <strong>take</strong>(<span title="System.int">int</span> count)
		</h4>
		<div class="content">Creates a `Dataset` with at most `count` elements from this dataset. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.int">int</span></code> count
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
elements of this dataset that should be taken to form the new dataset.
If `count` is -1, or if `count` is greater than the size of this
dataset, the new dataset will contain all elements of this dataset. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="take_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>take_dyn</strong>(<span title="System.object">object</span> count)
		</h4>
		<div class="content">Creates a `Dataset` with at most `count` elements from this dataset. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> count
						</dt>
						<dd>A <a href="..\..\tf\dtypes\int64.md"><code>tf.int64</code></a> scalar <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a>, representing the number of
elements of this dataset that should be taken to form the new dataset.
If `count` is -1, or if `count` is greater than the size of this
dataset, the new dataset will contain all elements of this dataset. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	
	<h3 class="section">Public static methods</h3>

	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> output_types, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> output_types, <a href="../LostTech.Gradient/PythonClassContainer.htm">PythonClassContainer</a> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/PythonClassContainer.htm">PythonClassContainer</a></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> output_types, <a href="../LostTech.Gradient/PythonClassContainer.htm">PythonClassContainer</a> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient/PythonClassContainer.htm">PythonClassContainer</a></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <a href="../tensorflow/DType.htm">DType</a> output_types, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><a href="../tensorflow/DType.htm">DType</a></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <a href="../tensorflow/DType.htm">DType</a> output_types, <span title="System.int">int</span> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><a href="../tensorflow/DType.htm">DType</a></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> output_types, <span title="System.int">int</span> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span> output_types, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IDictionary<string, object>">IDictionary&lt;string, object&gt;</span></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_generator" class="method">
		<h4>
			<a href="../tensorflow.data/Dataset.htm">Dataset</a> <strong>from_generator</strong>(<a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a> generator, <span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span> output_types, <span title="System.int">int</span> output_shapes, <span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span> args)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are generated by `generator`. <p></p> The `generator` argument must be a callable object that returns
an object that supports the `iter()` protocol (e.g. a generator function).
The elements generated by `generator` must be compatible with the given
`output_types` and (optional) `output_shapes` arguments.
NOTE: The current implementation of `Dataset.from_generator()` uses
<a href="..\..\tf\numpy_function.md"><code>tf.numpy_function</code></a> and inherits the same constraints. In particular, it
requires the `Dataset`- and `Iterator`-related operations to be placed
on a device in the same process as the Python program that called
`Dataset.from_generator()`. The body of `generator` will not be
serialized in a `GraphDef`, and you should not use this method if you
need to serialize your model and restore it in a different environment. <p></p> NOTE: If `generator` depends on mutable global variables or other external
state, be aware that the runtime may invoke `generator` multiple times
(in order to support repeating the `Dataset`) and at any time
between the call to `Dataset.from_generator()` and the production of the
first element from the generator. Mutating global variables or external
state can cause undefined behavior, and we recommend that you explicitly
cache any external state in `generator` before calling
`Dataset.from_generator()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient/PythonFunctionContainer.htm">PythonFunctionContainer</a></code> generator
						</dt>
						<dd>A callable object that returns an object that supports the
`iter()` protocol. If `args` is not specified, `generator` must take no
arguments; otherwise it must take as many arguments as there are values
in `args`. 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<object>">IEnumerable&lt;object&gt;</span></code> output_types
						</dt>
						<dd>A nested structure of <a href="..\..\tf\dtypes\DType.md"><code>tf.DType</code></a> objects corresponding to
each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> output_shapes
						</dt>
						<dd>(Optional.) A nested structure of <a href="..\..\tf\TensorShape.md"><code>tf.TensorShape</code></a> objects
corresponding to each component of an element yielded by `generator`. 
						</dd>
						<dt>
							<code><span title="System.Nullable<ValueTuple<object>>">Nullable&lt;ValueTuple&lt;object&gt;&gt;</span></code> args
						</dt>
						<dd>(Optional.) A tuple of <a href="..\..\tf\Tensor.md"><code>tf.Tensor</code></a> objects that will be evaluated
and passed to `generator` as NumPy-array arguments. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>import itertools
            tf.compat.v1.enable_eager_execution() <p></p> def gen():
  for i in itertools.count(1):
    yield (i, [1] * i) <p></p> ds = tf.data.Dataset.from_generator(
    gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None]))) <p></p> for value in ds.take(2):
  print value
# (1, array([1]))
# (2, array([1, 1])) </pre>
</div>
		</div>
	</div>
	<div id="from_sparse_tensor_slices" class="method">
		<h4>
			<a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a> <strong>from_sparse_tensor_slices</strong>(<a href="../tensorflow/SparseTensor.htm">SparseTensor</a> sparse_tensor)
		</h4>
		<div class="content">Splits each rank-N <a href="..\..\tf\sparse\SparseTensor.md"><code>tf.SparseTensor</code></a> in this dataset row-wise. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.from_tensor_slices()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow/SparseTensor.htm">SparseTensor</a></code> sparse_tensor
						</dt>
						<dd>A <a href="..\..\tf\sparse\SparseTensor.md"><code>tf.SparseTensor</code></a>. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="from_sparse_tensor_slices_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>from_sparse_tensor_slices_dyn</strong>(<span title="System.object">object</span> sparse_tensor)
		</h4>
		<div class="content">Splits each rank-N <a href="..\..\tf\sparse\SparseTensor.md"><code>tf.SparseTensor</code></a> in this dataset row-wise. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.from_tensor_slices()`. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> sparse_tensor
						</dt>
						<dd>A <a href="..\..\tf\sparse\SparseTensor.md"><code>tf.SparseTensor</code></a>. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="from_tensor_slices" class="method">
		<h4>
			<a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a> <strong>from_tensor_slices</strong>(<span title="System.object">object</span> tensors)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are slices of the given tensors. <p></p> Note that if `tensors` contains a NumPy array, and eager execution is not
enabled, the values will be embedded in the graph as one or more
<a href="..\..\tf\constant.md"><code>tf.constant</code></a> operations. For large datasets (> 1 GB), this can waste
memory and run into byte limits of graph serialization. If `tensors`
contains one or more large NumPy arrays, consider the alternative described
in [this guide](
https://tensorflow.org/guide/datasets#consuming_numpy_arrays). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>A dataset element, with each component having the same size in
the 0th dimension. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="from_tensor_slices_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>from_tensor_slices_dyn</strong>(<span title="System.object">object</span> tensors)
		</h4>
		<div class="content">Creates a `Dataset` whose elements are slices of the given tensors. <p></p> Note that if `tensors` contains a NumPy array, and eager execution is not
enabled, the values will be embedded in the graph as one or more
<a href="..\..\tf\constant.md"><code>tf.constant</code></a> operations. For large datasets (> 1 GB), this can waste
memory and run into byte limits of graph serialization. If `tensors`
contains one or more large NumPy arrays, consider the alternative described
in [this guide](
https://tensorflow.org/guide/datasets#consuming_numpy_arrays). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>A dataset element, with each component having the same size in
the 0th dimension. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="from_tensors" class="method">
		<h4>
			<a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a> <strong>from_tensors</strong>(<span title="System.object">object</span> tensors)
		</h4>
		<div class="content">Creates a `Dataset` with a single element, comprising the given tensors. <p></p> Note that if `tensors` contains a NumPy array, and eager execution is not
enabled, the values will be embedded in the graph as one or more
<a href="..\..\tf\constant.md"><code>tf.constant</code></a> operations. For large datasets (> 1 GB), this can waste
memory and run into byte limits of graph serialization. If `tensors`
contains one or more large NumPy arrays, consider the alternative described
in [this
guide](https://tensorflow.org/guide/datasets#consuming_numpy_arrays). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>A dataset element. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="from_tensors_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>from_tensors_dyn</strong>(<span title="System.object">object</span> tensors)
		</h4>
		<div class="content">Creates a `Dataset` with a single element, comprising the given tensors. <p></p> Note that if `tensors` contains a NumPy array, and eager execution is not
enabled, the values will be embedded in the graph as one or more
<a href="..\..\tf\constant.md"><code>tf.constant</code></a> operations. For large datasets (> 1 GB), this can waste
memory and run into byte limits of graph serialization. If `tensors`
contains one or more large NumPy arrays, consider the alternative described
in [this
guide](https://tensorflow.org/guide/datasets#consuming_numpy_arrays). 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> tensors
						</dt>
						<dd>A dataset element. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="zip" class="method">
		<h4>
			<a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a> <strong>zip</strong>(<a href="../tensorflow.data/Dataset.htm">Dataset</a> datasets)
		</h4>
		<div class="content">Creates a `Dataset` by zipping together the given datasets. <p></p> This method has similar semantics to the built-in `zip()` function
in Python, with the main difference being that the `datasets`
argument can be an arbitrary nested structure of `Dataset` objects. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.data/Dataset.htm">Dataset</a></code> datasets
						</dt>
						<dd>A nested structure of datasets. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>a = Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]
            b = Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]
            c = Dataset.range(7, 13).batch(2)  # ==> [ [7, 8], [9, 10], [11, 12] ]
            d = Dataset.range(13, 15)  # ==> [ 13, 14 ] <p></p> # The nested structure of the `datasets` argument determines the
# structure of elements in the resulting dataset.
Dataset.zip((a, b))  # ==> [ (1, 4), (2, 5), (3, 6) ]
Dataset.zip((b, a))  # ==> [ (4, 1), (5, 2), (6, 3) ] <p></p> # The `datasets` argument may contain an arbitrary number of
# datasets.
Dataset.zip((a, b, c))  # ==> [ (1, 4, [7, 8]),
                        #       (2, 5, [9, 10]),
                        #       (3, 6, [11, 12]) ] <p></p> # The number of elements in the resulting dataset is the same as
# the size of the smallest dataset in `datasets`.
Dataset.zip((a, d))  # ==> [ (1, 13), (2, 14) ] </pre>
</div>
		</div>
	</div>
	<div id="zip" class="method">
		<h4>
			<a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a> <strong>zip</strong>(<a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a> datasets)
		</h4>
		<div class="content">Creates a `Dataset` by zipping together the given datasets. <p></p> This method has similar semantics to the built-in `zip()` function
in Python, with the main difference being that the `datasets`
argument can be an arbitrary nested structure of `Dataset` objects. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></code> datasets
						</dt>
						<dd>A nested structure of datasets. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>a = Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]
            b = Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]
            c = Dataset.range(7, 13).batch(2)  # ==> [ [7, 8], [9, 10], [11, 12] ]
            d = Dataset.range(13, 15)  # ==> [ 13, 14 ] <p></p> # The nested structure of the `datasets` argument determines the
# structure of elements in the resulting dataset.
Dataset.zip((a, b))  # ==> [ (1, 4), (2, 5), (3, 6) ]
Dataset.zip((b, a))  # ==> [ (4, 1), (5, 2), (6, 3) ] <p></p> # The `datasets` argument may contain an arbitrary number of
# datasets.
Dataset.zip((a, b, c))  # ==> [ (1, 4, [7, 8]),
                        #       (2, 5, [9, 10]),
                        #       (3, 6, [11, 12]) ] <p></p> # The number of elements in the resulting dataset is the same as
# the size of the smallest dataset in `datasets`.
Dataset.zip((a, d))  # ==> [ (1, 13), (2, 14) ] </pre>
</div>
		</div>
	</div>
	<div id="zip" class="method">
		<h4>
			<a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a> <strong>zip</strong>(<span title="System.object">object</span> datasets)
		</h4>
		<div class="content">Creates a `Dataset` by zipping together the given datasets. <p></p> This method has similar semantics to the built-in `zip()` function
in Python, with the main difference being that the `datasets`
argument can be an arbitrary nested structure of `Dataset` objects. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><span title="System.object">object</span></code> datasets
						</dt>
						<dd>A nested structure of datasets. 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><a href="../tensorflow.python.data.ops.dataset_ops/DatasetV1Adapter.htm">DatasetV1Adapter</a></code>
					</dt>
					<dd><p></p> 
					</dd>
				</dl>
			</div>
<div class="example">
  <a href="javascript:void(0)">Show Example</a>
  <pre>a = Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]
            b = Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]
            c = Dataset.range(7, 13).batch(2)  # ==> [ [7, 8], [9, 10], [11, 12] ]
            d = Dataset.range(13, 15)  # ==> [ 13, 14 ] <p></p> # The nested structure of the `datasets` argument determines the
# structure of elements in the resulting dataset.
Dataset.zip((a, b))  # ==> [ (1, 4), (2, 5), (3, 6) ]
Dataset.zip((b, a))  # ==> [ (4, 1), (5, 2), (6, 3) ] <p></p> # The `datasets` argument may contain an arbitrary number of
# datasets.
Dataset.zip((a, b, c))  # ==> [ (1, 4, [7, 8]),
                        #       (2, 5, [9, 10]),
                        #       (3, 6, [11, 12]) ] <p></p> # The number of elements in the resulting dataset is the same as
# the size of the smallest dataset in `datasets`.
Dataset.zip((a, d))  # ==> [ (1, 13), (2, 14) ] </pre>
</div>
		</div>
	</div>
	
	<h3 class="section">Public properties</h3>

	<div id="element_spec" class="method">
		<h4>
			<span title="System.object">object</span> <strong>element_spec</strong> get; 
		</h4>
		<div class="content">The type specification of an element of this dataset. 

		</div>
	</div>
	<div id="element_spec_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>element_spec_dyn</strong> get; 
		</h4>
		<div class="content">The type specification of an element of this dataset. 

		</div>
	</div>
	<div id="output_classes" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_classes</strong> get; 
		</h4>
		<div class="content">Returns the class of each component of an element of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_classes(dataset)`. 

		</div>
	</div>
	<div id="output_classes_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_classes_dyn</strong> get; 
		</h4>
		<div class="content">Returns the class of each component of an element of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_classes(dataset)`. 

		</div>
	</div>
	<div id="output_shapes" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_shapes</strong> get; 
		</h4>
		<div class="content">Returns the shape of each component of an element of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`. 

		</div>
	</div>
	<div id="output_shapes_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_shapes_dyn</strong> get; 
		</h4>
		<div class="content">Returns the shape of each component of an element of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`. 

		</div>
	</div>
	<div id="output_types" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_types</strong> get; 
		</h4>
		<div class="content">Returns the type of each component of an element of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(dataset)`. 

		</div>
	</div>
	<div id="output_types_dyn" class="method">
		<h4>
			<span title="System.object">object</span> <strong>output_types_dyn</strong> get; 
		</h4>
		<div class="content">Returns the type of each component of an element of this dataset. (deprecated) <p></p> Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_types(dataset)`. 

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>