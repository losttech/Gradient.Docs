<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Frameset//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-frameset.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
	<!--[if lt IE 9]>
	<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <title>Loss - LostTech.TensorFlow Documentation</title>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
    <link type="text/css" rel="stylesheet" href="../main.css"/>
    <script type="text/javascript" src="../js/jquery-1.3.2.min.js"></script>
    <script type="text/javascript" src="../js/jquery.scrollTo-min.js"></script>
    <script type="text/javascript" src="../js/navigation.js"></script>
    <script type="text/javascript" src="../js/example.js"></script>
  </head>
  <body>
  	<header><h1>LostTech.TensorFlow : API Documentation</h1>
	</header>

    <nav id="namespaces">
      <iframe src="../namespaces.htm"></iframe>
    </nav><nav id="types">
  <h2 class="fixed">Types in tensorflow.keras.losses</h2>
	<div class="scroll">
		<ul>
				<li>
            <a href="../tensorflow.keras.losses/BinaryCrossentropy.htm">BinaryCrossentropy</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/CategoricalCrossentropy.htm">CategoricalCrossentropy</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/CategoricalHinge.htm">CategoricalHinge</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/CosineSimilarity.htm">CosineSimilarity</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/Hinge.htm">Hinge</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/Huber.htm">Huber</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IBinaryCrossentropy.htm">IBinaryCrossentropy</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/ICategoricalCrossentropy.htm">ICategoricalCrossentropy</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/ICategoricalHinge.htm">ICategoricalHinge</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/ICosineSimilarity.htm">ICosineSimilarity</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IHinge.htm">IHinge</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IHuber.htm">IHuber</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IKLDivergence.htm">IKLDivergence</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/ILogCosh.htm">ILogCosh</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/ILoss.htm">ILoss</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IMeanAbsoluteError.htm">IMeanAbsoluteError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IMeanAbsolutePercentageError.htm">IMeanAbsolutePercentageError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IMeanSquaredError.htm">IMeanSquaredError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IMeanSquaredLogarithmicError.htm">IMeanSquaredLogarithmicError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/IPoisson.htm">IPoisson</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/ISparseCategoricalCrossentropy.htm">ISparseCategoricalCrossentropy</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/ISquaredHinge.htm">ISquaredHinge</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/KLDivergence.htm">KLDivergence</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/LogCosh.htm">LogCosh</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/Loss.htm" class="current">Loss</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/losses.htm">losses</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/MeanAbsoluteError.htm">MeanAbsoluteError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/MeanAbsolutePercentageError.htm">MeanAbsolutePercentageError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/MeanSquaredError.htm">MeanSquaredError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/MeanSquaredLogarithmicError.htm">MeanSquaredLogarithmicError</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/Poisson.htm">Poisson</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/SparseCategoricalCrossentropy.htm">SparseCategoricalCrossentropy</a>
        </li>
				<li>
            <a href="../tensorflow.keras.losses/SquaredHinge.htm">SquaredHinge</a>
        </li>
		</ul>
	</div>
</nav>
	<article>
    <header>
		<p class="class"><strong>Type</strong> Loss</p>
	</header>
	<section>
		<header>
		<p><strong>Namespace</strong> tensorflow.keras.losses</p>
		<p><strong>Parent</strong> <a href="../LostTech.Gradient/PythonObjectContainer.htm">PythonObjectContainer</a></p>
		<p><strong>Interfaces</strong> <a href="../tensorflow.keras.losses/ILoss.htm">ILoss</a></p>
		</header>
    <div class="sub-header">
			<div id="summary">Loss base class. <p></p> To be implemented by subclasses:
* `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. <p></p> Example subclass implementation:
```
class MeanSquaredError(Loss):
def call(self, y_true, y_pred):
y_pred = ops.convert_to_tensor(y_pred)
y_true = math_ops.cast(y_true, y_pred.dtype)
return K.mean(math_ops.square(y_pred - y_true), axis=-1)
``` <p></p> When used with <a href="..\..\..\tf\distribute\Strategy.md"><code>tf.distribute.Strategy</code></a>, outside of built-in training loops
such as <a href="..\..\..\tf\keras.md"><code>tf.keras</code></a> `compile` and `fit`, please use 'SUM' or 'NONE' reduction
types, and reduce losses explicitly in your training loop. Using 'AUTO' or
'SUM_OVER_BATCH_SIZE' will raise an error. <p></p> Please see
https://www.tensorflow.org/alpha/tutorials/distribute/training_loops for more
details on this. <p></p> You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:
```
with strategy.scope():
loss_obj = tf.keras.losses.CategoricalCrossentropy(
reduction=tf.keras.losses.Reduction.NONE)
....
loss = (tf.reduce_sum(loss_obj(labels, predictions)) *
(1. / global_batch_size))
``` 
			</div>
		
		
			<h3 class="section">Methods</h3>
			<ul>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#__call__">__call__</a></li>
			</ul>
		
			<h3 class="section">Properties</h3>
			<ul>
				<li><a href="../tensorflow.keras.losses/Loss.htm#name">name</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#PythonObject">PythonObject</a></li>
				<li><a href="../tensorflow.keras.losses/Loss.htm#reduction">reduction</a></li>
			</ul>
		
	</div>
	
	<h3 class="section">Public instance methods</h3>

	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <a href="../numpy/ndarray.htm">ndarray</a> y_pred, <span title="System.double">double</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_pred, <span title="System.double">double</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> y_pred, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> y_pred, <span title="System.int">int</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> y_pred, <span title="System.double">double</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <a href="../numpy/ndarray.htm">ndarray</a> y_pred, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <a href="../numpy/ndarray.htm">ndarray</a> y_pred, <span title="System.int">int</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <a href="../numpy/ndarray.htm">ndarray</a> y_pred, <span title="System.double">double</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_pred, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_pred, <span title="System.int">int</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_pred, <span title="System.double">double</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> y_pred, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> y_pred, <span title="System.int">int</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span> y_pred, <span title="System.double">double</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.Collections.Generic.IEnumerable<IGraphNodeBase>">IEnumerable&lt;IGraphNodeBase&gt;</span></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.double">double</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <a href="../numpy/ndarray.htm">ndarray</a> y_pred, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../numpy/ndarray.htm">ndarray</a> y_true, <a href="../numpy/ndarray.htm">ndarray</a> y_pred, <span title="System.int">int</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../numpy/ndarray.htm">ndarray</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_pred, <span title="System.int">int</span> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><span title="System.int">int</span></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	<div id="__call__" class="method">
		<h4>
			<span title="System.object">object</span> <strong>__call__</strong>(<a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_true, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> y_pred, <a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a> sample_weight)
		</h4>
		<div class="content">Invokes the `Loss` instance. 


			<div class="parameters">
				<h5>Parameters</h5>
				<dl>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_true
						</dt>
						<dd>Ground truth values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> y_pred
						</dt>
						<dd>The predicted values. shape = `[batch_size, d0,.. dN]` 
						</dd>
						<dt>
							<code><a href="../LostTech.Gradient.ManualWrappers/IGraphNodeBase.htm">IGraphNodeBase</a></code> sample_weight
						</dt>
						<dd>Optional `sample_weight` acts as a
coefficient for the loss. If a scalar is provided, then the loss is
simply scaled by the given value. If `sample_weight` is a tensor of size
`[batch_size]`, then the total loss for each sample of the batch is
rescaled by the corresponding element in the `sample_weight` vector. If
the shape of `sample_weight` is `[batch_size, d0,.. dN-1]` (or can be
broadcasted to this shape), then each loss element of `y_pred` is scaled
by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss
functions reduce by 1 dimension, usually axis=-1.) 
						</dd>
				</dl>
			</div>

			<div class="return">

				<h5>Returns</h5>
				<dl>
					<dt>
						<code><span title="System.object">object</span></code>
					</dt>
					<dd>Weighted loss float `Tensor`. If `reduction` is `NONE`, this has
shape `[batch_size, d0,.. dN-1]`; otherwise, it is scalar. (Note `dN-1`
because all loss functions reduce by 1 dimension, usually axis=-1.) 
					</dd>
				</dl>
			</div>

		</div>
	</div>
	
	
	<h3 class="section">Public properties</h3>

	<div id="name" class="method">
		<h4>
			<span title="System.string">string</span> <strong>name</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="PythonObject" class="method">
		<h4>
			<span title="System.object">object</span> <strong>PythonObject</strong> get; 
		</h4>
		<div class="content">

		</div>
	</div>
	<div id="reduction" class="method">
		<h4>
			<span title="System.string">string</span> <strong>reduction</strong> get; set;
		</h4>
		<div class="content">

		</div>
	</div>
	</section>
	</article><footer>
	<span id="version">Built from v1.15.0.0 of LostTech.TensorFlow</span>
	<span id="docu-link">
		Generated by <a href="http://docu.jagregory.com">docu</a>
	</span>
</footer>
  </body>
</html>